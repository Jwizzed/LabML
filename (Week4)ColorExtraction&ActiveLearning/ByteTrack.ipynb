{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84feb8f3-0707-4317-b99d-b02d7636c89b",
   "metadata": {},
   "source": [
    "# Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bc04f37-2b0f-4f26-9956-955553eb21a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Downloads\\\\DeepFashion2\\\\DeepFashion2'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import supervision as sv\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "HOME = os.getcwd()\n",
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725bc1d3-e123-46c2-8cd6-1aed1f57305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics supervision --upgrade supervision\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653daf3a-d503-4a0c-91ca-fad27376e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"{HOME}/yolov9/deepfashion2-m-11k-1\"\n",
    "IMAGE_PATH = f\"{HOME}/yolov9/deepfashion2-m-11k-1/train/images\"\n",
    "WEIGHT_PATH = f\"{HOME}/yolov9/weights/yolov9c.pt\"\n",
    "# SINGLE_IMAGE = IMAGE_PATH + \"/000001_png.rf.3efc63de2fe2bab8882abc87437bf048.jpg\"\n",
    "SINGLE_IMAGE = \"yolov9/mytest/5.jpeg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2c3a2-9e08-4547-a578-dc3bee72f5a3",
   "metadata": {},
   "source": [
    "# Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4931546a-c014-4195-9b27-fdc8bba85342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9c summary: 618 layers, 25590912 parameters, 0 gradients, 104.0 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(618, 25590912, 0, 104.02268160000003)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(WEIGHT_PATH)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3673d92-8052-4fa0-a588-a6c340f38671",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "156b8bb6-12e6-400e-b1ef-5280fe84eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_video_path = f\"{HOME}/yolov9/myvid/background video _ people _ walking _.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbda20-e6c1-4cb6-bf82-1c32edce6027",
   "metadata": {},
   "source": [
    "# ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cfc0526-9961-4623-82b2-22be23de59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\n",
    "ELLIPSE_ANNOTATOR = sv.EllipseAnnotator()\n",
    "LABEL_ANNOTATOR = sv.LabelAnnotator(text_color=sv.Color.from_hex(\"#000000\"))\n",
    "tracker = sv.ByteTrack(minimum_matching_threshold=0.5)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "polygons = load_zones_config(file_path=zone_configuration_path)\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=source_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd2606-1941-4c50-a584-d60b3ac1618e",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6538f550-2d27-4a38-ad37-29ecdc865c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPSBasedTimer:\n",
    "    \"\"\"\n",
    "    A timer that calculates the duration each object has been detected based on frames\n",
    "    per second (FPS).\n",
    "\n",
    "    Attributes:\n",
    "        fps (int): The frame rate of the video stream, used to calculate time durations.\n",
    "        frame_id (int): The current frame number in the sequence.\n",
    "        tracker_id2frame_id (Dict[int, int]): Maps each tracker's ID to the frame number\n",
    "            at which it was first detected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fps: int = 30) -> None:\n",
    "        \"\"\"Initializes the FPSBasedTimer with the specified frames per second rate.\n",
    "\n",
    "        Args:\n",
    "            fps (int, optional): The frame rate of the video stream. Defaults to 30.\n",
    "        \"\"\"\n",
    "        self.fps = fps\n",
    "        self.frame_id = 0\n",
    "        self.tracker_id2frame_id: Dict[int, int] = {}\n",
    "\n",
    "    def tick(self, detections: sv.Detections) -> np.ndarray:\n",
    "        \"\"\"Processes the current frame, updating time durations for each tracker.\n",
    "\n",
    "        Args:\n",
    "            detections: The detections for the current frame, including tracker IDs.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time durations (in seconds) for each detected tracker, since\n",
    "                their first detection.\n",
    "        \"\"\"\n",
    "        self.frame_id += 1\n",
    "        times = []\n",
    "\n",
    "        for tracker_id in detections.tracker_id:\n",
    "            self.tracker_id2frame_id.setdefault(tracker_id, self.frame_id)\n",
    "\n",
    "            start_frame_id = self.tracker_id2frame_id[tracker_id]\n",
    "            time_duration = (self.frame_id - start_frame_id) / self.fps\n",
    "            times.append(time_duration)\n",
    "\n",
    "        return np.array(times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8691a2-c5f8-4784-80da-654f5263f8b0",
   "metadata": {},
   "source": [
    "## Draw Pologon Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8554bca5-881a-416e-9be9-54e62552b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"yolov9/myvid/background video _ people _ walking _.mp4\"\n",
    "zone_configuration_path =  \"scripts/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f85ed7ec-208e-405a-9751-88b7515e5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polygons saved to scripts/config.json\n"
     ]
    }
   ],
   "source": [
    "!python scripts/draw_zones.py --source_path \"{source_path}\" --zone_configuration_path \"{zone_configuration_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec439e94-5bbe-42ac-a124-bbbdd8170988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zones_config(file_path: str):\n",
    "    \"\"\"\n",
    "    Load polygon zone configurations from a JSON file.\n",
    "\n",
    "    This function reads a JSON file which contains polygon coordinates, and\n",
    "    converts them into a list of NumPy arrays. Each polygon is represented as\n",
    "    a NumPy array of coordinates.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: A list of polygons, each represented as a NumPy array.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        return [np.array(polygon, np.int32) for polygon in data]\n",
    "\n",
    "def find_in_list(array: np.ndarray, search_list):\n",
    "    \"\"\"Determines if elements of a numpy array are present in a list.\n",
    "\n",
    "    Args:\n",
    "        array (np.ndarray): The numpy array of integers to check.\n",
    "        search_list (List[int]): The list of integers to search within.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array of booleans, where each boolean indicates whether\n",
    "        the corresponding element in `array` is found in `search_list`.\n",
    "    \"\"\"\n",
    "    if not search_list:\n",
    "        return np.ones(array.shape, dtype=bool)\n",
    "    else:\n",
    "        return np.isin(array, search_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94631d9a-be16-448a-a907-49b659c91ddc",
   "metadata": {},
   "source": [
    "## Short Ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02e76435-b856-422b-bcb3-d195c531e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zones\n",
    "polygons = load_zones_config(file_path=zone_configuration_path)\n",
    "zones = [\n",
    "    sv.PolygonZone(\n",
    "        polygon=polygon,\n",
    "        frame_resolution_wh=resolution_wh,\n",
    "        triggering_anchors=(sv.Position.CENTER,),\n",
    "    )\n",
    "    for polygon in polygons\n",
    "]\n",
    "\n",
    "# Timers\n",
    "timers = [FPSBasedTimer(video_info.fps) for _ in zones]\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    results = model(frame, verbose=False, device=device, conf=0.3)[0]\n",
    "    \n",
    "    # Detections\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[detections.class_id == 0]\n",
    "    detections = detections.with_nms(threshold=0.7)\n",
    "    detections = detections[find_in_list(detections.class_id, [0])]\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "\n",
    "    # Annotation\n",
    "    annotated_frame = frame.copy()\n",
    "    for idx, zone in enumerate(zones):\n",
    "        annotated_frame = sv.draw_polygon(\n",
    "            scene=annotated_frame, polygon=zone.polygon, color=COLORS.by_idx(idx)\n",
    "        )\n",
    "        detections_in_zone = detections[zone.trigger(detections)]\n",
    "        time_in_zone = timers[idx].tick(detections_in_zone)\n",
    "        \n",
    "        labels = [\n",
    "                f\"#{tracker_id} {int(time // 60):02d}:{int(time % 60):02d}\"\n",
    "                for tracker_id, time in zip(detections_in_zone.tracker_id, time_in_zone)\n",
    "        ]\n",
    "        \n",
    "        annotated_frame = ELLIPSE_ANNOTATOR.annotate(\n",
    "            scene=annotated_frame, detections=detections_in_zone)\n",
    "        annotated_frame = LABEL_ANNOTATOR.annotate(\n",
    "            scene=annotated_frame, detections=detections_in_zone, labels=labels)\n",
    "        \n",
    "        return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=source_video_path,\n",
    "    target_path=\"output_video.mp4\",\n",
    "    callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13fb08-9f92-4c65-b075-75bbdd1efc2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Full Ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81cc19e7-8172-4b22-b1f5-da3f5206b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_info = sv.VideoInfo.from_video_path(video_path=source_video_path)\n",
    "# frames_generator = sv.get_video_frames_generator(source_video_path)\n",
    "\n",
    "# frame = next(frames_generator)\n",
    "# resolution_wh = frame.shape[1], frame.shape[0]\n",
    "\n",
    "# # For saving the video\n",
    "# output_video_path = \"output_video.mp4\"\n",
    "# fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "# fps = video_info.fps\n",
    "# output_video = cv2.VideoWriter(output_video_path, fourcc, fps, resolution_wh)\n",
    "\n",
    "# for frame in frames_generator:\n",
    "#     results = model(frame, verbose=False, device=device, conf=0.3)[0]\n",
    "#     detections = sv.Detections.from_ultralytics(results)\n",
    "#     detections = detections.with_nms(threshold=0.7)\n",
    "#     detections = tracker.update_with_detections(detections)\n",
    "#     detections = detections[detections.class_id == 0]\n",
    "\n",
    "#     annotated_frame = frame.copy()\n",
    "\n",
    "#     annotated_frame = COLOR_ANNOTATOR.annotate(\n",
    "#         scene=annotated_frame,\n",
    "#         detections=detections,\n",
    "#     )\n",
    "\n",
    "#     labels = [\n",
    "#         f\"#{tracker_id}\"\n",
    "#         for tracker_id in detections.tracker_id\n",
    "#     ]\n",
    "\n",
    "#     annotated_frame = LABEL_ANNOTATOR.annotate(\n",
    "#         scene=annotated_frame,\n",
    "#         detections=detections,\n",
    "#         labels=labels,\n",
    "#     )\n",
    "#     output_video.write(annotated_frame)\n",
    "\n",
    "#     cv2.imshow(\"Processed Video\", annotated_frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "\n",
    "# output_video.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
