{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8035b49",
   "metadata": {
    "id": "PfgBeuAevIXc",
    "outputId": "940c3dce-bd09-4aea-eef9-8c79207167be",
    "papermill": {
     "duration": 2.647323,
     "end_time": "2024-03-24T05:14:57.741239",
     "exception": false,
     "start_time": "2024-03-24T05:14:55.093916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'upar_dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/speckean/upar_dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280684a",
   "metadata": {
    "id": "N-Qkzag4vzzN",
    "papermill": {
     "duration": 0.005476,
     "end_time": "2024-03-24T05:14:57.752895",
     "exception": false,
     "start_time": "2024-03-24T05:14:57.747419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f0f842",
   "metadata": {
    "id": "qPx3W0h-vT97",
    "papermill": {
     "duration": 6.510552,
     "end_time": "2024-03-24T05:15:04.269106",
     "exception": false,
     "start_time": "2024-03-24T05:14:57.758554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5285dcad",
   "metadata": {
    "id": "g3cYG_hs_bhO",
    "outputId": "f79cb5d2-c440-46b4-8db0-44f1532b73ba",
    "papermill": {
     "duration": 0.01563,
     "end_time": "2024-03-24T05:15:04.290855",
     "exception": false,
     "start_time": "2024-03-24T05:15:04.275225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Downloads\\\\LabML\\\\FashionDetection(Week3)\\\\UPAR(Soft-Biometric-Classification)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2745abc",
   "metadata": {
    "id": "lUdsEedzid6v",
    "papermill": {
     "duration": 0.005736,
     "end_time": "2024-03-24T05:15:04.302361",
     "exception": false,
     "start_time": "2024-03-24T05:15:04.296625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f574b4",
   "metadata": {
    "id": "96_bEv_IiiA6",
    "papermill": {
     "duration": 0.005605,
     "end_time": "2024-03-24T05:15:04.314333",
     "exception": false,
     "start_time": "2024-03-24T05:15:04.308728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get a data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80748cdc",
   "metadata": {
    "id": "oRRDnWjIi-zS",
    "outputId": "89e7bd3d-ce24-4a17-c47d-3db23c5491fc",
    "papermill": {
     "duration": 14.449861,
     "end_time": "2024-03-24T05:15:18.769918",
     "exception": false,
     "start_time": "2024-03-24T05:15:04.320057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Downloads\\LabML\\FashionDetection(Week3)\\UPAR(Soft-Biometric-Classification)\\upar_challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'upar_challenge' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: gdown in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: easydict in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (1.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/speckean/upar_challenge.git\n",
    "%cd upar_challenge\n",
    "!pip install tqdm gdown requests easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2e1f4b",
   "metadata": {
    "id": "6vrUotcziocm",
    "outputId": "2d7bdb8c-f538-44be-e008-4aadf50c74ed",
    "papermill": {
     "duration": 48.727697,
     "end_time": "2024-03-24T05:16:07.504705",
     "exception": false,
     "start_time": "2024-03-24T05:15:18.777008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Market 1501 dataset\n",
      "Extract Market 1501 dataset\n",
      "C:\\Users\\USER\\Downloads\\LabML\\FashionDetection(Week3)\\UPAR(Soft-Biometric-Classification)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM\n",
      "From (redirected): https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM&confirm=t&uuid=1c7aa9b8-2cfc-41e1-ba45-db8dcd2e6ac3\n",
      "To: C:\\Users\\USER\\Downloads\\LabML\\FashionDetection(Week3)\\UPAR(Soft-Biometric-Classification)\\upar_challenge\\data\\market_1501.zip\n",
      "\n",
      "  0%|          | 0.00/153M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/153M [00:00<00:46, 3.29MB/s]\n",
      "  2%|1         | 2.62M/153M [00:00<00:12, 11.6MB/s]\n",
      "  3%|3         | 4.72M/153M [00:00<00:19, 7.70MB/s]\n",
      "  5%|5         | 7.86M/153M [00:00<00:11, 12.5MB/s]\n",
      "  7%|7         | 11.0M/153M [00:00<00:08, 16.4MB/s]\n",
      "  9%|9         | 14.2M/153M [00:00<00:06, 20.1MB/s]\n",
      " 11%|#1        | 17.3M/153M [00:01<00:05, 23.0MB/s]\n",
      " 14%|#3        | 21.0M/153M [00:01<00:05, 26.1MB/s]\n",
      " 16%|#6        | 24.6M/153M [00:01<00:04, 28.0MB/s]\n",
      " 19%|#8        | 28.3M/153M [00:01<00:04, 29.3MB/s]\n",
      " 21%|##        | 31.5M/153M [00:01<00:04, 29.7MB/s]\n",
      " 23%|##2       | 35.1M/153M [00:01<00:03, 30.8MB/s]\n",
      " 25%|##5       | 38.8M/153M [00:01<00:03, 31.5MB/s]\n",
      " 28%|##7       | 42.5M/153M [00:01<00:03, 31.6MB/s]\n",
      " 30%|###       | 46.1M/153M [00:02<00:04, 24.7MB/s]\n",
      " 34%|###3      | 51.4M/153M [00:02<00:03, 30.0MB/s]\n",
      " 36%|###6      | 55.1M/153M [00:02<00:03, 30.0MB/s]\n",
      " 38%|###8      | 58.7M/153M [00:02<00:03, 30.7MB/s]\n",
      " 41%|####      | 62.4M/153M [00:02<00:02, 30.2MB/s]\n",
      " 43%|####3     | 66.1M/153M [00:02<00:02, 31.0MB/s]\n",
      " 46%|####5     | 69.7M/153M [00:02<00:02, 31.4MB/s]\n",
      " 48%|####8     | 73.4M/153M [00:02<00:02, 31.1MB/s]\n",
      " 50%|#####     | 77.1M/153M [00:02<00:02, 31.1MB/s]\n",
      " 53%|#####2    | 80.2M/153M [00:03<00:02, 31.0MB/s]\n",
      " 55%|#####4    | 83.4M/153M [00:03<00:02, 29.0MB/s]\n",
      " 57%|#####6    | 86.5M/153M [00:03<00:02, 29.5MB/s]\n",
      " 59%|#####9    | 90.2M/153M [00:03<00:02, 30.1MB/s]\n",
      " 61%|######1   | 93.3M/153M [00:03<00:02, 21.8MB/s]\n",
      " 64%|######3   | 97.0M/153M [00:03<00:02, 24.3MB/s]\n",
      " 66%|######5   | 100M/153M [00:03<00:02, 24.8MB/s] \n",
      " 68%|######7   | 104M/153M [00:04<00:01, 27.0MB/s]\n",
      " 70%|#######   | 107M/153M [00:04<00:01, 28.8MB/s]\n",
      " 72%|#######2  | 111M/153M [00:04<00:01, 27.2MB/s]\n",
      " 75%|#######4  | 114M/153M [00:04<00:01, 28.8MB/s]\n",
      " 77%|#######6  | 117M/153M [00:04<00:01, 28.9MB/s]\n",
      " 79%|#######8  | 121M/153M [00:04<00:01, 29.0MB/s]\n",
      " 81%|########1 | 124M/153M [00:04<00:00, 30.3MB/s]\n",
      " 83%|########3 | 127M/153M [00:04<00:00, 28.4MB/s]\n",
      " 86%|########5 | 131M/153M [00:04<00:00, 29.7MB/s]\n",
      " 88%|########8 | 135M/153M [00:05<00:00, 30.3MB/s]\n",
      " 91%|######### | 138M/153M [00:05<00:00, 31.0MB/s]\n",
      " 93%|#########2| 142M/153M [00:05<00:00, 27.1MB/s]\n",
      " 95%|#########4| 145M/153M [00:05<00:00, 24.3MB/s]\n",
      " 97%|#########7| 148M/153M [00:05<00:00, 26.3MB/s]\n",
      "100%|#########9| 152M/153M [00:05<00:00, 28.1MB/s]\n",
      "100%|##########| 153M/153M [00:05<00:00, 26.7MB/s]\n",
      "\n",
      "Extracting :   0%|          | 0/68042 [00:00<?, ?it/s]\n",
      "Extracting :   0%|          | 178/68042 [00:00<00:38, 1778.37it/s]\n",
      "Extracting :   1%|          | 356/68042 [00:00<00:38, 1737.49it/s]\n",
      "Extracting :   1%|          | 538/68042 [00:00<00:38, 1773.94it/s]\n",
      "Extracting :   1%|1         | 716/68042 [00:00<00:38, 1761.88it/s]\n",
      "Extracting :   1%|1         | 893/68042 [00:00<00:38, 1733.26it/s]\n",
      "Extracting :   2%|1         | 1067/68042 [00:00<00:38, 1734.99it/s]\n",
      "Extracting :   2%|1         | 1243/68042 [00:00<00:38, 1742.55it/s]\n",
      "Extracting :   2%|2         | 1418/68042 [00:00<00:38, 1721.37it/s]\n",
      "Extracting :   2%|2         | 1598/68042 [00:00<00:38, 1744.66it/s]\n",
      "Extracting :   3%|2         | 1774/68042 [00:01<00:37, 1748.84it/s]\n",
      "Extracting :   3%|2         | 1952/68042 [00:01<00:37, 1757.83it/s]\n",
      "Extracting :   3%|3         | 2128/68042 [00:01<00:37, 1747.42it/s]\n",
      "Extracting :   3%|3         | 2303/68042 [00:01<00:37, 1737.28it/s]\n",
      "Extracting :   4%|3         | 2477/68042 [00:01<00:37, 1727.28it/s]\n",
      "Extracting :   4%|3         | 2650/68042 [00:01<00:38, 1717.38it/s]\n",
      "Extracting :   4%|4         | 2822/68042 [00:01<00:38, 1714.80it/s]\n",
      "Extracting :   4%|4         | 2996/68042 [00:01<00:37, 1721.84it/s]\n",
      "Extracting :   5%|4         | 3169/68042 [00:01<00:37, 1723.81it/s]\n",
      "Extracting :   5%|4         | 3342/68042 [00:01<00:37, 1720.05it/s]\n",
      "Extracting :   5%|5         | 3517/68042 [00:02<00:37, 1728.52it/s]\n",
      "Extracting :   5%|5         | 3690/68042 [00:02<00:37, 1713.12it/s]\n",
      "Extracting :   6%|5         | 3862/68042 [00:02<00:37, 1709.62it/s]\n",
      "Extracting :   6%|5         | 4043/68042 [00:02<00:36, 1739.00it/s]\n",
      "Extracting :   6%|6         | 4222/68042 [00:02<00:36, 1753.73it/s]\n",
      "Extracting :   6%|6         | 4398/68042 [00:02<00:36, 1755.13it/s]\n",
      "Extracting :   7%|6         | 4576/68042 [00:02<00:36, 1762.08it/s]\n",
      "Extracting :   7%|6         | 4753/68042 [00:02<00:36, 1732.84it/s]\n",
      "Extracting :   7%|7         | 4927/68042 [00:02<00:36, 1724.29it/s]\n",
      "Extracting :   8%|7         | 5104/68042 [00:02<00:36, 1737.34it/s]\n",
      "Extracting :   8%|7         | 5285/68042 [00:03<00:35, 1756.76it/s]\n",
      "Extracting :   8%|8         | 5467/68042 [00:03<00:35, 1775.10it/s]\n",
      "Extracting :   8%|8         | 5645/68042 [00:03<00:35, 1760.37it/s]\n",
      "Extracting :   9%|8         | 5822/68042 [00:03<00:35, 1747.15it/s]\n",
      "Extracting :   9%|8         | 5997/68042 [00:03<00:35, 1747.52it/s]\n",
      "Extracting :   9%|9         | 6172/68042 [00:03<00:35, 1737.46it/s]\n",
      "Extracting :   9%|9         | 6346/68042 [00:03<00:35, 1722.66it/s]\n",
      "Extracting :  10%|9         | 6520/68042 [00:03<00:35, 1727.32it/s]\n",
      "Extracting :  10%|9         | 6694/68042 [00:03<00:35, 1730.60it/s]\n",
      "Extracting :  10%|#         | 6868/68042 [00:03<00:35, 1712.02it/s]\n",
      "Extracting :  10%|#         | 7046/68042 [00:04<00:35, 1731.67it/s]\n",
      "Extracting :  11%|#         | 7236/68042 [00:04<00:34, 1781.19it/s]\n",
      "Extracting :  11%|#         | 7441/68042 [00:04<00:32, 1860.76it/s]\n",
      "Extracting :  11%|#1        | 7656/68042 [00:04<00:31, 1946.56it/s]\n",
      "Extracting :  12%|#1        | 7856/68042 [00:04<00:30, 1962.00it/s]\n",
      "Extracting :  12%|#1        | 8053/68042 [00:04<00:30, 1945.53it/s]\n",
      "Extracting :  12%|#2        | 8248/68042 [00:04<00:30, 1940.56it/s]\n",
      "Extracting :  12%|#2        | 8443/68042 [00:04<00:30, 1925.65it/s]\n",
      "Extracting :  13%|#2        | 8636/68042 [00:04<00:31, 1909.43it/s]\n",
      "Extracting :  13%|#2        | 8838/68042 [00:04<00:30, 1941.59it/s]\n",
      "Extracting :  13%|#3        | 9043/68042 [00:05<00:29, 1973.23it/s]\n",
      "Extracting :  14%|#3        | 9241/68042 [00:05<00:30, 1957.22it/s]\n",
      "Extracting :  14%|#3        | 9453/68042 [00:05<00:29, 2004.94it/s]\n",
      "Extracting :  14%|#4        | 9654/68042 [00:05<00:29, 2005.90it/s]\n",
      "Extracting :  14%|#4        | 9859/68042 [00:05<00:28, 2018.50it/s]\n",
      "Extracting :  15%|#4        | 10061/68042 [00:05<00:29, 1994.56it/s]\n",
      "Extracting :  15%|#5        | 10261/68042 [00:05<00:28, 1995.63it/s]\n",
      "Extracting :  15%|#5        | 10461/68042 [00:05<00:28, 1990.46it/s]\n",
      "Extracting :  16%|#5        | 10661/68042 [00:05<00:28, 1986.85it/s]\n",
      "Extracting :  16%|#5        | 10870/68042 [00:05<00:28, 2017.00it/s]\n",
      "Extracting :  16%|#6        | 11072/68042 [00:06<00:28, 2011.35it/s]\n",
      "Extracting :  17%|#6        | 11274/68042 [00:06<00:28, 1983.80it/s]\n",
      "Extracting :  17%|#6        | 11473/68042 [00:06<00:29, 1950.49it/s]\n",
      "Extracting :  17%|#7        | 11669/68042 [00:06<00:30, 1834.49it/s]\n",
      "Extracting :  17%|#7        | 11860/68042 [00:06<00:30, 1855.33it/s]\n",
      "Extracting :  18%|#7        | 12067/68042 [00:06<00:29, 1916.50it/s]\n",
      "Extracting :  18%|#8        | 12271/68042 [00:06<00:28, 1951.93it/s]\n",
      "Extracting :  18%|#8        | 12473/68042 [00:06<00:28, 1971.39it/s]\n",
      "Extracting :  19%|#8        | 12674/68042 [00:06<00:27, 1982.26it/s]\n",
      "Extracting :  19%|#8        | 12876/68042 [00:07<00:27, 1992.92it/s]\n",
      "Extracting :  19%|#9        | 13085/68042 [00:07<00:27, 2021.27it/s]\n",
      "Extracting :  20%|#9        | 13288/68042 [00:07<00:27, 2005.37it/s]\n",
      "Extracting :  20%|#9        | 13489/68042 [00:07<00:27, 1994.36it/s]\n",
      "Extracting :  20%|##        | 13690/68042 [00:07<00:27, 1998.43it/s]\n",
      "Extracting :  20%|##        | 13890/68042 [00:07<00:27, 1963.31it/s]\n",
      "Extracting :  21%|##        | 14094/68042 [00:07<00:27, 1985.37it/s]\n",
      "Extracting :  21%|##1       | 14294/68042 [00:07<00:27, 1989.16it/s]\n",
      "Extracting :  21%|##1       | 14506/68042 [00:07<00:26, 2027.49it/s]\n",
      "Extracting :  22%|##1       | 14721/68042 [00:07<00:25, 2063.44it/s]\n",
      "Extracting :  22%|##1       | 14928/68042 [00:08<00:26, 2040.46it/s]\n",
      "Extracting :  22%|##2       | 15133/68042 [00:08<00:26, 2012.88it/s]\n",
      "Extracting :  23%|##2       | 15335/68042 [00:08<00:27, 1951.22it/s]\n",
      "Extracting :  23%|##2       | 15535/68042 [00:08<00:26, 1964.82it/s]\n",
      "Extracting :  23%|##3       | 15738/68042 [00:08<00:26, 1983.35it/s]\n",
      "Extracting :  23%|##3       | 15937/68042 [00:08<00:26, 1950.28it/s]\n",
      "Extracting :  24%|##3       | 16136/68042 [00:08<00:26, 1961.38it/s]\n",
      "Extracting :  24%|##4       | 16333/68042 [00:08<00:26, 1963.39it/s]\n",
      "Extracting :  24%|##4       | 16533/68042 [00:08<00:26, 1973.71it/s]\n",
      "Extracting :  25%|##4       | 16733/68042 [00:08<00:25, 1980.98it/s]\n",
      "Extracting :  25%|##4       | 16932/68042 [00:09<00:26, 1954.00it/s]\n",
      "Extracting :  25%|##5       | 17128/68042 [00:09<00:26, 1921.17it/s]\n",
      "Extracting :  25%|##5       | 17335/68042 [00:09<00:25, 1964.27it/s]\n",
      "Extracting :  26%|##5       | 17538/68042 [00:09<00:25, 1983.11it/s]\n",
      "Extracting :  26%|##6       | 17741/68042 [00:09<00:25, 1996.49it/s]\n",
      "Extracting :  26%|##6       | 17942/68042 [00:09<00:25, 1999.97it/s]\n",
      "Extracting :  27%|##6       | 18143/68042 [00:09<00:24, 1996.47it/s]\n",
      "Extracting :  27%|##6       | 18345/68042 [00:09<00:24, 2002.93it/s]\n",
      "Extracting :  27%|##7       | 18549/68042 [00:09<00:24, 2013.46it/s]\n",
      "Extracting :  28%|##7       | 18751/68042 [00:09<00:24, 2002.88it/s]\n",
      "Extracting :  28%|##7       | 18954/68042 [00:10<00:24, 2010.42it/s]\n",
      "Extracting :  28%|##8       | 19156/68042 [00:10<00:24, 1988.97it/s]\n",
      "Extracting :  28%|##8       | 19355/68042 [00:10<00:24, 1982.80it/s]\n",
      "Extracting :  29%|##8       | 19561/68042 [00:10<00:24, 2005.22it/s]\n",
      "Extracting :  29%|##9       | 19762/68042 [00:10<00:24, 1982.49it/s]\n",
      "Extracting :  29%|##9       | 19961/68042 [00:10<00:24, 1984.18it/s]\n",
      "Extracting :  30%|##9       | 20169/68042 [00:10<00:23, 2012.09it/s]\n",
      "Extracting :  30%|##9       | 20381/68042 [00:10<00:23, 2043.68it/s]\n",
      "Extracting :  30%|###       | 20586/68042 [00:10<00:23, 2009.00it/s]\n",
      "Extracting :  31%|###       | 20788/68042 [00:10<00:23, 1982.44it/s]\n",
      "Extracting :  31%|###       | 20987/68042 [00:11<00:24, 1921.66it/s]\n",
      "Extracting :  31%|###1      | 21180/68042 [00:11<00:24, 1896.22it/s]\n",
      "Extracting :  31%|###1      | 21381/68042 [00:11<00:24, 1928.55it/s]\n",
      "Extracting :  32%|###1      | 21575/68042 [00:11<00:24, 1920.15it/s]\n",
      "Extracting :  32%|###2      | 21775/68042 [00:11<00:23, 1943.10it/s]\n",
      "Extracting :  32%|###2      | 21988/68042 [00:11<00:23, 1997.68it/s]\n",
      "Extracting :  33%|###2      | 22200/68042 [00:11<00:22, 2033.40it/s]\n",
      "Extracting :  33%|###2      | 22409/68042 [00:11<00:22, 2049.74it/s]\n",
      "Extracting :  33%|###3      | 22615/68042 [00:11<00:22, 2046.14it/s]\n",
      "Extracting :  34%|###3      | 22832/68042 [00:12<00:21, 2082.49it/s]\n",
      "Extracting :  34%|###3      | 23044/68042 [00:12<00:21, 2093.07it/s]\n",
      "Extracting :  34%|###4      | 23254/68042 [00:12<00:22, 2027.90it/s]\n",
      "Extracting :  34%|###4      | 23458/68042 [00:12<00:23, 1903.99it/s]\n",
      "Extracting :  35%|###4      | 23662/68042 [00:12<00:22, 1941.77it/s]\n",
      "Extracting :  35%|###5      | 23864/68042 [00:12<00:22, 1963.68it/s]\n",
      "Extracting :  35%|###5      | 24084/68042 [00:12<00:21, 2031.79it/s]\n",
      "Extracting :  36%|###5      | 24290/68042 [00:12<00:21, 2039.52it/s]\n",
      "Extracting :  36%|###6      | 24501/68042 [00:12<00:21, 2059.77it/s]\n",
      "Extracting :  36%|###6      | 24708/68042 [00:12<00:21, 2056.14it/s]\n",
      "Extracting :  37%|###6      | 24917/68042 [00:13<00:20, 2065.64it/s]\n",
      "Extracting :  37%|###6      | 25124/68042 [00:13<00:20, 2066.38it/s]\n",
      "Extracting :  37%|###7      | 25331/68042 [00:13<00:20, 2048.55it/s]\n",
      "Extracting :  38%|###7      | 25552/68042 [00:13<00:20, 2095.93it/s]\n",
      "Extracting :  38%|###7      | 25770/68042 [00:13<00:19, 2120.39it/s]\n",
      "Extracting :  38%|###8      | 25983/68042 [00:13<00:19, 2122.68it/s]\n",
      "Extracting :  38%|###8      | 26196/68042 [00:13<00:20, 2068.63it/s]\n",
      "Extracting :  39%|###8      | 26410/68042 [00:13<00:19, 2089.02it/s]\n",
      "Extracting :  39%|###9      | 26620/68042 [00:13<00:20, 2031.64it/s]\n",
      "Extracting :  39%|###9      | 26824/68042 [00:13<00:20, 1970.48it/s]\n",
      "Extracting :  40%|###9      | 27022/68042 [00:14<00:20, 1955.86it/s]\n",
      "Extracting :  40%|####      | 27218/68042 [00:14<00:21, 1864.90it/s]\n",
      "Extracting :  40%|####      | 27406/68042 [00:14<00:21, 1847.81it/s]\n",
      "Extracting :  41%|####      | 27592/68042 [00:14<00:22, 1765.22it/s]\n",
      "Extracting :  41%|####      | 27776/68042 [00:14<00:22, 1785.68it/s]\n",
      "Extracting :  41%|####1     | 27956/68042 [00:14<00:22, 1759.16it/s]\n",
      "Extracting :  41%|####1     | 28144/68042 [00:14<00:22, 1793.26it/s]\n",
      "Extracting :  42%|####1     | 28334/68042 [00:14<00:21, 1823.75it/s]\n",
      "Extracting :  42%|####1     | 28517/68042 [00:14<00:21, 1825.09it/s]\n",
      "Extracting :  42%|####2     | 28711/68042 [00:15<00:21, 1858.49it/s]\n",
      "Extracting :  42%|####2     | 28911/68042 [00:15<00:20, 1899.88it/s]\n",
      "Extracting :  43%|####2     | 29102/68042 [00:15<00:20, 1869.04it/s]\n",
      "Extracting :  43%|####3     | 29305/68042 [00:15<00:20, 1915.92it/s]\n",
      "Extracting :  43%|####3     | 29503/68042 [00:15<00:19, 1934.37it/s]\n",
      "Extracting :  44%|####3     | 29703/68042 [00:15<00:19, 1953.35it/s]\n",
      "Extracting :  44%|####3     | 29900/68042 [00:15<00:19, 1957.78it/s]\n",
      "Extracting :  44%|####4     | 30097/68042 [00:15<00:19, 1960.89it/s]\n",
      "Extracting :  45%|####4     | 30294/68042 [00:15<00:20, 1847.08it/s]\n",
      "Extracting :  45%|####4     | 30487/68042 [00:15<00:20, 1870.35it/s]\n",
      "Extracting :  45%|####5     | 30683/68042 [00:16<00:19, 1895.88it/s]\n",
      "Extracting :  45%|####5     | 30874/68042 [00:16<00:19, 1882.95it/s]\n",
      "Extracting :  46%|####5     | 31063/68042 [00:16<00:19, 1851.90it/s]\n",
      "Extracting :  46%|####5     | 31257/68042 [00:16<00:19, 1877.09it/s]\n",
      "Extracting :  46%|####6     | 31450/68042 [00:16<00:19, 1892.14it/s]\n",
      "Extracting :  47%|####6     | 31648/68042 [00:16<00:18, 1917.61it/s]\n",
      "Extracting :  47%|####6     | 31862/68042 [00:16<00:18, 1983.11it/s]\n",
      "Extracting :  47%|####7     | 32065/68042 [00:16<00:18, 1996.54it/s]\n",
      "Extracting :  47%|####7     | 32273/68042 [00:16<00:17, 2020.89it/s]\n",
      "Extracting :  48%|####7     | 32476/68042 [00:16<00:18, 1911.40it/s]\n",
      "Extracting :  48%|####8     | 32681/68042 [00:17<00:18, 1950.72it/s]\n",
      "Extracting :  48%|####8     | 32878/68042 [00:17<00:18, 1858.42it/s]\n",
      "Extracting :  49%|####8     | 33081/68042 [00:17<00:18, 1906.50it/s]\n",
      "Extracting :  49%|####8     | 33283/68042 [00:17<00:17, 1938.67it/s]\n",
      "Extracting :  49%|####9     | 33496/68042 [00:17<00:17, 1993.83it/s]\n",
      "Extracting :  50%|####9     | 33697/68042 [00:17<00:17, 1986.43it/s]\n",
      "Extracting :  50%|####9     | 33903/68042 [00:17<00:17, 2007.55it/s]\n",
      "Extracting :  50%|#####     | 34105/68042 [00:17<00:17, 1992.98it/s]\n",
      "Extracting :  50%|#####     | 34315/68042 [00:17<00:16, 2024.05it/s]\n",
      "Extracting :  51%|#####     | 34520/68042 [00:18<00:16, 2031.19it/s]\n",
      "Extracting :  51%|#####1    | 34724/68042 [00:18<00:16, 1985.98it/s]\n",
      "Extracting :  51%|#####1    | 34923/68042 [00:18<00:16, 1985.24it/s]\n",
      "Extracting :  52%|#####1    | 35126/68042 [00:18<00:16, 1997.92it/s]\n",
      "Extracting :  52%|#####1    | 35340/68042 [00:18<00:16, 2039.50it/s]\n",
      "Extracting :  52%|#####2    | 35545/68042 [00:18<00:16, 1927.33it/s]\n",
      "Extracting :  53%|#####2    | 35743/68042 [00:18<00:16, 1941.90it/s]\n",
      "Extracting :  53%|#####2    | 35947/68042 [00:18<00:16, 1969.84it/s]\n",
      "Extracting :  53%|#####3    | 36163/68042 [00:18<00:15, 2025.02it/s]\n",
      "Extracting :  53%|#####3    | 36377/68042 [00:18<00:15, 2058.39it/s]\n",
      "Extracting :  54%|#####3    | 36586/68042 [00:19<00:15, 2067.20it/s]\n",
      "Extracting :  54%|#####4    | 36801/68042 [00:19<00:14, 2091.26it/s]\n",
      "Extracting :  54%|#####4    | 37011/68042 [00:19<00:15, 2068.61it/s]\n",
      "Extracting :  55%|#####4    | 37219/68042 [00:19<00:15, 2035.24it/s]\n",
      "Extracting :  55%|#####5    | 37429/68042 [00:19<00:14, 2053.70it/s]\n",
      "Extracting :  55%|#####5    | 37635/68042 [00:19<00:14, 2042.93it/s]\n",
      "Extracting :  56%|#####5    | 37840/68042 [00:19<00:14, 2044.47it/s]\n",
      "Extracting :  56%|#####5    | 38045/68042 [00:19<00:14, 2039.50it/s]\n",
      "Extracting :  56%|#####6    | 38250/68042 [00:19<00:15, 1949.36it/s]\n",
      "Extracting :  57%|#####6    | 38446/68042 [00:19<00:15, 1924.17it/s]\n",
      "Extracting :  57%|#####6    | 38652/68042 [00:20<00:14, 1962.86it/s]\n",
      "Extracting :  57%|#####7    | 38866/68042 [00:20<00:14, 2014.03it/s]\n",
      "Extracting :  57%|#####7    | 39071/68042 [00:20<00:14, 2024.08it/s]\n",
      "Extracting :  58%|#####7    | 39276/68042 [00:20<00:14, 2031.21it/s]\n",
      "Extracting :  58%|#####8    | 39491/68042 [00:20<00:13, 2065.94it/s]\n",
      "Extracting :  58%|#####8    | 39698/68042 [00:20<00:13, 2048.27it/s]\n",
      "Extracting :  59%|#####8    | 39904/68042 [00:20<00:13, 2021.20it/s]\n",
      "Extracting :  59%|#####8    | 40118/68042 [00:20<00:13, 2055.64it/s]\n",
      "Extracting :  59%|#####9    | 40324/68042 [00:20<00:13, 2056.38it/s]\n",
      "Extracting :  60%|#####9    | 40530/68042 [00:20<00:13, 2056.90it/s]\n",
      "Extracting :  60%|#####9    | 40736/68042 [00:21<00:13, 2033.02it/s]\n",
      "Extracting :  60%|######    | 40940/68042 [00:21<00:13, 1964.69it/s]\n",
      "Extracting :  60%|######    | 41137/68042 [00:21<00:14, 1803.52it/s]\n",
      "Extracting :  61%|######    | 41337/68042 [00:21<00:14, 1857.15it/s]\n",
      "Extracting :  61%|######1   | 41533/68042 [00:21<00:14, 1885.92it/s]\n",
      "Extracting :  61%|######1   | 41732/68042 [00:21<00:13, 1915.32it/s]\n",
      "Extracting :  62%|######1   | 41941/68042 [00:21<00:13, 1965.62it/s]\n",
      "Extracting :  62%|######1   | 42140/68042 [00:21<00:13, 1972.24it/s]\n",
      "Extracting :  62%|######2   | 42345/68042 [00:21<00:12, 1994.69it/s]\n",
      "Extracting :  63%|######2   | 42557/68042 [00:22<00:12, 2031.31it/s]\n",
      "Extracting :  63%|######2   | 42761/68042 [00:22<00:12, 2027.32it/s]\n",
      "Extracting :  63%|######3   | 42965/68042 [00:22<00:12, 1971.82it/s]\n",
      "Extracting :  63%|######3   | 43167/68042 [00:22<00:12, 1985.37it/s]\n",
      "Extracting :  64%|######3   | 43370/68042 [00:22<00:12, 1997.98it/s]\n",
      "Extracting :  64%|######4   | 43571/68042 [00:22<00:12, 1971.87it/s]\n",
      "Extracting :  64%|######4   | 43769/68042 [00:22<00:13, 1848.61it/s]\n",
      "Extracting :  65%|######4   | 43962/68042 [00:22<00:12, 1871.06it/s]\n",
      "Extracting :  65%|######4   | 44153/68042 [00:22<00:12, 1881.80it/s]\n",
      "Extracting :  65%|######5   | 44343/68042 [00:22<00:12, 1881.13it/s]\n",
      "Extracting :  65%|######5   | 44536/68042 [00:23<00:12, 1894.93it/s]\n",
      "Extracting :  66%|######5   | 44732/68042 [00:23<00:12, 1913.62it/s]\n",
      "Extracting :  66%|######6   | 44933/68042 [00:23<00:11, 1941.63it/s]\n",
      "Extracting :  66%|######6   | 45129/68042 [00:23<00:11, 1946.61it/s]\n",
      "Extracting :  67%|######6   | 45324/68042 [00:23<00:11, 1947.02it/s]\n",
      "Extracting :  67%|######6   | 45519/68042 [00:23<00:11, 1935.88it/s]\n",
      "Extracting :  67%|######7   | 45717/68042 [00:23<00:11, 1948.48it/s]\n",
      "Extracting :  67%|######7   | 45912/68042 [00:23<00:12, 1837.10it/s]\n",
      "Extracting :  68%|######7   | 46107/68042 [00:23<00:11, 1868.97it/s]\n",
      "Extracting :  68%|######8   | 46295/68042 [00:24<00:12, 1758.50it/s]\n",
      "Extracting :  68%|######8   | 46500/68042 [00:24<00:11, 1839.78it/s]\n",
      "Extracting :  69%|######8   | 46696/68042 [00:24<00:11, 1873.67it/s]\n",
      "Extracting :  69%|######8   | 46885/68042 [00:24<00:11, 1872.48it/s]\n",
      "Extracting :  69%|######9   | 47094/68042 [00:24<00:10, 1935.54it/s]\n",
      "Extracting :  70%|######9   | 47298/68042 [00:24<00:10, 1965.78it/s]\n",
      "Extracting :  70%|######9   | 47496/68042 [00:24<00:10, 1963.64it/s]\n",
      "Extracting :  70%|#######   | 47703/68042 [00:24<00:10, 1994.65it/s]\n",
      "Extracting :  70%|#######   | 47920/68042 [00:24<00:09, 2046.27it/s]\n",
      "Extracting :  71%|#######   | 48125/68042 [00:24<00:09, 2034.68it/s]\n",
      "Extracting :  71%|#######1  | 48329/68042 [00:25<00:09, 2035.72it/s]\n",
      "Extracting :  71%|#######1  | 48545/68042 [00:25<00:09, 2072.21it/s]\n",
      "Extracting :  72%|#######1  | 48753/68042 [00:25<00:09, 2049.47it/s]\n",
      "Extracting :  72%|#######1  | 48959/68042 [00:25<00:09, 1931.69it/s]\n",
      "Extracting :  72%|#######2  | 49160/68042 [00:25<00:09, 1953.50it/s]\n",
      "Extracting :  73%|#######2  | 49367/68042 [00:25<00:09, 1986.68it/s]\n",
      "Extracting :  73%|#######2  | 49569/68042 [00:25<00:09, 1995.89it/s]\n",
      "Extracting :  73%|#######3  | 49770/68042 [00:25<00:09, 1993.63it/s]\n",
      "Extracting :  73%|#######3  | 49971/68042 [00:25<00:09, 1997.94it/s]\n",
      "Extracting :  74%|#######3  | 50172/68042 [00:25<00:08, 2000.92it/s]\n",
      "Extracting :  74%|#######4  | 50373/68042 [00:26<00:08, 1991.27it/s]\n",
      "Extracting :  74%|#######4  | 50573/68042 [00:26<00:08, 1975.70it/s]\n",
      "Extracting :  75%|#######4  | 50771/68042 [00:26<00:09, 1914.08it/s]\n",
      "Extracting :  75%|#######4  | 50966/68042 [00:26<00:08, 1924.01it/s]\n",
      "Extracting :  75%|#######5  | 51166/68042 [00:26<00:08, 1945.77it/s]\n",
      "Extracting :  75%|#######5  | 51361/68042 [00:26<00:08, 1940.76it/s]\n",
      "Extracting :  76%|#######5  | 51556/68042 [00:26<00:08, 1865.63it/s]\n",
      "Extracting :  76%|#######6  | 51758/68042 [00:26<00:08, 1909.69it/s]\n",
      "Extracting :  76%|#######6  | 51952/68042 [00:26<00:08, 1917.99it/s]\n",
      "Extracting :  77%|#######6  | 52145/68042 [00:26<00:08, 1915.41it/s]\n",
      "Extracting :  77%|#######6  | 52337/68042 [00:27<00:08, 1916.26it/s]\n",
      "Extracting :  77%|#######7  | 52533/68042 [00:27<00:08, 1928.71it/s]\n",
      "Extracting :  78%|#######7  | 52734/68042 [00:27<00:07, 1952.38it/s]\n",
      "Extracting :  78%|#######7  | 52930/68042 [00:27<00:07, 1948.30it/s]\n",
      "Extracting :  78%|#######8  | 53125/68042 [00:27<00:07, 1942.48it/s]\n",
      "Extracting :  78%|#######8  | 53334/68042 [00:27<00:07, 1985.89it/s]\n",
      "Extracting :  79%|#######8  | 53533/68042 [00:27<00:07, 1980.68it/s]\n",
      "Extracting :  79%|#######8  | 53732/68042 [00:27<00:07, 1965.32it/s]\n",
      "Extracting :  79%|#######9  | 53932/68042 [00:27<00:07, 1975.08it/s]\n",
      "Extracting :  80%|#######9  | 54141/68042 [00:27<00:06, 2008.76it/s]\n",
      "Extracting :  80%|#######9  | 54342/68042 [00:28<00:07, 1890.08it/s]\n",
      "Extracting :  80%|########  | 54546/68042 [00:28<00:06, 1932.49it/s]\n",
      "Extracting :  80%|########  | 54741/68042 [00:28<00:06, 1909.34it/s]\n",
      "Extracting :  81%|########  | 54933/68042 [00:28<00:06, 1906.40it/s]\n",
      "Extracting :  81%|########1 | 55128/68042 [00:28<00:06, 1918.63it/s]\n",
      "Extracting :  81%|########1 | 55336/68042 [00:28<00:06, 1965.63it/s]\n",
      "Extracting :  82%|########1 | 55535/68042 [00:28<00:06, 1972.31it/s]\n",
      "Extracting :  82%|########1 | 55738/68042 [00:28<00:06, 1988.92it/s]\n",
      "Extracting :  82%|########2 | 55945/68042 [00:28<00:06, 2012.54it/s]\n",
      "Extracting :  83%|########2 | 56147/68042 [00:29<00:06, 1978.76it/s]\n",
      "Extracting :  83%|########2 | 56346/68042 [00:29<00:05, 1969.90it/s]\n",
      "Extracting :  83%|########3 | 56544/68042 [00:29<00:05, 1955.02it/s]\n",
      "Extracting :  83%|########3 | 56740/68042 [00:29<00:06, 1862.89it/s]\n",
      "Extracting :  84%|########3 | 56928/68042 [00:29<00:06, 1825.48it/s]\n",
      "Extracting :  84%|########3 | 57128/68042 [00:29<00:05, 1874.82it/s]\n",
      "Extracting :  84%|########4 | 57329/68042 [00:29<00:05, 1911.15it/s]\n",
      "Extracting :  85%|########4 | 57536/68042 [00:29<00:05, 1957.09it/s]\n",
      "Extracting :  85%|########4 | 57733/68042 [00:29<00:05, 1943.18it/s]\n",
      "Extracting :  85%|########5 | 57928/68042 [00:29<00:05, 1811.31it/s]\n",
      "Extracting :  85%|########5 | 58124/68042 [00:30<00:05, 1852.67it/s]\n",
      "Extracting :  86%|########5 | 58316/68042 [00:30<00:05, 1871.48it/s]\n",
      "Extracting :  86%|########5 | 58514/68042 [00:30<00:05, 1902.51it/s]\n",
      "Extracting :  86%|########6 | 58708/68042 [00:30<00:04, 1912.99it/s]\n",
      "Extracting :  87%|########6 | 58902/68042 [00:30<00:04, 1920.44it/s]\n",
      "Extracting :  87%|########6 | 59095/68042 [00:30<00:04, 1917.08it/s]\n",
      "Extracting :  87%|########7 | 59289/68042 [00:30<00:04, 1923.37it/s]\n",
      "Extracting :  87%|########7 | 59482/68042 [00:30<00:04, 1863.73it/s]\n",
      "Extracting :  88%|########7 | 59679/68042 [00:30<00:04, 1894.23it/s]\n",
      "Extracting :  88%|########7 | 59876/68042 [00:30<00:04, 1916.00it/s]\n",
      "Extracting :  88%|########8 | 60080/68042 [00:31<00:04, 1952.15it/s]\n",
      "Extracting :  89%|########8 | 60276/68042 [00:31<00:03, 1942.41it/s]\n",
      "Extracting :  89%|########8 | 60471/68042 [00:31<00:04, 1877.28it/s]\n",
      "Extracting :  89%|########9 | 60660/68042 [00:31<00:03, 1880.48it/s]\n",
      "Extracting :  89%|########9 | 60865/68042 [00:31<00:03, 1929.65it/s]\n",
      "Extracting :  90%|########9 | 61075/68042 [00:31<00:03, 1979.37it/s]\n",
      "Extracting :  90%|######### | 61286/68042 [00:31<00:03, 2017.56it/s]\n",
      "Extracting :  90%|######### | 61502/68042 [00:31<00:03, 2059.38it/s]\n",
      "Extracting :  91%|######### | 61741/68042 [00:31<00:02, 2157.37it/s]\n",
      "Extracting :  91%|#########1| 61957/68042 [00:32<00:02, 2151.14it/s]\n",
      "Extracting :  91%|#########1| 62173/68042 [00:32<00:02, 2060.90it/s]\n",
      "Extracting :  92%|#########1| 62380/68042 [00:32<00:02, 2051.11it/s]\n",
      "Extracting :  92%|#########1| 62597/68042 [00:32<00:02, 2085.21it/s]\n",
      "Extracting :  92%|#########2| 62825/68042 [00:32<00:02, 2141.87it/s]\n",
      "Extracting :  93%|#########2| 63044/68042 [00:32<00:02, 2155.54it/s]\n",
      "Extracting :  93%|#########2| 63272/68042 [00:32<00:02, 2191.83it/s]\n",
      "Extracting :  93%|#########3| 63517/68042 [00:32<00:01, 2268.16it/s]\n",
      "Extracting :  94%|#########3| 63747/68042 [00:32<00:01, 2277.05it/s]\n",
      "Extracting :  94%|#########4| 63979/68042 [00:32<00:01, 2289.25it/s]\n",
      "Extracting :  94%|#########4| 64209/68042 [00:33<00:01, 2244.74it/s]\n",
      "Extracting :  95%|#########4| 64434/68042 [00:33<00:01, 2232.59it/s]\n",
      "Extracting :  95%|#########5| 64658/68042 [00:33<00:01, 2208.08it/s]\n",
      "Extracting :  95%|#########5| 64879/68042 [00:33<00:01, 2188.74it/s]\n",
      "Extracting :  96%|#########5| 65104/68042 [00:33<00:01, 2206.15it/s]\n",
      "Extracting :  96%|#########6| 65325/68042 [00:33<00:01, 2060.39it/s]\n",
      "Extracting :  96%|#########6| 65533/68042 [00:33<00:01, 2019.68it/s]\n",
      "Extracting :  97%|#########6| 65751/68042 [00:33<00:01, 2064.53it/s]\n",
      "Extracting :  97%|#########6| 65959/68042 [00:33<00:01, 2027.56it/s]\n",
      "Extracting :  97%|#########7| 66163/68042 [00:33<00:00, 2007.51it/s]\n",
      "Extracting :  98%|#########7| 66365/68042 [00:34<00:00, 1999.05it/s]\n",
      "Extracting :  98%|#########7| 66567/68042 [00:34<00:00, 2004.61it/s]\n",
      "Extracting :  98%|#########8| 66768/68042 [00:34<00:00, 1988.14it/s]\n",
      "Extracting :  98%|#########8| 66972/68042 [00:34<00:00, 2002.77it/s]\n",
      "Extracting :  99%|#########8| 67196/68042 [00:34<00:00, 2072.33it/s]\n",
      "Extracting :  99%|#########9| 67404/68042 [00:34<00:00, 2008.46it/s]\n",
      "Extracting :  99%|#########9| 67606/68042 [00:34<00:00, 2011.29it/s]\n",
      "Extracting : 100%|#########9| 67808/68042 [00:34<00:00, 1984.16it/s]\n",
      "Extracting : 100%|#########9| 68007/68042 [00:34<00:00, 1830.09it/s]\n",
      "Extracting : 100%|##########| 68042/68042 [00:34<00:00, 1946.59it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"download_datasets.py\", line 147, in <module>\n",
      "    prepare_datasets(args.data_dir)\n",
      "  File \"download_datasets.py\", line 123, in prepare_datasets\n",
      "    prepare_market(dataset_path)\n",
      "  File \"download_datasets.py\", line 64, in prepare_market\n",
      "    Path(dataset_path / \"Market-1501-v15.09.15\").rename(market_1501_path)\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\Yolo\\lib\\pathlib.py\", line 1359, in rename\n",
      "    self._accessor.rename(self, target)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'data\\\\Market-1501-v15.09.15' -> 'data\\\\Market1501'\n",
      "Access is denied.\n"
     ]
    }
   ],
   "source": [
    "!python download_datasets.py\n",
    "%cd {HOME}\n",
    "!move upar_challenge/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4fd210",
   "metadata": {
    "id": "PYxNcR4XyKL0",
    "outputId": "457e5b22-fb5d-466e-faf5-334207115f4b",
    "papermill": {
     "duration": 0.08005,
     "end_time": "2024-03-24T05:16:07.658732",
     "exception": false,
     "start_time": "2024-03-24T05:16:07.578682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'data/PA100k/release_data/release_data' does not exist.\n"
     ]
    }
   ],
   "source": [
    "current_dir_path = 'data/PA100k/release_data/release_data'\n",
    "new_dir_path = 'data/PA100k/data'\n",
    "\n",
    "\n",
    "if os.path.exists(current_dir_path):\n",
    "    if not os.path.exists(new_dir_path):\n",
    "        os.rename(current_dir_path, new_dir_path)\n",
    "        print(f\"Directory renamed from '{current_dir_path}' to '{new_dir_path}'\")\n",
    "\n",
    "    else:\n",
    "        print(f\"The target directory '{new_dir_path}' already exists.\")\n",
    "else:\n",
    "    print(f\"The directory '{current_dir_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21086f2a",
   "metadata": {
    "id": "Ipq3tXRkijzw",
    "papermill": {
     "duration": 0.068064,
     "end_time": "2024-03-24T05:16:07.799718",
     "exception": false,
     "start_time": "2024-03-24T05:16:07.731654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Construct dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd343740",
   "metadata": {
    "id": "7Y15r_o5v07f",
    "papermill": {
     "duration": 0.091063,
     "end_time": "2024-03-24T05:16:07.967420",
     "exception": false,
     "start_time": "2024-03-24T05:16:07.876357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class UPAR(data.Dataset):\n",
    "  \"\"\"\n",
    "  Load UPAR dataset from pickle file\n",
    "\n",
    "  split: whether to use train/val/trainval/test split\n",
    "  partition: partition id 0-9\n",
    "  root: path to datasets, original datasets must be in this directory\n",
    "  data_path: path to UPAR pickle file\n",
    "  transform: training data transforms\n",
    "  target_transforms: evaluation data transforms\n",
    "  \"\"\"\n",
    "  def __init__(self, split='train', partition=0, root=HOME, data_path='upar_dataset/UPAR/dataset_all.pkl', transform=None, target_transform=None):\n",
    "    dataset_info = pickle.load(open(data_path, 'rb+'))\n",
    "    self.dataset_info = dataset_info\n",
    "\n",
    "    img_id = dataset_info.image_name\n",
    "    attr_label = dataset_info.label\n",
    "\n",
    "    assert split in dataset_info.partition.keys(), f'split {split} does not exist'\n",
    "\n",
    "    self.dataset = 'UPAR'\n",
    "    self.transform = transform  # data transforms during training\n",
    "    self.target_transform = target_transform  # data transforms during testing\n",
    "\n",
    "    self.root_path = root+\"/data\"  # path to datasets\n",
    "\n",
    "    self.attr_id = dataset_info.attr_name  # attribute names\n",
    "    self.attr_num = len(self.attr_id)  # number of attributes\n",
    "\n",
    "\t\t# load partition\n",
    "    self.img_idx = dataset_info.partition[split]\n",
    "\n",
    "    if isinstance(self.img_idx, list):\n",
    "      self.img_idx = self.img_idx[partition]\n",
    "\n",
    "    if isinstance(self.img_idx, list):\n",
    "      self.img_idx = np.hstack(self.img_idx)\n",
    "\n",
    "    self.img_idx = np.array([i for i in self.img_idx if not any(folder in img_id[i] for folder in [\"RAP\"])])\n",
    "\n",
    "\n",
    "    self.img_num = self.img_idx.shape[0]\n",
    "    self.img_id = [img_id[i] for i in self.img_idx]\n",
    "    self.label = attr_label[self.img_idx]\n",
    "\n",
    "    # set sub-dataset lengths to enable evaluation on sub-datasets\n",
    "    self.sub_dataset_lengths = [len(d) for d in dataset_info.partition.test[partition]]\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      \"\"\"\n",
    "      get dataset item by index\n",
    "\n",
    "      index: item index\n",
    "      return: image data (img) with corresponding ground truth labels (gt_label), dataset id (did), and image path (imgname)\n",
    "      \"\"\"\n",
    "      imgname, gt_label, imgidx = self.img_id[index], self.label[index], self.img_idx[index]\n",
    "      did = self.dataset_info.dataset_ids[imgidx]\n",
    "      imgpath = os.path.join(self.root_path, imgname)\n",
    "      img = Image.open(imgpath)\n",
    "\n",
    "      if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "\n",
    "      gt_label = gt_label.astype(np.float32)\n",
    "\n",
    "      if self.target_transform is not None:\n",
    "          gt_label = self.transform(gt_label)\n",
    "\n",
    "      return img, gt_label, did, imgname\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    get length of dataset\n",
    "    \"\"\"\n",
    "    return len(self.img_id)\n",
    "\n",
    "\n",
    "def get_transform(height, width):\n",
    "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    train_transform = [\n",
    "        T.Resize((height, width))\n",
    "    ]\n",
    "\n",
    "    train_transform += [\n",
    "        T.Pad(10),\n",
    "        T.RandomCrop((height, width)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "    ]\n",
    "\n",
    "    train_transform += [\n",
    "        T.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    "    train_transform = T.Compose(train_transform)\n",
    "\n",
    "    valid_transform = T.Compose([\n",
    "        T.Resize((height, width)),\n",
    "        T.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    return train_transform, valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70911fb",
   "metadata": {
    "id": "C__-utMewC83",
    "papermill": {
     "duration": 0.96624,
     "end_time": "2024-03-24T05:16:09.006464",
     "exception": false,
     "start_time": "2024-03-24T05:16:08.040224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "train_transform, valid_transform = get_transform(height, width)\n",
    "\n",
    "\n",
    "train_dataset = UPAR(split='train', partition=0, transform=train_transform)\n",
    "val_dataset = UPAR(split='val', partition=0, transform=valid_transform)\n",
    "test_dataset = UPAR(split='test', partition=0, transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d289f3b4",
   "metadata": {
    "id": "bAwTtSBfwpBn",
    "papermill": {
     "duration": 0.080362,
     "end_time": "2024-03-24T05:16:09.153745",
     "exception": false,
     "start_time": "2024-03-24T05:16:09.073383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce3361",
   "metadata": {
    "id": "zYqxrG3p_sc0",
    "papermill": {
     "duration": 0.071181,
     "end_time": "2024-03-24T05:16:09.303562",
     "exception": false,
     "start_time": "2024-03-24T05:16:09.232381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbf30c6",
   "metadata": {
    "id": "We87CAdU-cww",
    "papermill": {
     "duration": 0.511964,
     "end_time": "2024-03-24T05:16:09.882262",
     "exception": false,
     "start_time": "2024-03-24T05:16:09.370298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e21a83f0",
   "metadata": {
    "id": "NrK7JzSxDzZD",
    "papermill": {
     "duration": 0.076892,
     "end_time": "2024-03-24T05:16:10.026732",
     "exception": false,
     "start_time": "2024-03-24T05:16:09.949840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 40\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(model.last_channel, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f7e054",
   "metadata": {
    "id": "guhqNIXxHkHS",
    "outputId": "b6849ff6-53fc-4bac-97b2-8d80d4211774",
    "papermill": {
     "duration": 2555.285983,
     "end_time": "2024-03-24T05:58:45.379531",
     "exception": false,
     "start_time": "2024-03-24T05:16:10.093548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Batch 0, Loss: 25.841175079345703\n",
      "Batch 100, Loss: 24.357088088989258\n",
      "Batch 200, Loss: 21.19321060180664\n",
      "Batch 300, Loss: 21.68756103515625\n",
      "Batch 400, Loss: 20.53506851196289\n",
      "Batch 500, Loss: 22.070159912109375\n",
      "Batch 600, Loss: 22.718292236328125\n",
      "Batch 700, Loss: 21.468585968017578\n",
      "Batch 800, Loss: 22.594327926635742\n",
      "Batch 900, Loss: 20.596675872802734\n",
      "Batch 1000, Loss: 21.22633171081543\n",
      "Batch 1100, Loss: 22.505311965942383\n",
      "Batch 1200, Loss: 22.17971420288086\n",
      "Batch 1300, Loss: 20.360164642333984\n",
      "Batch 1400, Loss: 21.76951789855957\n",
      "Batch 1500, Loss: 20.807865142822266\n",
      "Batch 1600, Loss: 20.776824951171875\n",
      "Batch 1700, Loss: 22.453277587890625\n",
      "Batch 1800, Loss: 21.05763053894043\n",
      "Batch 1900, Loss: 21.46938705444336\n",
      "Batch 2000, Loss: 21.30282211303711\n",
      "Batch 2100, Loss: 21.58887481689453\n",
      "Batch 2200, Loss: 21.445358276367188\n",
      "Batch 2300, Loss: 21.745054244995117\n",
      "Batch 2400, Loss: 20.860084533691406\n",
      "Batch 2500, Loss: 21.121145248413086\n",
      "Batch 2600, Loss: 20.813039779663086\n",
      "Batch 2700, Loss: 21.85641860961914\n",
      "Batch 2800, Loss: 21.505992889404297\n",
      "Batch 2900, Loss: 21.158763885498047\n",
      "Batch 3000, Loss: 22.381885528564453\n",
      "Batch 3100, Loss: 21.75775718688965\n",
      "Average Training Loss: 21.84618669611807\n",
      "Average Validation Loss: 22.37751705088514\n",
      "Epoch 2/100\n",
      "----------\n",
      "Batch 0, Loss: 21.881668090820312\n",
      "Batch 100, Loss: 21.155288696289062\n",
      "Batch 200, Loss: 22.5124568939209\n",
      "Batch 300, Loss: 21.582841873168945\n",
      "Batch 400, Loss: 21.731830596923828\n",
      "Batch 500, Loss: 20.056528091430664\n",
      "Batch 600, Loss: 21.450347900390625\n",
      "Batch 700, Loss: 22.1678524017334\n",
      "Batch 800, Loss: 21.52767562866211\n",
      "Batch 900, Loss: 22.891132354736328\n",
      "Batch 1000, Loss: 21.605152130126953\n",
      "Batch 1100, Loss: 20.957530975341797\n",
      "Batch 1200, Loss: 20.389524459838867\n",
      "Batch 1300, Loss: 21.71523666381836\n",
      "Batch 1400, Loss: 21.789133071899414\n",
      "Batch 1500, Loss: 21.951610565185547\n",
      "Batch 1600, Loss: 21.92145347595215\n",
      "Batch 1700, Loss: 21.253711700439453\n",
      "Batch 1800, Loss: 21.280805587768555\n",
      "Batch 1900, Loss: 22.449216842651367\n",
      "Batch 2000, Loss: 22.09926414489746\n",
      "Batch 2100, Loss: 21.315692901611328\n",
      "Batch 2200, Loss: 21.59462547302246\n",
      "Batch 2300, Loss: 20.706436157226562\n",
      "Batch 2400, Loss: 20.984691619873047\n",
      "Batch 2500, Loss: 21.509227752685547\n",
      "Batch 2600, Loss: 22.22992515563965\n",
      "Batch 2700, Loss: 22.021163940429688\n",
      "Batch 2800, Loss: 22.48504066467285\n",
      "Batch 2900, Loss: 21.395240783691406\n",
      "Batch 3000, Loss: 21.080644607543945\n",
      "Batch 3100, Loss: 21.95956039428711\n",
      "Average Training Loss: 21.6000279263989\n",
      "Average Validation Loss: 22.233504806680884\n",
      "Epoch 3/100\n",
      "----------\n",
      "Batch 0, Loss: 21.56722068786621\n",
      "Batch 100, Loss: 22.036279678344727\n",
      "Batch 200, Loss: 21.492685317993164\n",
      "Batch 300, Loss: 21.2157039642334\n",
      "Batch 400, Loss: 21.962921142578125\n",
      "Batch 500, Loss: 21.954036712646484\n",
      "Batch 600, Loss: 21.9390869140625\n",
      "Batch 700, Loss: 22.5970401763916\n",
      "Batch 800, Loss: 21.047462463378906\n",
      "Batch 900, Loss: 20.495943069458008\n",
      "Batch 1000, Loss: 22.251787185668945\n",
      "Batch 1100, Loss: 19.680824279785156\n",
      "Batch 1200, Loss: 22.113962173461914\n",
      "Batch 1300, Loss: 21.631881713867188\n",
      "Batch 1400, Loss: 21.41259002685547\n",
      "Batch 1500, Loss: 20.79920196533203\n",
      "Batch 1600, Loss: 21.554534912109375\n",
      "Batch 1700, Loss: 21.23360824584961\n",
      "Batch 1800, Loss: 21.4572811126709\n",
      "Batch 1900, Loss: 21.103506088256836\n",
      "Batch 2000, Loss: 20.81218719482422\n",
      "Batch 2100, Loss: 21.0306396484375\n",
      "Batch 2200, Loss: 21.76654815673828\n",
      "Batch 2300, Loss: 21.03954315185547\n",
      "Batch 2400, Loss: 21.564741134643555\n",
      "Batch 2500, Loss: 20.977909088134766\n",
      "Batch 2600, Loss: 20.750762939453125\n",
      "Batch 2700, Loss: 21.26740264892578\n",
      "Batch 2800, Loss: 22.22432518005371\n",
      "Batch 2900, Loss: 20.693626403808594\n",
      "Batch 3000, Loss: 22.063465118408203\n",
      "Batch 3100, Loss: 21.503787994384766\n",
      "Average Training Loss: 21.487978262452376\n",
      "Average Validation Loss: 22.172541249052006\n",
      "Epoch 4/100\n",
      "----------\n",
      "Batch 0, Loss: 21.239105224609375\n",
      "Batch 100, Loss: 20.966875076293945\n",
      "Batch 200, Loss: 21.370525360107422\n",
      "Batch 300, Loss: 21.644577026367188\n",
      "Batch 400, Loss: 21.642539978027344\n",
      "Batch 500, Loss: 21.661184310913086\n",
      "Batch 600, Loss: 20.44959259033203\n",
      "Batch 700, Loss: 20.882431030273438\n",
      "Batch 800, Loss: 20.926589965820312\n",
      "Batch 900, Loss: 22.344409942626953\n",
      "Batch 1000, Loss: 20.288902282714844\n",
      "Batch 1100, Loss: 21.93079948425293\n",
      "Batch 1200, Loss: 22.18178939819336\n",
      "Batch 1300, Loss: 20.908798217773438\n",
      "Batch 1400, Loss: 21.69773292541504\n",
      "Batch 1500, Loss: 21.25162124633789\n",
      "Batch 1600, Loss: 20.9774169921875\n",
      "Batch 1700, Loss: 22.358184814453125\n",
      "Batch 1800, Loss: 21.958232879638672\n",
      "Batch 1900, Loss: 20.79408836364746\n",
      "Batch 2000, Loss: 21.861255645751953\n",
      "Batch 2100, Loss: 21.44337272644043\n",
      "Batch 2200, Loss: 21.52878189086914\n",
      "Batch 2300, Loss: 20.646686553955078\n",
      "Batch 2400, Loss: 22.136516571044922\n",
      "Batch 2500, Loss: 20.436784744262695\n",
      "Batch 2600, Loss: 20.614009857177734\n",
      "Batch 2700, Loss: 20.720746994018555\n",
      "Batch 2800, Loss: 21.105363845825195\n",
      "Batch 2900, Loss: 20.440399169921875\n",
      "Batch 3000, Loss: 21.46695327758789\n",
      "Batch 3100, Loss: 19.832921981811523\n",
      "Average Training Loss: 21.412875546753863\n",
      "Average Validation Loss: 22.1080445228739\n",
      "Epoch 5/100\n",
      "----------\n",
      "Batch 0, Loss: 20.113740921020508\n",
      "Batch 100, Loss: 20.953475952148438\n",
      "Batch 200, Loss: 21.119823455810547\n",
      "Batch 300, Loss: 21.97522735595703\n",
      "Batch 400, Loss: 21.922069549560547\n",
      "Batch 500, Loss: 20.584102630615234\n",
      "Batch 600, Loss: 21.552658081054688\n",
      "Batch 700, Loss: 21.208988189697266\n",
      "Batch 800, Loss: 21.23052215576172\n",
      "Batch 900, Loss: 20.930347442626953\n",
      "Batch 1000, Loss: 21.900684356689453\n",
      "Batch 1100, Loss: 21.55512237548828\n",
      "Batch 1200, Loss: 21.873966217041016\n",
      "Batch 1300, Loss: 21.85323143005371\n",
      "Batch 1400, Loss: 22.0988712310791\n",
      "Batch 1500, Loss: 20.587350845336914\n",
      "Batch 1600, Loss: 21.248821258544922\n",
      "Batch 1700, Loss: 23.480411529541016\n",
      "Batch 1800, Loss: 21.541492462158203\n",
      "Batch 1900, Loss: 21.60186004638672\n",
      "Batch 2000, Loss: 22.210552215576172\n",
      "Batch 2100, Loss: 20.901561737060547\n",
      "Batch 2200, Loss: 20.205486297607422\n",
      "Batch 2300, Loss: 20.244930267333984\n",
      "Batch 2400, Loss: 22.700815200805664\n",
      "Batch 2500, Loss: 22.122282028198242\n",
      "Batch 2600, Loss: 21.812061309814453\n",
      "Batch 2700, Loss: 21.575109481811523\n",
      "Batch 2800, Loss: 20.455867767333984\n",
      "Batch 2900, Loss: 20.927467346191406\n",
      "Batch 3000, Loss: 22.25579833984375\n",
      "Batch 3100, Loss: 20.807025909423828\n",
      "Average Training Loss: 21.36111381945719\n",
      "Average Validation Loss: 22.08019433123\n",
      "Epoch 6/100\n",
      "----------\n",
      "Batch 0, Loss: 22.678119659423828\n",
      "Batch 100, Loss: 20.1219539642334\n",
      "Batch 200, Loss: 21.060972213745117\n",
      "Batch 300, Loss: 20.66663932800293\n",
      "Batch 400, Loss: 21.728927612304688\n",
      "Batch 500, Loss: 22.084272384643555\n",
      "Batch 600, Loss: 21.64361572265625\n",
      "Batch 700, Loss: 21.900264739990234\n",
      "Batch 800, Loss: 21.322750091552734\n",
      "Batch 900, Loss: 22.175922393798828\n",
      "Batch 1000, Loss: 20.865644454956055\n",
      "Batch 1100, Loss: 20.554420471191406\n",
      "Batch 1200, Loss: 22.499980926513672\n",
      "Batch 1300, Loss: 21.411523818969727\n",
      "Batch 1400, Loss: 20.12462043762207\n",
      "Batch 1500, Loss: 21.360685348510742\n",
      "Batch 1600, Loss: 21.607666015625\n",
      "Batch 1700, Loss: 21.93091583251953\n",
      "Batch 1800, Loss: 21.439889907836914\n",
      "Batch 1900, Loss: 20.96080780029297\n",
      "Batch 2000, Loss: 20.031051635742188\n",
      "Batch 2100, Loss: 21.083271026611328\n",
      "Batch 2200, Loss: 21.65215301513672\n",
      "Batch 2300, Loss: 20.296985626220703\n",
      "Batch 2400, Loss: 21.95024299621582\n",
      "Batch 2500, Loss: 20.770801544189453\n",
      "Batch 2600, Loss: 20.891403198242188\n",
      "Batch 2700, Loss: 22.83575439453125\n",
      "Batch 2800, Loss: 21.501876831054688\n",
      "Batch 2900, Loss: 21.617355346679688\n",
      "Batch 3000, Loss: 21.809873580932617\n",
      "Batch 3100, Loss: 20.046470642089844\n",
      "Average Training Loss: 21.327330331462637\n",
      "Average Validation Loss: 22.05622901510685\n",
      "Epoch 7/100\n",
      "----------\n",
      "Batch 0, Loss: 21.068424224853516\n",
      "Batch 100, Loss: 21.443599700927734\n",
      "Batch 200, Loss: 21.37567901611328\n",
      "Batch 300, Loss: 19.863784790039062\n",
      "Batch 400, Loss: 21.231002807617188\n",
      "Batch 500, Loss: 22.054302215576172\n",
      "Batch 600, Loss: 21.929466247558594\n",
      "Batch 700, Loss: 22.33249282836914\n",
      "Batch 800, Loss: 22.222698211669922\n",
      "Batch 900, Loss: 20.940309524536133\n",
      "Batch 1000, Loss: 21.260395050048828\n",
      "Batch 1100, Loss: 21.27127456665039\n",
      "Batch 1200, Loss: 20.493022918701172\n",
      "Batch 1300, Loss: 20.778976440429688\n",
      "Batch 1400, Loss: 21.010616302490234\n",
      "Batch 1500, Loss: 20.76457405090332\n",
      "Batch 1600, Loss: 21.38608169555664\n",
      "Batch 1700, Loss: 21.150625228881836\n",
      "Batch 1800, Loss: 21.651283264160156\n",
      "Batch 1900, Loss: 22.163469314575195\n",
      "Batch 2000, Loss: 20.367969512939453\n",
      "Batch 2100, Loss: 21.225975036621094\n",
      "Batch 2200, Loss: 20.23666000366211\n",
      "Batch 2300, Loss: 21.628562927246094\n",
      "Batch 2400, Loss: 20.51046371459961\n",
      "Batch 2500, Loss: 21.5916748046875\n",
      "Batch 2600, Loss: 20.11191177368164\n",
      "Batch 2700, Loss: 20.92542839050293\n",
      "Batch 2800, Loss: 21.653974533081055\n",
      "Batch 2900, Loss: 20.993019104003906\n",
      "Batch 3000, Loss: 21.888229370117188\n",
      "Batch 3100, Loss: 21.381229400634766\n",
      "Average Training Loss: 21.299184781601106\n",
      "Average Validation Loss: 22.049553867096595\n",
      "Epoch 8/100\n",
      "----------\n",
      "Batch 0, Loss: 21.542377471923828\n",
      "Batch 100, Loss: 20.750913619995117\n",
      "Batch 200, Loss: 19.89633560180664\n",
      "Batch 300, Loss: 20.844818115234375\n",
      "Batch 400, Loss: 21.454133987426758\n",
      "Batch 500, Loss: 21.574190139770508\n",
      "Batch 600, Loss: 19.711624145507812\n",
      "Batch 700, Loss: 21.382583618164062\n",
      "Batch 800, Loss: 21.97393035888672\n",
      "Batch 900, Loss: 22.415428161621094\n",
      "Batch 1000, Loss: 20.849857330322266\n",
      "Batch 1100, Loss: 20.140209197998047\n",
      "Batch 1200, Loss: 21.480915069580078\n",
      "Batch 1300, Loss: 20.5939998626709\n",
      "Batch 1400, Loss: 21.19322967529297\n",
      "Batch 1500, Loss: 21.239482879638672\n",
      "Batch 1600, Loss: 22.4782657623291\n",
      "Batch 1700, Loss: 21.223377227783203\n",
      "Batch 1800, Loss: 21.097063064575195\n",
      "Batch 1900, Loss: 21.883663177490234\n",
      "Batch 2000, Loss: 21.22199058532715\n",
      "Batch 2100, Loss: 22.489818572998047\n",
      "Batch 2200, Loss: 21.72409439086914\n",
      "Batch 2300, Loss: 20.819101333618164\n",
      "Batch 2400, Loss: 21.744213104248047\n",
      "Batch 2500, Loss: 21.635276794433594\n",
      "Batch 2600, Loss: 21.40481185913086\n",
      "Batch 2700, Loss: 20.658843994140625\n",
      "Batch 2800, Loss: 21.387767791748047\n",
      "Batch 2900, Loss: 20.083721160888672\n",
      "Batch 3000, Loss: 22.060544967651367\n",
      "Batch 3100, Loss: 21.942646026611328\n",
      "Average Training Loss: 21.278951959148923\n",
      "Average Validation Loss: 22.037430276262\n",
      "Epoch 9/100\n",
      "----------\n",
      "Batch 0, Loss: 20.893680572509766\n",
      "Batch 100, Loss: 22.400039672851562\n",
      "Batch 200, Loss: 20.470199584960938\n",
      "Batch 300, Loss: 20.403085708618164\n",
      "Batch 400, Loss: 20.711654663085938\n",
      "Batch 500, Loss: 21.362060546875\n",
      "Batch 600, Loss: 22.335224151611328\n",
      "Batch 700, Loss: 22.790855407714844\n",
      "Batch 800, Loss: 21.377525329589844\n",
      "Batch 900, Loss: 21.962486267089844\n",
      "Batch 1000, Loss: 21.875289916992188\n",
      "Batch 1100, Loss: 21.103025436401367\n",
      "Batch 1200, Loss: 21.917255401611328\n",
      "Batch 1300, Loss: 21.067251205444336\n",
      "Batch 1400, Loss: 20.44268798828125\n",
      "Batch 1500, Loss: 21.14751434326172\n",
      "Batch 1600, Loss: 21.625532150268555\n",
      "Batch 1700, Loss: 21.45587158203125\n",
      "Batch 1800, Loss: 21.250267028808594\n",
      "Batch 1900, Loss: 21.255882263183594\n",
      "Batch 2000, Loss: 21.350048065185547\n",
      "Batch 2100, Loss: 20.67696189880371\n",
      "Batch 2200, Loss: 20.407981872558594\n",
      "Batch 2300, Loss: 21.60093879699707\n",
      "Batch 2400, Loss: 21.269752502441406\n",
      "Batch 2500, Loss: 20.366615295410156\n",
      "Batch 2600, Loss: 20.672107696533203\n",
      "Batch 2700, Loss: 22.01638412475586\n",
      "Batch 2800, Loss: 20.786216735839844\n",
      "Batch 2900, Loss: 21.358787536621094\n",
      "Batch 3000, Loss: 21.524044036865234\n",
      "Batch 3100, Loss: 21.169214248657227\n",
      "Average Training Loss: 21.265567214737715\n",
      "Average Validation Loss: 22.02964853327325\n",
      "Epoch 10/100\n",
      "----------\n",
      "Batch 0, Loss: 21.7901554107666\n",
      "Batch 100, Loss: 20.351057052612305\n",
      "Batch 200, Loss: 20.68997573852539\n",
      "Batch 300, Loss: 21.671525955200195\n",
      "Batch 400, Loss: 21.291187286376953\n",
      "Batch 500, Loss: 21.37428092956543\n",
      "Batch 600, Loss: 21.390277862548828\n",
      "Batch 700, Loss: 22.20482635498047\n",
      "Batch 800, Loss: 21.08128547668457\n",
      "Batch 900, Loss: 21.56999969482422\n",
      "Batch 1000, Loss: 21.747692108154297\n",
      "Batch 1100, Loss: 22.473621368408203\n",
      "Batch 1200, Loss: 21.23465347290039\n",
      "Batch 1300, Loss: 19.942138671875\n",
      "Batch 1400, Loss: 21.28864288330078\n",
      "Batch 1500, Loss: 21.475250244140625\n",
      "Batch 1600, Loss: 20.958600997924805\n",
      "Batch 1700, Loss: 21.183549880981445\n",
      "Batch 1800, Loss: 20.70388412475586\n",
      "Batch 1900, Loss: 21.366741180419922\n",
      "Batch 2000, Loss: 21.15443992614746\n",
      "Batch 2100, Loss: 20.754478454589844\n",
      "Batch 2200, Loss: 20.865726470947266\n",
      "Batch 2300, Loss: 20.746932983398438\n",
      "Batch 2400, Loss: 20.93359375\n",
      "Batch 2500, Loss: 20.814971923828125\n",
      "Batch 2600, Loss: 21.11336898803711\n",
      "Batch 2700, Loss: 20.279157638549805\n",
      "Batch 2800, Loss: 21.760231018066406\n",
      "Batch 2900, Loss: 20.685897827148438\n",
      "Batch 3000, Loss: 21.362701416015625\n",
      "Batch 3100, Loss: 21.087482452392578\n",
      "Average Training Loss: 21.254290965373887\n",
      "Average Validation Loss: 22.02677664249501\n",
      "Epoch 11/100\n",
      "----------\n",
      "Batch 0, Loss: 21.01734161376953\n",
      "Batch 100, Loss: 22.551685333251953\n",
      "Batch 200, Loss: 20.89080047607422\n",
      "Batch 300, Loss: 22.21721649169922\n",
      "Batch 400, Loss: 21.32175064086914\n",
      "Batch 500, Loss: 21.055206298828125\n",
      "Batch 600, Loss: 20.580501556396484\n",
      "Batch 700, Loss: 22.539518356323242\n",
      "Batch 800, Loss: 21.959259033203125\n",
      "Batch 900, Loss: 20.660627365112305\n",
      "Batch 1000, Loss: 22.24724578857422\n",
      "Batch 1100, Loss: 20.322893142700195\n",
      "Batch 1200, Loss: 21.735435485839844\n",
      "Batch 1300, Loss: 21.631267547607422\n",
      "Batch 1400, Loss: 20.40298080444336\n",
      "Batch 1500, Loss: 21.796672821044922\n",
      "Batch 1600, Loss: 21.498245239257812\n",
      "Batch 1700, Loss: 21.611751556396484\n",
      "Batch 1800, Loss: 22.291446685791016\n",
      "Batch 1900, Loss: 20.763809204101562\n",
      "Batch 2000, Loss: 21.720943450927734\n",
      "Batch 2100, Loss: 21.811912536621094\n",
      "Batch 2200, Loss: 21.54262924194336\n",
      "Batch 2300, Loss: 22.897193908691406\n",
      "Batch 2400, Loss: 20.530447006225586\n",
      "Batch 2500, Loss: 21.739593505859375\n",
      "Batch 2600, Loss: 20.692771911621094\n",
      "Batch 2700, Loss: 21.658483505249023\n",
      "Batch 2800, Loss: 21.441862106323242\n",
      "Batch 2900, Loss: 22.182708740234375\n",
      "Batch 3000, Loss: 19.957542419433594\n",
      "Batch 3100, Loss: 21.681583404541016\n",
      "Average Training Loss: 21.247354446779983\n",
      "Average Validation Loss: 22.02907563067497\n",
      "Epoch 12/100\n",
      "----------\n",
      "Batch 0, Loss: 21.21170425415039\n",
      "Batch 100, Loss: 21.32909393310547\n",
      "Batch 200, Loss: 20.93008804321289\n",
      "Batch 300, Loss: 20.260665893554688\n",
      "Batch 400, Loss: 22.278316497802734\n",
      "Batch 500, Loss: 22.122764587402344\n",
      "Batch 600, Loss: 21.71966552734375\n",
      "Batch 700, Loss: 21.792057037353516\n",
      "Batch 800, Loss: 20.85049819946289\n",
      "Batch 900, Loss: 20.770248413085938\n",
      "Batch 1000, Loss: 20.934947967529297\n",
      "Batch 1100, Loss: 21.43122100830078\n",
      "Batch 1200, Loss: 19.99526596069336\n",
      "Batch 1300, Loss: 20.665998458862305\n",
      "Batch 1400, Loss: 20.700462341308594\n",
      "Batch 1500, Loss: 20.756351470947266\n",
      "Batch 1600, Loss: 19.946748733520508\n",
      "Batch 1700, Loss: 21.741043090820312\n",
      "Batch 1800, Loss: 21.262435913085938\n",
      "Batch 1900, Loss: 21.872032165527344\n",
      "Batch 2000, Loss: 22.229267120361328\n",
      "Batch 2100, Loss: 20.349727630615234\n",
      "Batch 2200, Loss: 20.81013298034668\n",
      "Batch 2300, Loss: 19.250877380371094\n",
      "Batch 2400, Loss: 21.724990844726562\n",
      "Batch 2500, Loss: 21.90660285949707\n",
      "Batch 2600, Loss: 21.601388931274414\n",
      "Batch 2700, Loss: 20.737247467041016\n",
      "Batch 2800, Loss: 21.12806510925293\n",
      "Batch 2900, Loss: 21.50884246826172\n",
      "Batch 3000, Loss: 21.914031982421875\n",
      "Batch 3100, Loss: 21.83057403564453\n",
      "Average Training Loss: 21.24201090766577\n",
      "Average Validation Loss: 22.023109119496446\n",
      "Epoch 13/100\n",
      "----------\n",
      "Batch 0, Loss: 21.473018646240234\n",
      "Batch 100, Loss: 19.561439514160156\n",
      "Batch 200, Loss: 20.361881256103516\n",
      "Batch 300, Loss: 20.927825927734375\n",
      "Batch 400, Loss: 22.455814361572266\n",
      "Batch 500, Loss: 20.99124526977539\n",
      "Batch 600, Loss: 20.406766891479492\n",
      "Batch 700, Loss: 21.394813537597656\n",
      "Batch 800, Loss: 22.13329315185547\n",
      "Batch 900, Loss: 20.609092712402344\n",
      "Batch 1000, Loss: 21.496978759765625\n",
      "Batch 1100, Loss: 22.637968063354492\n",
      "Batch 1200, Loss: 21.660057067871094\n",
      "Batch 1300, Loss: 21.583864212036133\n",
      "Batch 1400, Loss: 20.54339599609375\n",
      "Batch 1500, Loss: 20.833389282226562\n",
      "Batch 1600, Loss: 21.304344177246094\n",
      "Batch 1700, Loss: 21.464006423950195\n",
      "Batch 1800, Loss: 22.686294555664062\n",
      "Batch 1900, Loss: 22.117534637451172\n",
      "Batch 2000, Loss: 22.801746368408203\n",
      "Batch 2100, Loss: 20.936124801635742\n",
      "Batch 2200, Loss: 20.604698181152344\n",
      "Batch 2300, Loss: 20.553985595703125\n",
      "Batch 2400, Loss: 20.512697219848633\n",
      "Batch 2500, Loss: 21.317785263061523\n",
      "Batch 2600, Loss: 21.24631118774414\n",
      "Batch 2700, Loss: 19.742124557495117\n",
      "Batch 2800, Loss: 22.282203674316406\n",
      "Batch 2900, Loss: 21.854419708251953\n",
      "Batch 3000, Loss: 20.799619674682617\n",
      "Batch 3100, Loss: 20.563180923461914\n",
      "Average Training Loss: 21.236875760342937\n",
      "Average Validation Loss: 22.025980032251237\n",
      "Epoch 14/100\n",
      "----------\n",
      "Batch 0, Loss: 20.512744903564453\n",
      "Batch 100, Loss: 20.02886199951172\n",
      "Batch 200, Loss: 21.350664138793945\n",
      "Batch 300, Loss: 21.35489845275879\n",
      "Batch 400, Loss: 21.444110870361328\n",
      "Batch 500, Loss: 22.30370330810547\n",
      "Batch 600, Loss: 20.224599838256836\n",
      "Batch 700, Loss: 23.282020568847656\n",
      "Batch 800, Loss: 21.594791412353516\n",
      "Batch 900, Loss: 21.98172378540039\n",
      "Batch 1000, Loss: 21.293418884277344\n",
      "Batch 1100, Loss: 21.956504821777344\n",
      "Batch 1200, Loss: 19.98244857788086\n",
      "Batch 1300, Loss: 21.152482986450195\n",
      "Batch 1400, Loss: 20.951202392578125\n",
      "Batch 1500, Loss: 20.344526290893555\n",
      "Batch 1600, Loss: 19.834972381591797\n",
      "Batch 1700, Loss: 21.607635498046875\n",
      "Batch 1800, Loss: 20.903484344482422\n",
      "Batch 1900, Loss: 22.247129440307617\n",
      "Batch 2000, Loss: 22.031496047973633\n",
      "Batch 2100, Loss: 21.236295700073242\n",
      "Batch 2200, Loss: 21.184947967529297\n",
      "Batch 2300, Loss: 20.17806625366211\n",
      "Batch 2400, Loss: 22.011642456054688\n",
      "Batch 2500, Loss: 20.924278259277344\n",
      "Batch 2600, Loss: 20.452089309692383\n",
      "Batch 2700, Loss: 20.575687408447266\n",
      "Batch 2800, Loss: 21.421188354492188\n",
      "Batch 2900, Loss: 21.156452178955078\n",
      "Batch 3000, Loss: 21.01296615600586\n",
      "Batch 3100, Loss: 21.91483497619629\n",
      "Average Training Loss: 21.235312352047014\n",
      "Average Validation Loss: 22.02533859090602\n",
      "Epoch 15/100\n",
      "----------\n",
      "Batch 0, Loss: 20.83987808227539\n",
      "Batch 100, Loss: 20.650827407836914\n",
      "Batch 200, Loss: 21.454368591308594\n",
      "Batch 300, Loss: 20.6271915435791\n",
      "Batch 400, Loss: 20.907302856445312\n",
      "Batch 500, Loss: 22.58310317993164\n",
      "Batch 600, Loss: 21.76932144165039\n",
      "Batch 700, Loss: 20.735042572021484\n",
      "Batch 800, Loss: 21.97477912902832\n",
      "Batch 900, Loss: 20.38037872314453\n",
      "Batch 1000, Loss: 21.091541290283203\n",
      "Batch 1100, Loss: 20.504756927490234\n",
      "Batch 1200, Loss: 21.154367446899414\n",
      "Batch 1300, Loss: 20.716705322265625\n",
      "Batch 1400, Loss: 20.676807403564453\n",
      "Batch 1500, Loss: 21.079078674316406\n",
      "Batch 1600, Loss: 20.22699546813965\n",
      "Batch 1700, Loss: 21.884117126464844\n",
      "Batch 1800, Loss: 20.393110275268555\n",
      "Batch 1900, Loss: 20.920970916748047\n",
      "Batch 2000, Loss: 20.245853424072266\n",
      "Batch 2100, Loss: 21.203144073486328\n",
      "Batch 2200, Loss: 20.39337158203125\n",
      "Batch 2300, Loss: 21.579029083251953\n",
      "Batch 2400, Loss: 21.370498657226562\n",
      "Batch 2500, Loss: 22.275314331054688\n",
      "Batch 2600, Loss: 21.597368240356445\n",
      "Batch 2700, Loss: 21.41073989868164\n",
      "Batch 2800, Loss: 21.364221572875977\n",
      "Batch 2900, Loss: 21.060361862182617\n",
      "Batch 3000, Loss: 20.79915428161621\n",
      "Batch 3100, Loss: 20.91355323791504\n",
      "Average Training Loss: 21.23276973559353\n",
      "Average Validation Loss: 22.028937530517577\n",
      "Epoch 16/100\n",
      "----------\n",
      "Batch 0, Loss: 21.238014221191406\n",
      "Batch 100, Loss: 22.665687561035156\n",
      "Batch 200, Loss: 21.13666343688965\n",
      "Batch 300, Loss: 21.207365036010742\n",
      "Batch 400, Loss: 21.321779251098633\n",
      "Batch 500, Loss: 22.595548629760742\n",
      "Batch 600, Loss: 21.356948852539062\n",
      "Batch 700, Loss: 20.645954132080078\n",
      "Batch 800, Loss: 21.710031509399414\n",
      "Batch 900, Loss: 21.907655715942383\n",
      "Batch 1000, Loss: 21.56599998474121\n",
      "Batch 1100, Loss: 22.09276008605957\n",
      "Batch 1200, Loss: 22.239519119262695\n",
      "Batch 1300, Loss: 21.811376571655273\n",
      "Batch 1400, Loss: 20.005664825439453\n",
      "Batch 1500, Loss: 21.3802433013916\n",
      "Batch 1600, Loss: 21.322105407714844\n",
      "Batch 1700, Loss: 20.111284255981445\n",
      "Batch 1800, Loss: 20.34193229675293\n",
      "Batch 1900, Loss: 21.650405883789062\n",
      "Batch 2000, Loss: 20.43871307373047\n",
      "Batch 2100, Loss: 21.317493438720703\n",
      "Batch 2200, Loss: 21.35471534729004\n",
      "Batch 2300, Loss: 21.02474594116211\n",
      "Batch 2400, Loss: 21.502513885498047\n",
      "Batch 2500, Loss: 21.903690338134766\n",
      "Batch 2600, Loss: 21.072315216064453\n",
      "Batch 2700, Loss: 21.900514602661133\n",
      "Batch 2800, Loss: 21.306087493896484\n",
      "Batch 2900, Loss: 21.65882110595703\n",
      "Batch 3000, Loss: 21.316192626953125\n",
      "Batch 3100, Loss: 21.40973472595215\n",
      "Average Training Loss: 21.23171850439853\n",
      "Average Validation Loss: 22.026448704334015\n",
      "Epoch 17/100\n",
      "----------\n",
      "Batch 0, Loss: 22.237104415893555\n",
      "Batch 100, Loss: 23.420236587524414\n",
      "Batch 200, Loss: 20.90842056274414\n",
      "Batch 300, Loss: 20.769390106201172\n",
      "Batch 400, Loss: 20.753023147583008\n",
      "Batch 500, Loss: 21.83510971069336\n",
      "Batch 600, Loss: 19.44808578491211\n",
      "Batch 700, Loss: 21.188995361328125\n",
      "Batch 800, Loss: 20.920536041259766\n",
      "Batch 900, Loss: 21.839204788208008\n",
      "Batch 1000, Loss: 19.85016632080078\n",
      "Batch 1100, Loss: 20.39238739013672\n",
      "Batch 1200, Loss: 21.847747802734375\n",
      "Batch 1300, Loss: 22.233686447143555\n",
      "Batch 1400, Loss: 22.28752899169922\n",
      "Batch 1500, Loss: 20.611732482910156\n",
      "Batch 1600, Loss: 22.11475372314453\n",
      "Batch 1700, Loss: 20.91058349609375\n",
      "Batch 1800, Loss: 20.993288040161133\n",
      "Batch 1900, Loss: 21.752111434936523\n",
      "Batch 2000, Loss: 20.69501495361328\n",
      "Batch 2100, Loss: 21.343910217285156\n",
      "Batch 2200, Loss: 21.84266471862793\n",
      "Batch 2300, Loss: 20.8835391998291\n",
      "Batch 2400, Loss: 21.785295486450195\n",
      "Batch 2500, Loss: 21.010440826416016\n",
      "Batch 2600, Loss: 21.220006942749023\n",
      "Batch 2700, Loss: 20.563106536865234\n",
      "Batch 2800, Loss: 21.386253356933594\n",
      "Batch 2900, Loss: 21.5255126953125\n",
      "Batch 3000, Loss: 21.564834594726562\n",
      "Batch 3100, Loss: 20.59386444091797\n",
      "Average Training Loss: 21.230779989378448\n",
      "Average Validation Loss: 22.023631558519728\n",
      "Epoch 18/100\n",
      "----------\n",
      "Batch 0, Loss: 20.316030502319336\n",
      "Batch 100, Loss: 20.205717086791992\n",
      "Batch 200, Loss: 21.735790252685547\n",
      "Batch 300, Loss: 21.399404525756836\n",
      "Batch 400, Loss: 20.349842071533203\n",
      "Batch 500, Loss: 20.88207244873047\n",
      "Batch 600, Loss: 20.538013458251953\n",
      "Batch 700, Loss: 21.959117889404297\n",
      "Batch 800, Loss: 20.190242767333984\n",
      "Batch 900, Loss: 21.27822494506836\n",
      "Batch 1000, Loss: 21.259376525878906\n",
      "Batch 1100, Loss: 21.49627113342285\n",
      "Batch 1200, Loss: 21.746599197387695\n",
      "Batch 1300, Loss: 20.467731475830078\n",
      "Batch 1400, Loss: 20.651517868041992\n",
      "Batch 1500, Loss: 20.90113067626953\n",
      "Batch 1600, Loss: 21.258811950683594\n",
      "Batch 1700, Loss: 21.026832580566406\n",
      "Batch 1800, Loss: 22.09853172302246\n",
      "Batch 1900, Loss: 21.687732696533203\n",
      "Batch 2000, Loss: 20.771873474121094\n",
      "Batch 2100, Loss: 22.55324363708496\n",
      "Batch 2200, Loss: 21.138408660888672\n",
      "Batch 2300, Loss: 21.161237716674805\n",
      "Batch 2400, Loss: 21.808452606201172\n",
      "Batch 2500, Loss: 22.225067138671875\n",
      "Batch 2600, Loss: 20.964427947998047\n",
      "Batch 2700, Loss: 20.675922393798828\n",
      "Batch 2800, Loss: 21.203392028808594\n",
      "Batch 2900, Loss: 21.206584930419922\n",
      "Batch 3000, Loss: 21.28701400756836\n",
      "Batch 3100, Loss: 22.248214721679688\n",
      "Average Training Loss: 21.229387100719617\n",
      "Average Validation Loss: 22.02370902528154\n",
      "Epoch 19/100\n",
      "----------\n",
      "Batch 0, Loss: 21.144214630126953\n",
      "Batch 100, Loss: 20.958683013916016\n",
      "Batch 200, Loss: 22.086034774780273\n",
      "Batch 300, Loss: 19.910728454589844\n",
      "Batch 400, Loss: 20.427875518798828\n",
      "Batch 500, Loss: 20.27977180480957\n",
      "Batch 600, Loss: 21.64766502380371\n",
      "Batch 700, Loss: 21.374286651611328\n",
      "Batch 800, Loss: 21.112133026123047\n",
      "Batch 900, Loss: 21.873619079589844\n",
      "Batch 1000, Loss: 21.593460083007812\n",
      "Batch 1100, Loss: 22.796388626098633\n",
      "Batch 1200, Loss: 21.439388275146484\n",
      "Batch 1300, Loss: 20.085826873779297\n",
      "Batch 1400, Loss: 20.423383712768555\n",
      "Batch 1500, Loss: 20.034555435180664\n",
      "Batch 1600, Loss: 20.655921936035156\n",
      "Batch 1700, Loss: 21.458059310913086\n",
      "Batch 1800, Loss: 21.635021209716797\n",
      "Batch 1900, Loss: 22.78037452697754\n",
      "Batch 2000, Loss: 20.670684814453125\n",
      "Batch 2100, Loss: 21.734926223754883\n",
      "Batch 2200, Loss: 21.85906219482422\n",
      "Batch 2300, Loss: 20.520084381103516\n",
      "Batch 2400, Loss: 20.143028259277344\n",
      "Batch 2500, Loss: 21.196212768554688\n",
      "Batch 2600, Loss: 21.28717803955078\n",
      "Batch 2700, Loss: 21.43764877319336\n",
      "Batch 2800, Loss: 20.92000961303711\n",
      "Batch 2900, Loss: 20.759485244750977\n",
      "Batch 3000, Loss: 20.597896575927734\n",
      "Batch 3100, Loss: 21.761463165283203\n",
      "Average Training Loss: 21.229781636148314\n",
      "Average Validation Loss: 22.023820256172343\n",
      "Epoch 20/100\n",
      "----------\n",
      "Batch 0, Loss: 20.616086959838867\n",
      "Batch 100, Loss: 20.89759063720703\n",
      "Batch 200, Loss: 21.26543617248535\n",
      "Batch 300, Loss: 20.338443756103516\n",
      "Batch 400, Loss: 20.893959045410156\n",
      "Batch 500, Loss: 21.065549850463867\n",
      "Batch 600, Loss: 20.93977928161621\n",
      "Batch 700, Loss: 21.18456268310547\n",
      "Batch 800, Loss: 22.38006591796875\n",
      "Batch 900, Loss: 21.272411346435547\n",
      "Batch 1000, Loss: 22.97595977783203\n",
      "Batch 1100, Loss: 21.19463539123535\n",
      "Batch 1200, Loss: 21.117694854736328\n",
      "Batch 1300, Loss: 21.146621704101562\n",
      "Batch 1400, Loss: 21.516639709472656\n",
      "Batch 1500, Loss: 20.922019958496094\n",
      "Batch 1600, Loss: 21.07923126220703\n",
      "Batch 1700, Loss: 21.999221801757812\n",
      "Batch 1800, Loss: 21.80855369567871\n",
      "Batch 1900, Loss: 20.761737823486328\n",
      "Batch 2000, Loss: 21.324302673339844\n",
      "Batch 2100, Loss: 21.842830657958984\n",
      "Batch 2200, Loss: 20.735445022583008\n",
      "Batch 2300, Loss: 20.278587341308594\n",
      "Batch 2400, Loss: 19.888315200805664\n",
      "Batch 2500, Loss: 21.225845336914062\n",
      "Batch 2600, Loss: 22.9729061126709\n",
      "Batch 2700, Loss: 20.996370315551758\n",
      "Batch 2800, Loss: 21.47638702392578\n",
      "Batch 2900, Loss: 21.761825561523438\n",
      "Batch 3000, Loss: 21.018815994262695\n",
      "Batch 3100, Loss: 19.697904586791992\n",
      "Average Training Loss: 21.22915647351408\n",
      "Average Validation Loss: 22.021732618453655\n",
      "Epoch 21/100\n",
      "----------\n",
      "Batch 0, Loss: 20.906156539916992\n",
      "Batch 100, Loss: 21.185691833496094\n",
      "Batch 200, Loss: 20.771488189697266\n",
      "Batch 300, Loss: 21.13104820251465\n",
      "Batch 400, Loss: 22.26418685913086\n",
      "Batch 500, Loss: 20.980615615844727\n",
      "Batch 600, Loss: 21.44788360595703\n",
      "Batch 700, Loss: 21.135417938232422\n",
      "Batch 800, Loss: 21.87879180908203\n",
      "Batch 900, Loss: 20.98537254333496\n",
      "Batch 1000, Loss: 21.749832153320312\n",
      "Batch 1100, Loss: 21.34307861328125\n",
      "Batch 1200, Loss: 22.03651237487793\n",
      "Batch 1300, Loss: 20.571914672851562\n",
      "Batch 1400, Loss: 21.08713150024414\n",
      "Batch 1500, Loss: 20.564926147460938\n",
      "Batch 1600, Loss: 22.22039031982422\n",
      "Batch 1700, Loss: 21.274017333984375\n",
      "Batch 1800, Loss: 22.037364959716797\n",
      "Batch 1900, Loss: 21.28934097290039\n",
      "Batch 2000, Loss: 21.88880157470703\n",
      "Batch 2100, Loss: 21.240795135498047\n",
      "Batch 2200, Loss: 22.188690185546875\n",
      "Batch 2300, Loss: 22.5067081451416\n",
      "Batch 2400, Loss: 22.14252471923828\n",
      "Batch 2500, Loss: 20.453041076660156\n",
      "Batch 2600, Loss: 20.689048767089844\n",
      "Batch 2700, Loss: 20.30145835876465\n",
      "Batch 2800, Loss: 21.293701171875\n",
      "Batch 2900, Loss: 21.4657039642334\n",
      "Batch 3000, Loss: 21.570598602294922\n",
      "Batch 3100, Loss: 21.35729217529297\n",
      "Average Training Loss: 21.22770440426795\n",
      "Average Validation Loss: 22.025821884642255\n",
      "Epoch 22/100\n",
      "----------\n",
      "Batch 0, Loss: 20.7379150390625\n",
      "Batch 100, Loss: 21.13690757751465\n",
      "Batch 200, Loss: 21.4134521484375\n",
      "Batch 300, Loss: 21.6876277923584\n",
      "Batch 400, Loss: 21.82685089111328\n",
      "Batch 500, Loss: 21.60346031188965\n",
      "Batch 600, Loss: 22.067943572998047\n",
      "Batch 700, Loss: 20.503828048706055\n",
      "Batch 800, Loss: 21.55615997314453\n",
      "Batch 900, Loss: 21.176490783691406\n",
      "Batch 1000, Loss: 20.532785415649414\n",
      "Batch 1100, Loss: 20.961658477783203\n",
      "Batch 1200, Loss: 21.037513732910156\n",
      "Batch 1300, Loss: 20.311979293823242\n",
      "Batch 1400, Loss: 20.54364776611328\n",
      "Batch 1500, Loss: 21.35122299194336\n",
      "Batch 1600, Loss: 20.67804718017578\n",
      "Batch 1700, Loss: 22.492977142333984\n",
      "Batch 1800, Loss: 20.08664321899414\n",
      "Batch 1900, Loss: 21.258134841918945\n",
      "Batch 2000, Loss: 21.112648010253906\n",
      "Batch 2100, Loss: 21.334253311157227\n",
      "Batch 2200, Loss: 22.025236129760742\n",
      "Batch 2300, Loss: 20.77041244506836\n",
      "Batch 2400, Loss: 21.33118438720703\n",
      "Batch 2500, Loss: 20.655841827392578\n",
      "Batch 2600, Loss: 21.51215362548828\n",
      "Batch 2700, Loss: 21.532958984375\n",
      "Batch 2800, Loss: 21.101598739624023\n",
      "Batch 2900, Loss: 21.51133918762207\n",
      "Batch 3000, Loss: 20.535991668701172\n",
      "Batch 3100, Loss: 20.4925537109375\n",
      "Average Training Loss: 21.22923021947458\n",
      "Average Validation Loss: 22.023704983325715\n",
      "Epoch 23/100\n",
      "----------\n",
      "Batch 0, Loss: 21.439434051513672\n",
      "Batch 100, Loss: 20.481834411621094\n",
      "Batch 200, Loss: 22.1361141204834\n",
      "Batch 300, Loss: 20.530513763427734\n",
      "Batch 400, Loss: 22.344200134277344\n",
      "Batch 500, Loss: 21.076061248779297\n",
      "Batch 600, Loss: 20.742963790893555\n",
      "Batch 700, Loss: 21.308931350708008\n",
      "Batch 800, Loss: 21.285411834716797\n",
      "Batch 900, Loss: 21.65798568725586\n",
      "Batch 1000, Loss: 21.936962127685547\n",
      "Batch 1100, Loss: 21.02264404296875\n",
      "Batch 1200, Loss: 21.238344192504883\n",
      "Batch 1300, Loss: 20.171245574951172\n",
      "Batch 1400, Loss: 21.418834686279297\n",
      "Batch 1500, Loss: 20.764312744140625\n",
      "Batch 1600, Loss: 20.660829544067383\n",
      "Batch 1700, Loss: 21.720996856689453\n",
      "Batch 1800, Loss: 20.158824920654297\n",
      "Batch 1900, Loss: 21.57881736755371\n",
      "Batch 2000, Loss: 20.896512985229492\n",
      "Batch 2100, Loss: 20.934337615966797\n",
      "Batch 2200, Loss: 21.32512855529785\n",
      "Batch 2300, Loss: 21.055896759033203\n",
      "Batch 2400, Loss: 21.07748031616211\n",
      "Batch 2500, Loss: 21.234601974487305\n",
      "Batch 2600, Loss: 21.461708068847656\n",
      "Batch 2700, Loss: 21.389469146728516\n",
      "Batch 2800, Loss: 21.43144416809082\n",
      "Batch 2900, Loss: 21.141517639160156\n",
      "Batch 3000, Loss: 20.59082794189453\n",
      "Batch 3100, Loss: 21.043254852294922\n",
      "Average Training Loss: 21.22915135570458\n",
      "Average Validation Loss: 22.02662039411829\n",
      "Epoch 24/100\n",
      "----------\n",
      "Batch 0, Loss: 20.484655380249023\n",
      "Batch 100, Loss: 21.704771041870117\n",
      "Batch 200, Loss: 22.238773345947266\n",
      "Batch 300, Loss: 21.712718963623047\n",
      "Batch 400, Loss: 21.784618377685547\n",
      "Batch 500, Loss: 20.722095489501953\n",
      "Batch 600, Loss: 21.506927490234375\n",
      "Batch 700, Loss: 21.186246871948242\n",
      "Batch 800, Loss: 20.599409103393555\n",
      "Batch 900, Loss: 20.95404815673828\n",
      "Batch 1000, Loss: 22.005910873413086\n",
      "Batch 1100, Loss: 21.30953025817871\n",
      "Batch 1200, Loss: 20.505064010620117\n",
      "Batch 1300, Loss: 22.237865447998047\n",
      "Batch 1400, Loss: 21.494247436523438\n",
      "Batch 1500, Loss: 21.67202377319336\n",
      "Batch 1600, Loss: 20.086181640625\n",
      "Batch 1700, Loss: 21.153762817382812\n",
      "Batch 1800, Loss: 21.673263549804688\n",
      "Batch 1900, Loss: 20.833173751831055\n",
      "Batch 2000, Loss: 21.579490661621094\n",
      "Batch 2100, Loss: 21.14997100830078\n",
      "Batch 2200, Loss: 21.14108657836914\n",
      "Batch 2300, Loss: 20.865100860595703\n",
      "Batch 2400, Loss: 21.241653442382812\n",
      "Batch 2500, Loss: 19.978849411010742\n",
      "Batch 2600, Loss: 21.362300872802734\n",
      "Batch 2700, Loss: 21.234066009521484\n",
      "Batch 2800, Loss: 20.800926208496094\n",
      "Batch 2900, Loss: 20.444091796875\n",
      "Batch 3000, Loss: 20.795822143554688\n",
      "Batch 3100, Loss: 20.98807716369629\n",
      "Average Training Loss: 21.227592326302563\n",
      "Average Validation Loss: 22.025975056912035\n",
      "Epoch 25/100\n",
      "----------\n",
      "Batch 0, Loss: 21.33240509033203\n",
      "Batch 100, Loss: 20.342967987060547\n",
      "Batch 200, Loss: 21.144142150878906\n",
      "Batch 300, Loss: 21.513416290283203\n",
      "Batch 400, Loss: 21.763500213623047\n",
      "Batch 500, Loss: 21.406906127929688\n",
      "Batch 600, Loss: 22.04443359375\n",
      "Batch 700, Loss: 21.817607879638672\n",
      "Batch 800, Loss: 21.618139266967773\n",
      "Batch 900, Loss: 20.561288833618164\n",
      "Batch 1000, Loss: 22.05873680114746\n",
      "Batch 1100, Loss: 21.202877044677734\n",
      "Batch 1200, Loss: 20.428133010864258\n",
      "Batch 1300, Loss: 21.648698806762695\n",
      "Batch 1400, Loss: 20.451698303222656\n",
      "Batch 1500, Loss: 20.915748596191406\n",
      "Batch 1600, Loss: 20.952442169189453\n",
      "Batch 1700, Loss: 21.933441162109375\n",
      "Batch 1800, Loss: 21.77593231201172\n",
      "Batch 1900, Loss: 21.36551856994629\n",
      "Batch 2000, Loss: 21.30937957763672\n",
      "Batch 2100, Loss: 19.85736083984375\n",
      "Batch 2200, Loss: 20.36434555053711\n",
      "Batch 2300, Loss: 20.139053344726562\n",
      "Batch 2400, Loss: 21.14824867248535\n",
      "Batch 2500, Loss: 20.34722900390625\n",
      "Batch 2600, Loss: 21.305511474609375\n",
      "Batch 2700, Loss: 22.338054656982422\n",
      "Batch 2800, Loss: 20.28837776184082\n",
      "Batch 2900, Loss: 21.427236557006836\n",
      "Batch 3000, Loss: 21.08855438232422\n",
      "Batch 3100, Loss: 22.373626708984375\n",
      "Average Training Loss: 21.228779868618528\n",
      "Average Validation Loss: 22.023667282753802\n",
      "Epoch 26/100\n",
      "----------\n",
      "Batch 0, Loss: 20.48733901977539\n",
      "Batch 100, Loss: 19.817506790161133\n",
      "Batch 200, Loss: 20.845794677734375\n",
      "Batch 300, Loss: 21.643959045410156\n",
      "Batch 400, Loss: 21.49587631225586\n",
      "Batch 500, Loss: 20.6488037109375\n",
      "Batch 600, Loss: 21.854042053222656\n",
      "Batch 700, Loss: 21.668930053710938\n",
      "Batch 800, Loss: 21.288345336914062\n",
      "Batch 900, Loss: 21.908191680908203\n",
      "Batch 1000, Loss: 21.002948760986328\n",
      "Batch 1100, Loss: 20.648841857910156\n",
      "Batch 1200, Loss: 19.895675659179688\n",
      "Batch 1300, Loss: 21.267704010009766\n",
      "Batch 1400, Loss: 21.75153350830078\n",
      "Batch 1500, Loss: 21.476062774658203\n",
      "Batch 1600, Loss: 21.5845890045166\n",
      "Batch 1700, Loss: 21.944046020507812\n",
      "Batch 1800, Loss: 21.63838768005371\n",
      "Batch 1900, Loss: 19.96074676513672\n",
      "Batch 2000, Loss: 21.651376724243164\n",
      "Batch 2100, Loss: 20.634990692138672\n",
      "Batch 2200, Loss: 19.905162811279297\n",
      "Batch 2300, Loss: 21.561317443847656\n",
      "Batch 2400, Loss: 22.368927001953125\n",
      "Batch 2500, Loss: 20.596405029296875\n",
      "Batch 2600, Loss: 19.148658752441406\n",
      "Batch 2700, Loss: 21.742965698242188\n",
      "Batch 2800, Loss: 21.516469955444336\n",
      "Batch 2900, Loss: 21.93182373046875\n",
      "Batch 3000, Loss: 22.050800323486328\n",
      "Batch 3100, Loss: 20.709030151367188\n",
      "Average Training Loss: 21.22836409634306\n",
      "Average Validation Loss: 22.026402960432335\n",
      "Epoch 27/100\n",
      "----------\n",
      "Batch 0, Loss: 21.396381378173828\n",
      "Batch 100, Loss: 20.901920318603516\n",
      "Batch 200, Loss: 20.739294052124023\n",
      "Batch 300, Loss: 19.906532287597656\n",
      "Batch 400, Loss: 20.846046447753906\n",
      "Batch 500, Loss: 21.144140243530273\n",
      "Batch 600, Loss: 22.094602584838867\n",
      "Batch 700, Loss: 20.632343292236328\n",
      "Batch 800, Loss: 21.89366912841797\n",
      "Batch 900, Loss: 21.329137802124023\n",
      "Batch 1000, Loss: 19.953121185302734\n",
      "Batch 1100, Loss: 20.928171157836914\n",
      "Batch 1200, Loss: 22.072357177734375\n",
      "Batch 1300, Loss: 22.349781036376953\n",
      "Batch 1400, Loss: 21.46124267578125\n",
      "Batch 1500, Loss: 21.54336166381836\n",
      "Batch 1600, Loss: 22.172025680541992\n",
      "Batch 1700, Loss: 22.546428680419922\n",
      "Batch 1800, Loss: 20.597881317138672\n",
      "Batch 1900, Loss: 21.94175910949707\n",
      "Batch 2000, Loss: 22.54763412475586\n",
      "Batch 2100, Loss: 20.927452087402344\n",
      "Batch 2200, Loss: 20.728866577148438\n",
      "Batch 2300, Loss: 21.379125595092773\n",
      "Batch 2400, Loss: 20.895832061767578\n",
      "Batch 2500, Loss: 20.82364273071289\n",
      "Batch 2600, Loss: 22.012134552001953\n",
      "Batch 2700, Loss: 20.321372985839844\n",
      "Batch 2800, Loss: 21.186532974243164\n",
      "Batch 2900, Loss: 20.574817657470703\n",
      "Batch 3000, Loss: 20.020050048828125\n",
      "Batch 3100, Loss: 20.800561904907227\n",
      "Average Training Loss: 21.228130900526168\n",
      "Average Validation Loss: 22.022683184197607\n",
      "Epoch 28/100\n",
      "----------\n",
      "Batch 0, Loss: 21.616634368896484\n",
      "Batch 100, Loss: 21.631389617919922\n",
      "Batch 200, Loss: 20.348094940185547\n",
      "Batch 300, Loss: 21.742563247680664\n",
      "Batch 400, Loss: 21.025493621826172\n",
      "Batch 500, Loss: 21.110942840576172\n",
      "Batch 600, Loss: 21.531484603881836\n",
      "Batch 700, Loss: 22.057893753051758\n",
      "Batch 800, Loss: 21.10507583618164\n",
      "Batch 900, Loss: 20.254114151000977\n",
      "Batch 1000, Loss: 20.741764068603516\n",
      "Batch 1100, Loss: 22.6801815032959\n",
      "Batch 1200, Loss: 23.198200225830078\n",
      "Batch 1300, Loss: 21.001419067382812\n",
      "Batch 1400, Loss: 20.696876525878906\n",
      "Batch 1500, Loss: 22.108646392822266\n",
      "Batch 1600, Loss: 21.45944595336914\n",
      "Batch 1700, Loss: 21.181354522705078\n",
      "Batch 1800, Loss: 20.419036865234375\n",
      "Batch 1900, Loss: 21.760129928588867\n",
      "Batch 2000, Loss: 21.076642990112305\n",
      "Batch 2100, Loss: 22.292648315429688\n",
      "Batch 2200, Loss: 21.082748413085938\n",
      "Batch 2300, Loss: 21.285629272460938\n",
      "Batch 2400, Loss: 20.97394371032715\n",
      "Batch 2500, Loss: 20.918834686279297\n",
      "Batch 2600, Loss: 21.83753204345703\n",
      "Batch 2700, Loss: 21.24182891845703\n",
      "Batch 2800, Loss: 21.711597442626953\n",
      "Batch 2900, Loss: 20.62646484375\n",
      "Batch 3000, Loss: 21.572877883911133\n",
      "Batch 3100, Loss: 22.85454559326172\n",
      "Average Training Loss: 21.22865601956996\n",
      "Average Validation Loss: 22.02176694261267\n",
      "Epoch 29/100\n",
      "----------\n",
      "Batch 0, Loss: 21.67684555053711\n",
      "Batch 100, Loss: 22.661663055419922\n",
      "Batch 200, Loss: 22.479534149169922\n",
      "Batch 300, Loss: 20.396533966064453\n",
      "Batch 400, Loss: 20.775638580322266\n",
      "Batch 500, Loss: 21.34145164489746\n",
      "Batch 600, Loss: 21.628904342651367\n",
      "Batch 700, Loss: 20.903213500976562\n",
      "Batch 800, Loss: 21.638444900512695\n",
      "Batch 900, Loss: 21.223888397216797\n",
      "Batch 1000, Loss: 20.173389434814453\n",
      "Batch 1100, Loss: 20.564992904663086\n",
      "Batch 1200, Loss: 21.33371353149414\n",
      "Batch 1300, Loss: 22.536466598510742\n",
      "Batch 1400, Loss: 21.909969329833984\n",
      "Batch 1500, Loss: 21.415782928466797\n",
      "Batch 1600, Loss: 21.675823211669922\n",
      "Batch 1700, Loss: 21.41452407836914\n",
      "Batch 1800, Loss: 20.837827682495117\n",
      "Batch 1900, Loss: 21.813350677490234\n",
      "Batch 2000, Loss: 20.76572036743164\n",
      "Batch 2100, Loss: 21.053510665893555\n",
      "Batch 2200, Loss: 23.301450729370117\n",
      "Batch 2300, Loss: 20.291709899902344\n",
      "Batch 2400, Loss: 21.70409393310547\n",
      "Batch 2500, Loss: 22.280027389526367\n",
      "Batch 2600, Loss: 19.696491241455078\n",
      "Batch 2700, Loss: 21.28949546813965\n",
      "Batch 2800, Loss: 20.578506469726562\n",
      "Batch 2900, Loss: 20.97467613220215\n",
      "Batch 3000, Loss: 20.946578979492188\n",
      "Batch 3100, Loss: 21.286928176879883\n",
      "Average Training Loss: 21.230320797015086\n",
      "Average Validation Loss: 22.023271840683957\n",
      "Epoch 30/100\n",
      "----------\n",
      "Batch 0, Loss: 21.584217071533203\n",
      "Batch 100, Loss: 21.643901824951172\n",
      "Batch 200, Loss: 22.134254455566406\n",
      "Batch 300, Loss: 20.287090301513672\n",
      "Batch 400, Loss: 22.006664276123047\n",
      "Batch 500, Loss: 20.59581756591797\n",
      "Batch 600, Loss: 21.613906860351562\n",
      "Batch 700, Loss: 21.259471893310547\n",
      "Batch 800, Loss: 21.74738311767578\n",
      "Batch 900, Loss: 21.677017211914062\n",
      "Batch 1000, Loss: 21.76248550415039\n",
      "Batch 1100, Loss: 20.065227508544922\n",
      "Batch 1200, Loss: 21.8363037109375\n",
      "Batch 1300, Loss: 21.026554107666016\n",
      "Batch 1400, Loss: 22.08127212524414\n",
      "Batch 1500, Loss: 20.8256778717041\n",
      "Batch 1600, Loss: 21.542739868164062\n",
      "Batch 1700, Loss: 20.434539794921875\n",
      "Batch 1800, Loss: 20.863582611083984\n",
      "Batch 1900, Loss: 20.88715362548828\n",
      "Batch 2000, Loss: 21.502674102783203\n",
      "Batch 2100, Loss: 21.617874145507812\n",
      "Batch 2200, Loss: 21.748506546020508\n",
      "Batch 2300, Loss: 20.307140350341797\n",
      "Batch 2400, Loss: 21.918649673461914\n",
      "Batch 2500, Loss: 21.726518630981445\n",
      "Batch 2600, Loss: 20.554344177246094\n",
      "Batch 2700, Loss: 20.935659408569336\n",
      "Batch 2800, Loss: 21.117515563964844\n",
      "Batch 2900, Loss: 21.583377838134766\n",
      "Batch 3000, Loss: 22.9631290435791\n",
      "Batch 3100, Loss: 21.523160934448242\n",
      "Average Training Loss: 21.22758355456151\n",
      "Average Validation Loss: 22.022337162748297\n",
      "Epoch 31/100\n",
      "----------\n",
      "Batch 0, Loss: 21.687091827392578\n",
      "Batch 100, Loss: 21.286352157592773\n",
      "Batch 200, Loss: 21.36707305908203\n",
      "Batch 300, Loss: 21.677764892578125\n",
      "Batch 400, Loss: 20.299579620361328\n",
      "Batch 500, Loss: 21.559234619140625\n",
      "Batch 600, Loss: 21.71783447265625\n",
      "Batch 700, Loss: 21.26420783996582\n",
      "Batch 800, Loss: 23.18413543701172\n",
      "Batch 900, Loss: 20.539281845092773\n",
      "Batch 1000, Loss: 22.37196159362793\n",
      "Batch 1100, Loss: 21.607425689697266\n",
      "Batch 1200, Loss: 21.821014404296875\n",
      "Batch 1300, Loss: 22.52729034423828\n",
      "Batch 1400, Loss: 20.394094467163086\n",
      "Batch 1500, Loss: 21.237611770629883\n",
      "Batch 1600, Loss: 21.964157104492188\n",
      "Batch 1700, Loss: 19.8510799407959\n",
      "Batch 1800, Loss: 20.637908935546875\n",
      "Batch 1900, Loss: 20.933345794677734\n",
      "Batch 2000, Loss: 20.651790618896484\n",
      "Batch 2100, Loss: 20.121532440185547\n",
      "Batch 2200, Loss: 21.264636993408203\n",
      "Batch 2300, Loss: 20.867807388305664\n",
      "Batch 2400, Loss: 21.12165069580078\n",
      "Batch 2500, Loss: 20.069580078125\n",
      "Batch 2600, Loss: 20.772598266601562\n",
      "Batch 2700, Loss: 20.644237518310547\n",
      "Batch 2800, Loss: 21.662981033325195\n",
      "Batch 2900, Loss: 21.340351104736328\n",
      "Batch 3000, Loss: 21.25200653076172\n",
      "Batch 3100, Loss: 22.452425003051758\n",
      "Average Training Loss: 21.228169960526717\n",
      "Average Validation Loss: 22.019984379220517\n",
      "Epoch 32/100\n",
      "----------\n",
      "Batch 0, Loss: 21.147300720214844\n",
      "Batch 100, Loss: 21.40886878967285\n",
      "Batch 200, Loss: 20.50244140625\n",
      "Batch 300, Loss: 20.939416885375977\n",
      "Batch 400, Loss: 19.5413761138916\n",
      "Batch 500, Loss: 22.06108283996582\n",
      "Batch 600, Loss: 21.866844177246094\n",
      "Batch 700, Loss: 19.522552490234375\n",
      "Batch 800, Loss: 21.092544555664062\n",
      "Batch 900, Loss: 21.675813674926758\n",
      "Batch 1000, Loss: 21.001750946044922\n",
      "Batch 1100, Loss: 20.53931427001953\n",
      "Batch 1200, Loss: 20.66952896118164\n",
      "Batch 1300, Loss: 21.763587951660156\n",
      "Batch 1400, Loss: 21.490886688232422\n",
      "Batch 1500, Loss: 20.52370262145996\n",
      "Batch 1600, Loss: 21.07285499572754\n",
      "Batch 1700, Loss: 21.431591033935547\n",
      "Batch 1800, Loss: 21.26830291748047\n",
      "Batch 1900, Loss: 22.691247940063477\n",
      "Batch 2000, Loss: 21.798500061035156\n",
      "Batch 2100, Loss: 21.683998107910156\n",
      "Batch 2200, Loss: 19.858503341674805\n",
      "Batch 2300, Loss: 21.53392791748047\n",
      "Batch 2400, Loss: 20.47386932373047\n",
      "Batch 2500, Loss: 21.49439811706543\n",
      "Batch 2600, Loss: 21.545934677124023\n",
      "Batch 2700, Loss: 22.29731559753418\n",
      "Batch 2800, Loss: 22.01987648010254\n",
      "Batch 2900, Loss: 22.909414291381836\n",
      "Batch 3000, Loss: 21.731342315673828\n",
      "Batch 3100, Loss: 22.47953224182129\n",
      "Average Training Loss: 21.227996345694738\n",
      "Average Validation Loss: 22.023291453909366\n",
      "Epoch 33/100\n",
      "----------\n",
      "Batch 0, Loss: 21.48189353942871\n",
      "Batch 100, Loss: 21.862289428710938\n",
      "Batch 200, Loss: 21.01185417175293\n",
      "Batch 300, Loss: 21.694732666015625\n",
      "Batch 400, Loss: 20.961868286132812\n",
      "Batch 500, Loss: 21.52353286743164\n",
      "Batch 600, Loss: 22.70511245727539\n",
      "Batch 700, Loss: 22.026338577270508\n",
      "Batch 800, Loss: 20.760963439941406\n",
      "Batch 900, Loss: 20.900232315063477\n",
      "Batch 1000, Loss: 21.4029483795166\n",
      "Batch 1100, Loss: 20.848264694213867\n",
      "Batch 1200, Loss: 21.0084228515625\n",
      "Batch 1300, Loss: 20.486927032470703\n",
      "Batch 1400, Loss: 21.817710876464844\n",
      "Batch 1500, Loss: 20.98761749267578\n",
      "Batch 1600, Loss: 20.663902282714844\n",
      "Batch 1700, Loss: 21.663848876953125\n",
      "Batch 1800, Loss: 19.712236404418945\n",
      "Batch 1900, Loss: 21.89678382873535\n",
      "Batch 2000, Loss: 21.07794952392578\n",
      "Batch 2100, Loss: 21.850887298583984\n",
      "Batch 2200, Loss: 20.102914810180664\n",
      "Batch 2300, Loss: 21.148401260375977\n",
      "Batch 2400, Loss: 20.350811004638672\n",
      "Batch 2500, Loss: 19.70677947998047\n",
      "Batch 2600, Loss: 20.109699249267578\n",
      "Batch 2700, Loss: 21.81978988647461\n",
      "Batch 2800, Loss: 22.24477767944336\n",
      "Batch 2900, Loss: 21.097759246826172\n",
      "Batch 3000, Loss: 21.115989685058594\n",
      "Batch 3100, Loss: 22.45765495300293\n",
      "Average Training Loss: 21.22997730378886\n",
      "Average Validation Loss: 22.02290791450663\n",
      "Epoch 34/100\n",
      "----------\n",
      "Batch 0, Loss: 21.783843994140625\n",
      "Batch 100, Loss: 20.598194122314453\n",
      "Batch 200, Loss: 22.300004959106445\n",
      "Batch 300, Loss: 20.919754028320312\n",
      "Batch 400, Loss: 20.713647842407227\n",
      "Batch 500, Loss: 21.898969650268555\n",
      "Batch 600, Loss: 20.18088722229004\n",
      "Batch 700, Loss: 20.230804443359375\n",
      "Batch 800, Loss: 20.258602142333984\n",
      "Batch 900, Loss: 20.984535217285156\n",
      "Batch 1000, Loss: 22.480249404907227\n",
      "Batch 1100, Loss: 21.15564727783203\n",
      "Batch 1200, Loss: 20.457902908325195\n",
      "Batch 1300, Loss: 20.597002029418945\n",
      "Batch 1400, Loss: 21.005207061767578\n",
      "Batch 1500, Loss: 19.43576431274414\n",
      "Batch 1600, Loss: 19.705501556396484\n",
      "Batch 1700, Loss: 20.932601928710938\n",
      "Batch 1800, Loss: 21.18669319152832\n",
      "Batch 1900, Loss: 21.177621841430664\n",
      "Batch 2000, Loss: 21.18331527709961\n",
      "Batch 2100, Loss: 21.30956268310547\n",
      "Batch 2200, Loss: 20.67795181274414\n",
      "Batch 2300, Loss: 21.44879150390625\n",
      "Batch 2400, Loss: 20.637378692626953\n",
      "Batch 2500, Loss: 21.571496963500977\n",
      "Batch 2600, Loss: 21.465240478515625\n",
      "Batch 2700, Loss: 21.296703338623047\n",
      "Batch 2800, Loss: 20.468631744384766\n",
      "Batch 2900, Loss: 22.010168075561523\n",
      "Batch 3000, Loss: 21.7034912109375\n",
      "Batch 3100, Loss: 21.24448013305664\n",
      "Average Training Loss: 21.22942480907489\n",
      "Average Validation Loss: 22.021851150025714\n",
      "Epoch 35/100\n",
      "----------\n",
      "Batch 0, Loss: 21.13362693786621\n",
      "Batch 100, Loss: 21.450088500976562\n",
      "Batch 200, Loss: 21.182037353515625\n",
      "Batch 300, Loss: 22.300411224365234\n",
      "Batch 400, Loss: 21.579195022583008\n",
      "Batch 500, Loss: 20.654998779296875\n",
      "Batch 600, Loss: 21.503692626953125\n",
      "Batch 700, Loss: 20.76029396057129\n",
      "Batch 800, Loss: 20.41490364074707\n",
      "Batch 900, Loss: 21.769142150878906\n",
      "Batch 1000, Loss: 20.671123504638672\n",
      "Batch 1100, Loss: 20.82024383544922\n",
      "Batch 1200, Loss: 21.125213623046875\n",
      "Batch 1300, Loss: 21.22719383239746\n",
      "Batch 1400, Loss: 20.939781188964844\n",
      "Batch 1500, Loss: 20.760452270507812\n",
      "Batch 1600, Loss: 20.620296478271484\n",
      "Batch 1700, Loss: 22.15758514404297\n",
      "Batch 1800, Loss: 21.269899368286133\n",
      "Batch 1900, Loss: 21.562206268310547\n",
      "Batch 2000, Loss: 20.972442626953125\n",
      "Batch 2100, Loss: 22.16874122619629\n",
      "Batch 2200, Loss: 21.74516487121582\n",
      "Batch 2300, Loss: 21.78211784362793\n",
      "Batch 2400, Loss: 21.357131958007812\n",
      "Batch 2500, Loss: 20.6693058013916\n",
      "Batch 2600, Loss: 21.87871742248535\n",
      "Batch 2700, Loss: 20.93668556213379\n",
      "Batch 2800, Loss: 21.01749610900879\n",
      "Batch 2900, Loss: 21.88147735595703\n",
      "Batch 3000, Loss: 20.633882522583008\n",
      "Batch 3100, Loss: 22.294849395751953\n",
      "Average Training Loss: 21.228307112482668\n",
      "Average Validation Loss: 22.023428973745794\n",
      "Epoch 36/100\n",
      "----------\n",
      "Batch 0, Loss: 22.188215255737305\n",
      "Batch 100, Loss: 22.60108757019043\n",
      "Batch 200, Loss: 21.356178283691406\n",
      "Batch 300, Loss: 21.309219360351562\n",
      "Batch 400, Loss: 21.259166717529297\n",
      "Batch 500, Loss: 19.842300415039062\n",
      "Batch 600, Loss: 22.2955322265625\n",
      "Batch 700, Loss: 21.297040939331055\n",
      "Batch 800, Loss: 21.887798309326172\n",
      "Batch 900, Loss: 20.89519500732422\n",
      "Batch 1000, Loss: 21.195640563964844\n",
      "Batch 1100, Loss: 21.602073669433594\n",
      "Batch 1200, Loss: 20.550132751464844\n",
      "Batch 1300, Loss: 20.289608001708984\n",
      "Batch 1400, Loss: 20.389318466186523\n",
      "Batch 1500, Loss: 20.52002716064453\n",
      "Batch 1600, Loss: 20.901866912841797\n",
      "Batch 1700, Loss: 20.6326847076416\n",
      "Batch 1800, Loss: 21.22476577758789\n",
      "Batch 1900, Loss: 21.47611427307129\n",
      "Batch 2000, Loss: 21.341922760009766\n",
      "Batch 2100, Loss: 20.801570892333984\n",
      "Batch 2200, Loss: 22.161380767822266\n",
      "Batch 2300, Loss: 22.18274688720703\n",
      "Batch 2400, Loss: 22.747941970825195\n",
      "Batch 2500, Loss: 21.629863739013672\n",
      "Batch 2600, Loss: 21.42753028869629\n",
      "Batch 2700, Loss: 21.036483764648438\n",
      "Batch 2800, Loss: 21.999418258666992\n",
      "Batch 2900, Loss: 21.322925567626953\n",
      "Batch 3000, Loss: 20.551288604736328\n",
      "Batch 3100, Loss: 20.718204498291016\n",
      "Average Training Loss: 21.229111983879225\n",
      "Average Validation Loss: 22.023335631350253\n",
      "Epoch 37/100\n",
      "----------\n",
      "Batch 0, Loss: 20.03439712524414\n",
      "Batch 100, Loss: 21.567747116088867\n",
      "Batch 200, Loss: 20.485111236572266\n",
      "Batch 300, Loss: 22.883174896240234\n",
      "Batch 400, Loss: 21.542407989501953\n",
      "Batch 500, Loss: 20.365266799926758\n",
      "Batch 600, Loss: 21.46329689025879\n",
      "Batch 700, Loss: 21.127059936523438\n",
      "Batch 800, Loss: 21.581302642822266\n",
      "Batch 900, Loss: 21.02789878845215\n",
      "Batch 1000, Loss: 21.595722198486328\n",
      "Batch 1100, Loss: 20.767107009887695\n",
      "Batch 1200, Loss: 21.844806671142578\n",
      "Batch 1300, Loss: 21.437908172607422\n",
      "Batch 1400, Loss: 21.809959411621094\n",
      "Batch 1500, Loss: 21.91799545288086\n",
      "Batch 1600, Loss: 20.927207946777344\n",
      "Batch 1700, Loss: 22.068347930908203\n",
      "Batch 1800, Loss: 21.25122833251953\n",
      "Batch 1900, Loss: 21.42226791381836\n",
      "Batch 2000, Loss: 20.77036476135254\n",
      "Batch 2100, Loss: 20.3890380859375\n",
      "Batch 2200, Loss: 20.850204467773438\n",
      "Batch 2300, Loss: 20.903799057006836\n",
      "Batch 2400, Loss: 20.622295379638672\n",
      "Batch 2500, Loss: 22.007328033447266\n",
      "Batch 2600, Loss: 20.675838470458984\n",
      "Batch 2700, Loss: 22.205936431884766\n",
      "Batch 2800, Loss: 21.559125900268555\n",
      "Batch 2900, Loss: 21.521100997924805\n",
      "Batch 3000, Loss: 20.590816497802734\n",
      "Batch 3100, Loss: 21.235994338989258\n",
      "Average Training Loss: 21.228441710387173\n",
      "Average Validation Loss: 22.017027128503678\n",
      "Epoch 38/100\n",
      "----------\n",
      "Batch 0, Loss: 22.49437713623047\n",
      "Batch 100, Loss: 21.81976890563965\n",
      "Batch 200, Loss: 21.834739685058594\n",
      "Batch 300, Loss: 21.855091094970703\n",
      "Batch 400, Loss: 20.805133819580078\n",
      "Batch 500, Loss: 21.83962631225586\n",
      "Batch 600, Loss: 20.44690704345703\n",
      "Batch 700, Loss: 21.137685775756836\n",
      "Batch 800, Loss: 21.094039916992188\n",
      "Batch 900, Loss: 22.089658737182617\n",
      "Batch 1000, Loss: 21.01071548461914\n",
      "Batch 1100, Loss: 20.645998001098633\n",
      "Batch 1200, Loss: 22.847492218017578\n",
      "Batch 1300, Loss: 21.852670669555664\n",
      "Batch 1400, Loss: 21.1428165435791\n",
      "Batch 1500, Loss: 20.05712127685547\n",
      "Batch 1600, Loss: 22.31574249267578\n",
      "Batch 1700, Loss: 21.33651351928711\n",
      "Batch 1800, Loss: 19.549846649169922\n",
      "Batch 1900, Loss: 21.75592041015625\n",
      "Batch 2000, Loss: 22.07880210876465\n",
      "Batch 2100, Loss: 20.30624771118164\n",
      "Batch 2200, Loss: 20.582996368408203\n",
      "Batch 2300, Loss: 20.565021514892578\n",
      "Batch 2400, Loss: 21.562280654907227\n",
      "Batch 2500, Loss: 21.572294235229492\n",
      "Batch 2600, Loss: 22.44214630126953\n",
      "Batch 2700, Loss: 20.78980827331543\n",
      "Batch 2800, Loss: 20.84705924987793\n",
      "Batch 2900, Loss: 21.38974380493164\n",
      "Batch 3000, Loss: 22.12985610961914\n",
      "Batch 3100, Loss: 21.749439239501953\n",
      "Average Training Loss: 21.22767363553132\n",
      "Average Validation Loss: 22.02427715545005\n",
      "Epoch 39/100\n",
      "----------\n",
      "Batch 0, Loss: 20.454511642456055\n",
      "Batch 100, Loss: 21.23990249633789\n",
      "Batch 200, Loss: 20.363855361938477\n",
      "Batch 300, Loss: 21.261518478393555\n",
      "Batch 400, Loss: 21.64837646484375\n",
      "Batch 500, Loss: 21.247587203979492\n",
      "Batch 600, Loss: 21.81821060180664\n",
      "Batch 700, Loss: 20.49818992614746\n",
      "Batch 800, Loss: 21.506484985351562\n",
      "Batch 900, Loss: 21.134136199951172\n",
      "Batch 1000, Loss: 21.61578369140625\n",
      "Batch 1100, Loss: 21.328128814697266\n",
      "Batch 1200, Loss: 21.72659683227539\n",
      "Batch 1300, Loss: 21.117191314697266\n",
      "Batch 1400, Loss: 20.397136688232422\n",
      "Batch 1500, Loss: 20.07179832458496\n",
      "Batch 1600, Loss: 21.69768524169922\n",
      "Batch 1700, Loss: 21.016210556030273\n",
      "Batch 1800, Loss: 21.16131591796875\n",
      "Batch 1900, Loss: 20.407299041748047\n",
      "Batch 2000, Loss: 20.557222366333008\n",
      "Batch 2100, Loss: 20.88772201538086\n",
      "Batch 2200, Loss: 22.0858211517334\n",
      "Batch 2300, Loss: 19.84357452392578\n",
      "Batch 2400, Loss: 20.691585540771484\n",
      "Batch 2500, Loss: 21.262374877929688\n",
      "Batch 2600, Loss: 21.07192611694336\n",
      "Batch 2700, Loss: 20.30059814453125\n",
      "Batch 2800, Loss: 22.06887435913086\n",
      "Batch 2900, Loss: 20.887903213500977\n",
      "Batch 3000, Loss: 21.589908599853516\n",
      "Batch 3100, Loss: 21.871150970458984\n",
      "Average Training Loss: 21.22786081474246\n",
      "Average Validation Loss: 22.025265681489984\n",
      "Epoch 40/100\n",
      "----------\n",
      "Batch 0, Loss: 21.749547958374023\n",
      "Batch 100, Loss: 20.301828384399414\n",
      "Batch 200, Loss: 21.372817993164062\n",
      "Batch 300, Loss: 21.25041389465332\n",
      "Batch 400, Loss: 21.80630111694336\n",
      "Batch 500, Loss: 21.805561065673828\n",
      "Batch 600, Loss: 22.254070281982422\n",
      "Batch 700, Loss: 22.306915283203125\n",
      "Batch 800, Loss: 20.991859436035156\n",
      "Batch 900, Loss: 20.0645751953125\n",
      "Batch 1000, Loss: 19.39272689819336\n",
      "Batch 1100, Loss: 19.902238845825195\n",
      "Batch 1200, Loss: 20.864891052246094\n",
      "Batch 1300, Loss: 21.42580795288086\n",
      "Batch 1400, Loss: 20.992000579833984\n",
      "Batch 1500, Loss: 20.93693733215332\n",
      "Batch 1600, Loss: 22.357547760009766\n",
      "Batch 1700, Loss: 20.781606674194336\n",
      "Batch 1800, Loss: 20.32158851623535\n",
      "Batch 1900, Loss: 21.428218841552734\n",
      "Batch 2000, Loss: 22.559585571289062\n",
      "Batch 2100, Loss: 21.419301986694336\n",
      "Batch 2200, Loss: 20.992280960083008\n",
      "Batch 2300, Loss: 20.695436477661133\n",
      "Batch 2400, Loss: 22.11346435546875\n",
      "Batch 2500, Loss: 21.5687198638916\n",
      "Batch 2600, Loss: 20.93354606628418\n",
      "Batch 2700, Loss: 21.60698699951172\n",
      "Batch 2800, Loss: 20.002119064331055\n",
      "Batch 2900, Loss: 21.3020076751709\n",
      "Batch 3000, Loss: 21.140321731567383\n",
      "Batch 3100, Loss: 21.48567008972168\n",
      "Average Training Loss: 21.229034026463825\n",
      "Average Validation Loss: 22.02217680342654\n",
      "Epoch 41/100\n",
      "----------\n",
      "Batch 0, Loss: 20.625442504882812\n",
      "Batch 100, Loss: 20.82582664489746\n",
      "Batch 200, Loss: 21.394474029541016\n",
      "Batch 300, Loss: 22.000858306884766\n",
      "Batch 400, Loss: 21.34213638305664\n",
      "Batch 500, Loss: 22.021427154541016\n",
      "Batch 600, Loss: 20.87397003173828\n",
      "Batch 700, Loss: 20.33184051513672\n",
      "Batch 800, Loss: 21.25198745727539\n",
      "Batch 900, Loss: 21.601350784301758\n",
      "Batch 1000, Loss: 22.15711784362793\n",
      "Batch 1100, Loss: 20.323509216308594\n",
      "Batch 1200, Loss: 20.984399795532227\n",
      "Batch 1300, Loss: 21.856136322021484\n",
      "Batch 1400, Loss: 21.614652633666992\n",
      "Batch 1500, Loss: 21.39411735534668\n",
      "Batch 1600, Loss: 21.900711059570312\n",
      "Batch 1700, Loss: 20.99981689453125\n",
      "Batch 1800, Loss: 22.795452117919922\n",
      "Batch 1900, Loss: 21.026805877685547\n",
      "Batch 2000, Loss: 21.443058013916016\n",
      "Batch 2100, Loss: 21.746776580810547\n",
      "Batch 2200, Loss: 22.44269561767578\n",
      "Batch 2300, Loss: 21.22596549987793\n",
      "Batch 2400, Loss: 20.96651268005371\n",
      "Batch 2500, Loss: 21.81928253173828\n",
      "Batch 2600, Loss: 21.037986755371094\n",
      "Batch 2700, Loss: 21.292591094970703\n",
      "Batch 2800, Loss: 22.370073318481445\n",
      "Batch 2900, Loss: 21.142398834228516\n",
      "Batch 3000, Loss: 21.076278686523438\n",
      "Batch 3100, Loss: 21.417285919189453\n",
      "Average Training Loss: 21.22939082138411\n",
      "Average Validation Loss: 22.023228649382897\n",
      "Epoch 42/100\n",
      "----------\n",
      "Batch 0, Loss: 21.135765075683594\n",
      "Batch 100, Loss: 21.48980712890625\n",
      "Batch 200, Loss: 21.152027130126953\n",
      "Batch 300, Loss: 21.25704574584961\n",
      "Batch 400, Loss: 20.86343765258789\n",
      "Batch 500, Loss: 22.2101993560791\n",
      "Batch 600, Loss: 20.997432708740234\n",
      "Batch 700, Loss: 20.709949493408203\n",
      "Batch 800, Loss: 21.249496459960938\n",
      "Batch 900, Loss: 20.972122192382812\n",
      "Batch 1000, Loss: 22.013580322265625\n",
      "Batch 1100, Loss: 20.666088104248047\n",
      "Batch 1200, Loss: 20.855487823486328\n",
      "Batch 1300, Loss: 21.004695892333984\n",
      "Batch 1400, Loss: 21.17527198791504\n",
      "Batch 1500, Loss: 22.11739158630371\n",
      "Batch 1600, Loss: 21.727344512939453\n",
      "Batch 1700, Loss: 21.243806838989258\n",
      "Batch 1800, Loss: 20.02246856689453\n",
      "Batch 1900, Loss: 20.37997055053711\n",
      "Batch 2000, Loss: 21.27981185913086\n",
      "Batch 2100, Loss: 20.671676635742188\n",
      "Batch 2200, Loss: 20.069229125976562\n",
      "Batch 2300, Loss: 21.57599639892578\n",
      "Batch 2400, Loss: 21.284215927124023\n",
      "Batch 2500, Loss: 20.95227813720703\n",
      "Batch 2600, Loss: 20.996238708496094\n",
      "Batch 2700, Loss: 20.212677001953125\n",
      "Batch 2800, Loss: 20.88636589050293\n",
      "Batch 2900, Loss: 20.95859146118164\n",
      "Batch 3000, Loss: 21.6551456451416\n",
      "Batch 3100, Loss: 20.969757080078125\n",
      "Average Training Loss: 21.22742638939816\n",
      "Average Validation Loss: 22.023466540397482\n",
      "Epoch 43/100\n",
      "----------\n",
      "Batch 0, Loss: 21.638395309448242\n",
      "Batch 100, Loss: 22.414960861206055\n",
      "Batch 200, Loss: 20.577163696289062\n",
      "Batch 300, Loss: 21.187044143676758\n",
      "Batch 400, Loss: 21.32872772216797\n",
      "Batch 500, Loss: 21.295358657836914\n",
      "Batch 600, Loss: 21.283742904663086\n",
      "Batch 700, Loss: 22.04419708251953\n",
      "Batch 800, Loss: 21.298702239990234\n",
      "Batch 900, Loss: 20.549840927124023\n",
      "Batch 1000, Loss: 21.840208053588867\n",
      "Batch 1100, Loss: 20.705780029296875\n",
      "Batch 1200, Loss: 21.167686462402344\n",
      "Batch 1300, Loss: 20.005016326904297\n",
      "Batch 1400, Loss: 20.521142959594727\n",
      "Batch 1500, Loss: 21.56000518798828\n",
      "Batch 1600, Loss: 21.261192321777344\n",
      "Batch 1700, Loss: 21.410533905029297\n",
      "Batch 1800, Loss: 21.217418670654297\n",
      "Batch 1900, Loss: 20.990140914916992\n",
      "Batch 2000, Loss: 20.304882049560547\n",
      "Batch 2100, Loss: 21.119548797607422\n",
      "Batch 2200, Loss: 20.4903507232666\n",
      "Batch 2300, Loss: 21.77751350402832\n",
      "Batch 2400, Loss: 21.98348617553711\n",
      "Batch 2500, Loss: 21.573495864868164\n",
      "Batch 2600, Loss: 20.408702850341797\n",
      "Batch 2700, Loss: 21.74202537536621\n",
      "Batch 2800, Loss: 20.468400955200195\n",
      "Batch 2900, Loss: 21.571269989013672\n",
      "Batch 3000, Loss: 19.76915740966797\n",
      "Batch 3100, Loss: 21.056529998779297\n",
      "Average Training Loss: 21.227864371001264\n",
      "Average Validation Loss: 22.020373287606745\n",
      "Epoch 44/100\n",
      "----------\n",
      "Batch 0, Loss: 21.398544311523438\n",
      "Batch 100, Loss: 21.684371948242188\n",
      "Batch 200, Loss: 21.79497528076172\n",
      "Batch 300, Loss: 21.022117614746094\n",
      "Batch 400, Loss: 22.154052734375\n",
      "Batch 500, Loss: 22.000080108642578\n",
      "Batch 600, Loss: 20.69310188293457\n",
      "Batch 700, Loss: 21.974802017211914\n",
      "Batch 800, Loss: 21.42652130126953\n",
      "Batch 900, Loss: 21.66207504272461\n",
      "Batch 1000, Loss: 20.408899307250977\n",
      "Batch 1100, Loss: 21.062034606933594\n",
      "Batch 1200, Loss: 19.97106170654297\n",
      "Batch 1300, Loss: 21.767629623413086\n",
      "Batch 1400, Loss: 21.01795196533203\n",
      "Batch 1500, Loss: 19.751998901367188\n",
      "Batch 1600, Loss: 21.671600341796875\n",
      "Batch 1700, Loss: 21.686595916748047\n",
      "Batch 1800, Loss: 21.37722396850586\n",
      "Batch 1900, Loss: 22.035634994506836\n",
      "Batch 2000, Loss: 21.339279174804688\n",
      "Batch 2100, Loss: 21.952905654907227\n",
      "Batch 2200, Loss: 22.629764556884766\n",
      "Batch 2300, Loss: 21.712114334106445\n",
      "Batch 2400, Loss: 21.56902313232422\n",
      "Batch 2500, Loss: 21.455602645874023\n",
      "Batch 2600, Loss: 20.920307159423828\n",
      "Batch 2700, Loss: 21.779842376708984\n",
      "Batch 2800, Loss: 21.469938278198242\n",
      "Batch 2900, Loss: 21.658092498779297\n",
      "Batch 3000, Loss: 20.522356033325195\n",
      "Batch 3100, Loss: 20.706241607666016\n",
      "Average Training Loss: 21.22808946907975\n",
      "Average Validation Loss: 22.02370171242572\n",
      "Epoch 45/100\n",
      "----------\n",
      "Batch 0, Loss: 22.548328399658203\n",
      "Batch 100, Loss: 20.573753356933594\n",
      "Batch 200, Loss: 22.03304672241211\n",
      "Batch 300, Loss: 21.364248275756836\n",
      "Batch 400, Loss: 21.007442474365234\n",
      "Batch 500, Loss: 21.089479446411133\n",
      "Batch 600, Loss: 21.961734771728516\n",
      "Batch 700, Loss: 21.628931045532227\n",
      "Batch 800, Loss: 21.215076446533203\n",
      "Batch 900, Loss: 21.52956771850586\n",
      "Batch 1000, Loss: 20.862491607666016\n",
      "Batch 1100, Loss: 21.43389129638672\n",
      "Batch 1200, Loss: 19.8859920501709\n",
      "Batch 1300, Loss: 20.81829071044922\n",
      "Batch 1400, Loss: 22.234859466552734\n",
      "Batch 1500, Loss: 20.97871208190918\n",
      "Batch 1600, Loss: 22.162378311157227\n",
      "Batch 1700, Loss: 21.179073333740234\n",
      "Batch 1800, Loss: 20.92275619506836\n",
      "Batch 1900, Loss: 21.099428176879883\n",
      "Batch 2000, Loss: 21.548965454101562\n",
      "Batch 2100, Loss: 20.536602020263672\n",
      "Batch 2200, Loss: 21.349191665649414\n",
      "Batch 2300, Loss: 21.454971313476562\n",
      "Batch 2400, Loss: 20.912063598632812\n",
      "Batch 2500, Loss: 21.50299072265625\n",
      "Batch 2600, Loss: 20.996797561645508\n",
      "Batch 2700, Loss: 20.117679595947266\n",
      "Batch 2800, Loss: 20.964542388916016\n",
      "Batch 2900, Loss: 21.79479217529297\n",
      "Batch 3000, Loss: 20.67560386657715\n",
      "Batch 3100, Loss: 21.637611389160156\n",
      "Average Training Loss: 21.226768812150446\n",
      "Average Validation Loss: 22.02648468017578\n",
      "Epoch 46/100\n",
      "----------\n",
      "Batch 0, Loss: 21.652503967285156\n",
      "Batch 100, Loss: 20.570058822631836\n",
      "Batch 200, Loss: 20.549816131591797\n",
      "Batch 300, Loss: 20.848796844482422\n",
      "Batch 400, Loss: 21.140911102294922\n",
      "Batch 500, Loss: 21.480804443359375\n",
      "Batch 600, Loss: 20.903533935546875\n",
      "Batch 700, Loss: 22.230270385742188\n",
      "Batch 800, Loss: 20.57750701904297\n",
      "Batch 900, Loss: 21.901470184326172\n",
      "Batch 1000, Loss: 22.400836944580078\n",
      "Batch 1100, Loss: 21.6624755859375\n",
      "Batch 1200, Loss: 23.143024444580078\n",
      "Batch 1300, Loss: 21.46358871459961\n",
      "Batch 1400, Loss: 22.136207580566406\n",
      "Batch 1500, Loss: 22.813827514648438\n",
      "Batch 1600, Loss: 21.416658401489258\n",
      "Batch 1700, Loss: 21.13880729675293\n",
      "Batch 1800, Loss: 20.268993377685547\n",
      "Batch 1900, Loss: 20.747404098510742\n",
      "Batch 2000, Loss: 22.02558135986328\n",
      "Batch 2100, Loss: 22.34475326538086\n",
      "Batch 2200, Loss: 21.295207977294922\n",
      "Batch 2300, Loss: 21.559215545654297\n",
      "Batch 2400, Loss: 20.733139038085938\n",
      "Batch 2500, Loss: 21.052200317382812\n",
      "Batch 2600, Loss: 21.75654411315918\n",
      "Batch 2700, Loss: 23.10198974609375\n",
      "Batch 2800, Loss: 20.65713119506836\n",
      "Batch 2900, Loss: 20.552427291870117\n",
      "Batch 3000, Loss: 20.80483055114746\n",
      "Batch 3100, Loss: 20.78814697265625\n",
      "Average Training Loss: 21.230728643237786\n",
      "Average Validation Loss: 22.023793313858356\n",
      "Epoch 47/100\n",
      "----------\n",
      "Batch 0, Loss: 20.414152145385742\n",
      "Batch 100, Loss: 21.65465545654297\n",
      "Batch 200, Loss: 21.957035064697266\n",
      "Batch 300, Loss: 21.072114944458008\n",
      "Batch 400, Loss: 21.694772720336914\n",
      "Batch 500, Loss: 21.590335845947266\n",
      "Batch 600, Loss: 21.914365768432617\n",
      "Batch 700, Loss: 21.239437103271484\n",
      "Batch 800, Loss: 20.876262664794922\n",
      "Batch 900, Loss: 20.129716873168945\n",
      "Batch 1000, Loss: 21.16277313232422\n",
      "Batch 1100, Loss: 20.87411880493164\n",
      "Batch 1200, Loss: 20.44771385192871\n",
      "Batch 1300, Loss: 20.28553009033203\n",
      "Batch 1400, Loss: 20.926652908325195\n",
      "Batch 1500, Loss: 22.453853607177734\n",
      "Batch 1600, Loss: 22.90602684020996\n",
      "Batch 1700, Loss: 21.002573013305664\n",
      "Batch 1800, Loss: 21.49282455444336\n",
      "Batch 1900, Loss: 21.221702575683594\n",
      "Batch 2000, Loss: 20.993284225463867\n",
      "Batch 2100, Loss: 21.218482971191406\n",
      "Batch 2200, Loss: 21.142566680908203\n",
      "Batch 2300, Loss: 20.508319854736328\n",
      "Batch 2400, Loss: 23.051176071166992\n",
      "Batch 2500, Loss: 21.9116153717041\n",
      "Batch 2600, Loss: 20.717254638671875\n",
      "Batch 2700, Loss: 21.39647674560547\n",
      "Batch 2800, Loss: 21.525348663330078\n",
      "Batch 2900, Loss: 20.820335388183594\n",
      "Batch 3000, Loss: 20.449678421020508\n",
      "Batch 3100, Loss: 22.55599594116211\n",
      "Average Training Loss: 21.2285326728384\n",
      "Average Validation Loss: 22.021566094743445\n",
      "Epoch 48/100\n",
      "----------\n",
      "Batch 0, Loss: 21.18693733215332\n",
      "Batch 100, Loss: 21.630455017089844\n",
      "Batch 200, Loss: 21.17660140991211\n",
      "Batch 300, Loss: 22.355587005615234\n",
      "Batch 400, Loss: 20.56622314453125\n",
      "Batch 500, Loss: 21.064010620117188\n",
      "Batch 600, Loss: 20.598480224609375\n",
      "Batch 700, Loss: 20.979969024658203\n",
      "Batch 800, Loss: 21.57402801513672\n",
      "Batch 900, Loss: 21.58743667602539\n",
      "Batch 1000, Loss: 20.80522918701172\n",
      "Batch 1100, Loss: 20.08436393737793\n",
      "Batch 1200, Loss: 22.486919403076172\n",
      "Batch 1300, Loss: 22.66806411743164\n",
      "Batch 1400, Loss: 21.097902297973633\n",
      "Batch 1500, Loss: 21.598527908325195\n",
      "Batch 1600, Loss: 20.693410873413086\n",
      "Batch 1700, Loss: 22.57587432861328\n",
      "Batch 1800, Loss: 20.77926254272461\n",
      "Batch 1900, Loss: 20.159767150878906\n",
      "Batch 2000, Loss: 21.50537109375\n",
      "Batch 2100, Loss: 21.538768768310547\n",
      "Batch 2200, Loss: 20.77322769165039\n",
      "Batch 2300, Loss: 21.096935272216797\n",
      "Batch 2400, Loss: 21.407878875732422\n",
      "Batch 2500, Loss: 21.439254760742188\n",
      "Batch 2600, Loss: 20.188507080078125\n",
      "Batch 2700, Loss: 22.013938903808594\n",
      "Batch 2800, Loss: 21.89154815673828\n",
      "Batch 2900, Loss: 21.035247802734375\n",
      "Batch 3000, Loss: 21.57884407043457\n",
      "Batch 3100, Loss: 20.937820434570312\n",
      "Average Training Loss: 21.228757867982974\n",
      "Average Validation Loss: 22.031519630107475\n",
      "Epoch 49/100\n",
      "----------\n",
      "Batch 0, Loss: 21.763837814331055\n",
      "Batch 100, Loss: 21.76220703125\n",
      "Batch 200, Loss: 21.868141174316406\n",
      "Batch 300, Loss: 20.4039306640625\n",
      "Batch 400, Loss: 22.108245849609375\n",
      "Batch 500, Loss: 21.055315017700195\n",
      "Batch 600, Loss: 21.66087532043457\n",
      "Batch 700, Loss: 21.6296443939209\n",
      "Batch 800, Loss: 21.282527923583984\n",
      "Batch 900, Loss: 21.846431732177734\n",
      "Batch 1000, Loss: 22.77556610107422\n",
      "Batch 1100, Loss: 21.04861068725586\n",
      "Batch 1200, Loss: 20.092790603637695\n",
      "Batch 1300, Loss: 21.71571922302246\n",
      "Batch 1400, Loss: 21.506277084350586\n",
      "Batch 1500, Loss: 21.524330139160156\n",
      "Batch 1600, Loss: 22.495670318603516\n",
      "Batch 1700, Loss: 22.24156379699707\n",
      "Batch 1800, Loss: 21.4577579498291\n",
      "Batch 1900, Loss: 22.019393920898438\n",
      "Batch 2000, Loss: 22.055347442626953\n",
      "Batch 2100, Loss: 21.933162689208984\n",
      "Batch 2200, Loss: 21.018823623657227\n",
      "Batch 2300, Loss: 21.36566162109375\n",
      "Batch 2400, Loss: 20.51957130432129\n",
      "Batch 2500, Loss: 22.318622589111328\n",
      "Batch 2600, Loss: 22.482666015625\n",
      "Batch 2700, Loss: 21.98568344116211\n",
      "Batch 2800, Loss: 20.991790771484375\n",
      "Batch 2900, Loss: 20.239774703979492\n",
      "Batch 3000, Loss: 22.324424743652344\n",
      "Batch 3100, Loss: 21.498994827270508\n",
      "Average Training Loss: 21.22951314224845\n",
      "Average Validation Loss: 22.021693765356186\n",
      "Epoch 50/100\n",
      "----------\n",
      "Batch 0, Loss: 20.220630645751953\n",
      "Batch 100, Loss: 21.243127822875977\n",
      "Batch 200, Loss: 21.845962524414062\n",
      "Batch 300, Loss: 20.727079391479492\n",
      "Batch 400, Loss: 21.645105361938477\n",
      "Batch 500, Loss: 21.965595245361328\n",
      "Batch 600, Loss: 20.639427185058594\n",
      "Batch 700, Loss: 20.466386795043945\n",
      "Batch 800, Loss: 21.47418975830078\n",
      "Batch 900, Loss: 20.893390655517578\n",
      "Batch 1000, Loss: 21.046443939208984\n",
      "Batch 1100, Loss: 20.319639205932617\n",
      "Batch 1200, Loss: 20.839107513427734\n",
      "Batch 1300, Loss: 20.499052047729492\n",
      "Batch 1400, Loss: 21.745689392089844\n",
      "Batch 1500, Loss: 21.071475982666016\n",
      "Batch 1600, Loss: 21.36439323425293\n",
      "Batch 1700, Loss: 20.461271286010742\n",
      "Batch 1800, Loss: 21.927440643310547\n",
      "Batch 1900, Loss: 21.598499298095703\n",
      "Batch 2000, Loss: 21.320152282714844\n",
      "Batch 2100, Loss: 19.70960807800293\n",
      "Batch 2200, Loss: 21.18366241455078\n",
      "Batch 2300, Loss: 19.824020385742188\n",
      "Batch 2400, Loss: 21.45855712890625\n",
      "Batch 2500, Loss: 20.359169006347656\n",
      "Batch 2600, Loss: 21.02456283569336\n",
      "Batch 2700, Loss: 22.15837860107422\n",
      "Batch 2800, Loss: 21.320209503173828\n",
      "Batch 2900, Loss: 20.26192855834961\n",
      "Batch 3000, Loss: 21.200138092041016\n",
      "Batch 3100, Loss: 21.868732452392578\n",
      "Average Training Loss: 21.22944901311064\n",
      "Average Validation Loss: 22.02284684688487\n",
      "Epoch 51/100\n",
      "----------\n",
      "Batch 0, Loss: 21.888952255249023\n",
      "Batch 100, Loss: 21.775959014892578\n",
      "Batch 200, Loss: 21.745946884155273\n",
      "Batch 300, Loss: 20.903827667236328\n",
      "Batch 400, Loss: 21.979324340820312\n",
      "Batch 500, Loss: 20.54092025756836\n",
      "Batch 600, Loss: 21.763521194458008\n",
      "Batch 700, Loss: 21.240955352783203\n",
      "Batch 800, Loss: 20.597618103027344\n",
      "Batch 900, Loss: 21.03308868408203\n",
      "Batch 1000, Loss: 22.622425079345703\n",
      "Batch 1100, Loss: 21.201499938964844\n",
      "Batch 1200, Loss: 20.818138122558594\n",
      "Batch 1300, Loss: 21.591114044189453\n",
      "Batch 1400, Loss: 20.69212532043457\n",
      "Batch 1500, Loss: 20.727127075195312\n",
      "Batch 1600, Loss: 20.704208374023438\n",
      "Batch 1700, Loss: 21.045059204101562\n",
      "Batch 1800, Loss: 20.791702270507812\n",
      "Batch 1900, Loss: 20.94884490966797\n",
      "Batch 2000, Loss: 21.328250885009766\n",
      "Batch 2100, Loss: 21.837059020996094\n",
      "Batch 2200, Loss: 20.73040008544922\n",
      "Batch 2300, Loss: 21.339801788330078\n",
      "Batch 2400, Loss: 21.170352935791016\n",
      "Batch 2500, Loss: 20.62708282470703\n",
      "Batch 2600, Loss: 21.46674919128418\n",
      "Batch 2700, Loss: 20.782106399536133\n",
      "Batch 2800, Loss: 21.412242889404297\n",
      "Batch 2900, Loss: 21.060440063476562\n",
      "Batch 3000, Loss: 21.189571380615234\n",
      "Batch 3100, Loss: 21.69603157043457\n",
      "Average Training Loss: 21.228141706408437\n",
      "Average Validation Loss: 22.025727974100317\n",
      "Epoch 52/100\n",
      "----------\n",
      "Batch 0, Loss: 20.63912010192871\n",
      "Batch 100, Loss: 20.634281158447266\n",
      "Batch 200, Loss: 21.767534255981445\n",
      "Batch 300, Loss: 20.45163345336914\n",
      "Batch 400, Loss: 21.79985809326172\n",
      "Batch 500, Loss: 22.013595581054688\n",
      "Batch 600, Loss: 21.732131958007812\n",
      "Batch 700, Loss: 21.35637664794922\n",
      "Batch 800, Loss: 22.675552368164062\n",
      "Batch 900, Loss: 22.74630355834961\n",
      "Batch 1000, Loss: 20.20060920715332\n",
      "Batch 1100, Loss: 21.11557388305664\n",
      "Batch 1200, Loss: 21.51239776611328\n",
      "Batch 1300, Loss: 20.733871459960938\n",
      "Batch 1400, Loss: 21.243362426757812\n",
      "Batch 1500, Loss: 21.502086639404297\n",
      "Batch 1600, Loss: 20.79033660888672\n",
      "Batch 1700, Loss: 21.525999069213867\n",
      "Batch 1800, Loss: 22.232194900512695\n",
      "Batch 1900, Loss: 21.475467681884766\n",
      "Batch 2000, Loss: 20.294593811035156\n",
      "Batch 2100, Loss: 20.131149291992188\n",
      "Batch 2200, Loss: 22.143808364868164\n",
      "Batch 2300, Loss: 20.057357788085938\n",
      "Batch 2400, Loss: 21.732547760009766\n",
      "Batch 2500, Loss: 21.54613494873047\n",
      "Batch 2600, Loss: 21.976398468017578\n",
      "Batch 2700, Loss: 21.939651489257812\n",
      "Batch 2800, Loss: 21.95928192138672\n",
      "Batch 2900, Loss: 20.781078338623047\n",
      "Batch 3000, Loss: 20.660072326660156\n",
      "Batch 3100, Loss: 21.271221160888672\n",
      "Average Training Loss: 21.2280916979598\n",
      "Average Validation Loss: 22.021577583475317\n",
      "Epoch 53/100\n",
      "----------\n",
      "Batch 0, Loss: 20.516193389892578\n",
      "Batch 100, Loss: 20.93262481689453\n",
      "Batch 200, Loss: 21.09213638305664\n",
      "Batch 300, Loss: 22.104856491088867\n",
      "Batch 400, Loss: 21.38447380065918\n",
      "Batch 500, Loss: 21.769775390625\n",
      "Batch 600, Loss: 22.363975524902344\n",
      "Batch 700, Loss: 22.454212188720703\n",
      "Batch 800, Loss: 20.767202377319336\n",
      "Batch 900, Loss: 21.01056671142578\n",
      "Batch 1000, Loss: 21.436031341552734\n",
      "Batch 1100, Loss: 20.87808609008789\n",
      "Batch 1200, Loss: 21.31329917907715\n",
      "Batch 1300, Loss: 21.757495880126953\n",
      "Batch 1400, Loss: 20.80992889404297\n",
      "Batch 1500, Loss: 20.607463836669922\n",
      "Batch 1600, Loss: 20.8023681640625\n",
      "Batch 1700, Loss: 21.993389129638672\n",
      "Batch 1800, Loss: 21.042924880981445\n",
      "Batch 1900, Loss: 21.8347110748291\n",
      "Batch 2000, Loss: 19.87826156616211\n",
      "Batch 2100, Loss: 20.412668228149414\n",
      "Batch 2200, Loss: 21.057802200317383\n",
      "Batch 2300, Loss: 21.096548080444336\n",
      "Batch 2400, Loss: 20.554824829101562\n",
      "Batch 2500, Loss: 20.56269645690918\n",
      "Batch 2600, Loss: 22.074914932250977\n",
      "Batch 2700, Loss: 21.647872924804688\n",
      "Batch 2800, Loss: 23.259593963623047\n",
      "Batch 2900, Loss: 22.216079711914062\n",
      "Batch 3000, Loss: 21.19434356689453\n",
      "Batch 3100, Loss: 22.535888671875\n",
      "Average Training Loss: 21.229695985638763\n",
      "Average Validation Loss: 22.023593415605262\n",
      "Epoch 54/100\n",
      "----------\n",
      "Batch 0, Loss: 21.972272872924805\n",
      "Batch 100, Loss: 20.94063949584961\n",
      "Batch 200, Loss: 20.529834747314453\n",
      "Batch 300, Loss: 21.232025146484375\n",
      "Batch 400, Loss: 22.380901336669922\n",
      "Batch 500, Loss: 21.808380126953125\n",
      "Batch 600, Loss: 20.07158851623535\n",
      "Batch 700, Loss: 22.013315200805664\n",
      "Batch 800, Loss: 21.506498336791992\n",
      "Batch 900, Loss: 20.918479919433594\n",
      "Batch 1000, Loss: 22.93337631225586\n",
      "Batch 1100, Loss: 20.157855987548828\n",
      "Batch 1200, Loss: 22.025146484375\n",
      "Batch 1300, Loss: 20.957555770874023\n",
      "Batch 1400, Loss: 21.26880645751953\n",
      "Batch 1500, Loss: 20.336200714111328\n",
      "Batch 1600, Loss: 21.126728057861328\n",
      "Batch 1700, Loss: 21.549354553222656\n",
      "Batch 1800, Loss: 22.113056182861328\n",
      "Batch 1900, Loss: 21.81938934326172\n",
      "Batch 2000, Loss: 20.380069732666016\n",
      "Batch 2100, Loss: 21.381057739257812\n",
      "Batch 2200, Loss: 20.97488784790039\n",
      "Batch 2300, Loss: 20.873401641845703\n",
      "Batch 2400, Loss: 21.859176635742188\n",
      "Batch 2500, Loss: 20.79387664794922\n",
      "Batch 2600, Loss: 20.700939178466797\n",
      "Batch 2700, Loss: 22.657638549804688\n",
      "Batch 2800, Loss: 22.013635635375977\n",
      "Batch 2900, Loss: 19.545989990234375\n",
      "Batch 3000, Loss: 21.240127563476562\n",
      "Batch 3100, Loss: 21.3884220123291\n",
      "Average Training Loss: 21.2280088247537\n",
      "Average Validation Loss: 22.025413622754684\n",
      "Epoch 55/100\n",
      "----------\n",
      "Batch 0, Loss: 21.56203842163086\n",
      "Batch 100, Loss: 19.866477966308594\n",
      "Batch 200, Loss: 20.936216354370117\n",
      "Batch 300, Loss: 21.677440643310547\n",
      "Batch 400, Loss: 21.0174503326416\n",
      "Batch 500, Loss: 20.736921310424805\n",
      "Batch 600, Loss: 21.516033172607422\n",
      "Batch 700, Loss: 20.0518856048584\n",
      "Batch 800, Loss: 21.36852264404297\n",
      "Batch 900, Loss: 20.855335235595703\n",
      "Batch 1000, Loss: 20.591245651245117\n",
      "Batch 1100, Loss: 20.98798942565918\n",
      "Batch 1200, Loss: 20.97235870361328\n",
      "Batch 1300, Loss: 21.71541404724121\n",
      "Batch 1400, Loss: 22.044334411621094\n",
      "Batch 1500, Loss: 20.193212509155273\n",
      "Batch 1600, Loss: 21.04831886291504\n",
      "Batch 1700, Loss: 21.51326560974121\n",
      "Batch 1800, Loss: 21.580429077148438\n",
      "Batch 1900, Loss: 20.413368225097656\n",
      "Batch 2000, Loss: 20.486774444580078\n",
      "Batch 2100, Loss: 21.948081970214844\n",
      "Batch 2200, Loss: 22.21416473388672\n",
      "Batch 2300, Loss: 21.415546417236328\n",
      "Batch 2400, Loss: 21.974306106567383\n",
      "Batch 2500, Loss: 21.00501823425293\n",
      "Batch 2600, Loss: 19.6752872467041\n",
      "Batch 2700, Loss: 21.03092384338379\n",
      "Batch 2800, Loss: 21.128768920898438\n",
      "Batch 2900, Loss: 20.879682540893555\n",
      "Batch 3000, Loss: 22.5963134765625\n",
      "Batch 3100, Loss: 22.037214279174805\n",
      "Average Training Loss: 21.229245918701018\n",
      "Average Validation Loss: 22.02005609147092\n",
      "Epoch 56/100\n",
      "----------\n",
      "Batch 0, Loss: 20.869699478149414\n",
      "Batch 100, Loss: 20.506492614746094\n",
      "Batch 200, Loss: 21.325511932373047\n",
      "Batch 300, Loss: 21.67446517944336\n",
      "Batch 400, Loss: 20.898727416992188\n",
      "Batch 500, Loss: 21.374149322509766\n",
      "Batch 600, Loss: 21.40868377685547\n",
      "Batch 700, Loss: 22.64602279663086\n",
      "Batch 800, Loss: 20.713581085205078\n",
      "Batch 900, Loss: 21.040218353271484\n",
      "Batch 1000, Loss: 21.19239044189453\n",
      "Batch 1100, Loss: 21.38664436340332\n",
      "Batch 1200, Loss: 20.14627456665039\n",
      "Batch 1300, Loss: 22.251354217529297\n",
      "Batch 1400, Loss: 21.469240188598633\n",
      "Batch 1500, Loss: 21.628944396972656\n",
      "Batch 1600, Loss: 21.490814208984375\n",
      "Batch 1700, Loss: 20.046772003173828\n",
      "Batch 1800, Loss: 22.15642547607422\n",
      "Batch 1900, Loss: 21.69321632385254\n",
      "Batch 2000, Loss: 21.50998306274414\n",
      "Batch 2100, Loss: 22.026649475097656\n",
      "Batch 2200, Loss: 21.340961456298828\n",
      "Batch 2300, Loss: 21.304750442504883\n",
      "Batch 2400, Loss: 21.772052764892578\n",
      "Batch 2500, Loss: 20.987567901611328\n",
      "Batch 2600, Loss: 20.828386306762695\n",
      "Batch 2700, Loss: 21.259151458740234\n",
      "Batch 2800, Loss: 20.996009826660156\n",
      "Batch 2900, Loss: 21.3560791015625\n",
      "Batch 3000, Loss: 21.40256118774414\n",
      "Batch 3100, Loss: 20.39470672607422\n",
      "Average Training Loss: 21.228057585903098\n",
      "Average Validation Loss: 22.02399831731269\n",
      "Epoch 57/100\n",
      "----------\n",
      "Batch 0, Loss: 21.741989135742188\n",
      "Batch 100, Loss: 20.703733444213867\n",
      "Batch 200, Loss: 21.927387237548828\n",
      "Batch 300, Loss: 20.828351974487305\n",
      "Batch 400, Loss: 20.651348114013672\n",
      "Batch 500, Loss: 22.174560546875\n",
      "Batch 600, Loss: 20.61678123474121\n",
      "Batch 700, Loss: 19.602825164794922\n",
      "Batch 800, Loss: 21.143291473388672\n",
      "Batch 900, Loss: 21.97607421875\n",
      "Batch 1000, Loss: 21.48792839050293\n",
      "Batch 1100, Loss: 20.29007339477539\n",
      "Batch 1200, Loss: 21.419267654418945\n",
      "Batch 1300, Loss: 22.131446838378906\n",
      "Batch 1400, Loss: 20.752408981323242\n",
      "Batch 1500, Loss: 21.574125289916992\n",
      "Batch 1600, Loss: 21.280014038085938\n",
      "Batch 1700, Loss: 21.167877197265625\n",
      "Batch 1800, Loss: 21.889087677001953\n",
      "Batch 1900, Loss: 22.046875\n",
      "Batch 2000, Loss: 21.14317512512207\n",
      "Batch 2100, Loss: 21.434444427490234\n",
      "Batch 2200, Loss: 23.01588249206543\n",
      "Batch 2300, Loss: 21.100784301757812\n",
      "Batch 2400, Loss: 22.615310668945312\n",
      "Batch 2500, Loss: 21.058170318603516\n",
      "Batch 2600, Loss: 21.126169204711914\n",
      "Batch 2700, Loss: 20.417985916137695\n",
      "Batch 2800, Loss: 19.469404220581055\n",
      "Batch 2900, Loss: 21.10761260986328\n",
      "Batch 3000, Loss: 20.80583953857422\n",
      "Batch 3100, Loss: 22.21303939819336\n",
      "Average Training Loss: 21.228806655218886\n",
      "Average Validation Loss: 22.022699498115703\n",
      "Epoch 58/100\n",
      "----------\n",
      "Batch 0, Loss: 22.738985061645508\n",
      "Batch 100, Loss: 20.649181365966797\n",
      "Batch 200, Loss: 21.595754623413086\n",
      "Batch 300, Loss: 21.407649993896484\n",
      "Batch 400, Loss: 21.214630126953125\n",
      "Batch 500, Loss: 21.458234786987305\n",
      "Batch 600, Loss: 22.319124221801758\n",
      "Batch 700, Loss: 21.413402557373047\n",
      "Batch 800, Loss: 22.213768005371094\n",
      "Batch 900, Loss: 20.791635513305664\n",
      "Batch 1000, Loss: 21.20783233642578\n",
      "Batch 1100, Loss: 20.43535614013672\n",
      "Batch 1200, Loss: 20.559776306152344\n",
      "Batch 1300, Loss: 23.060394287109375\n",
      "Batch 1400, Loss: 20.77306365966797\n",
      "Batch 1500, Loss: 20.75246810913086\n",
      "Batch 1600, Loss: 21.24932098388672\n",
      "Batch 1700, Loss: 21.47835922241211\n",
      "Batch 1800, Loss: 21.306812286376953\n",
      "Batch 1900, Loss: 20.88884735107422\n",
      "Batch 2000, Loss: 21.554054260253906\n",
      "Batch 2100, Loss: 21.399429321289062\n",
      "Batch 2200, Loss: 20.41653823852539\n",
      "Batch 2300, Loss: 22.250276565551758\n",
      "Batch 2400, Loss: 20.74921417236328\n",
      "Batch 2500, Loss: 21.244129180908203\n",
      "Batch 2600, Loss: 21.61100959777832\n",
      "Batch 2700, Loss: 21.730146408081055\n",
      "Batch 2800, Loss: 21.085704803466797\n",
      "Batch 2900, Loss: 21.713634490966797\n",
      "Batch 3000, Loss: 21.233551025390625\n",
      "Batch 3100, Loss: 21.740474700927734\n",
      "Average Training Loss: 21.22912631083384\n",
      "Average Validation Loss: 22.02241902452834\n",
      "Epoch 59/100\n",
      "----------\n",
      "Batch 0, Loss: 21.498804092407227\n",
      "Batch 100, Loss: 22.11733627319336\n",
      "Batch 200, Loss: 21.192564010620117\n",
      "Batch 300, Loss: 20.36658477783203\n",
      "Batch 400, Loss: 20.12959098815918\n",
      "Batch 500, Loss: 21.327014923095703\n",
      "Batch 600, Loss: 21.43718719482422\n",
      "Batch 700, Loss: 20.773130416870117\n",
      "Batch 800, Loss: 21.247085571289062\n",
      "Batch 900, Loss: 20.686378479003906\n",
      "Batch 1000, Loss: 20.075572967529297\n",
      "Batch 1100, Loss: 20.83211898803711\n",
      "Batch 1200, Loss: 21.948564529418945\n",
      "Batch 1300, Loss: 21.67974281311035\n",
      "Batch 1400, Loss: 20.26622772216797\n",
      "Batch 1500, Loss: 21.689857482910156\n",
      "Batch 1600, Loss: 23.454387664794922\n",
      "Batch 1700, Loss: 22.166179656982422\n",
      "Batch 1800, Loss: 21.027488708496094\n",
      "Batch 1900, Loss: 21.72991180419922\n",
      "Batch 2000, Loss: 22.09611701965332\n",
      "Batch 2100, Loss: 20.83800506591797\n",
      "Batch 2200, Loss: 21.39385986328125\n",
      "Batch 2300, Loss: 21.08358383178711\n",
      "Batch 2400, Loss: 21.49785041809082\n",
      "Batch 2500, Loss: 20.48342514038086\n",
      "Batch 2600, Loss: 21.02837371826172\n",
      "Batch 2700, Loss: 21.688905715942383\n",
      "Batch 2800, Loss: 20.93517303466797\n",
      "Batch 2900, Loss: 20.38582992553711\n",
      "Batch 3000, Loss: 20.361547470092773\n",
      "Batch 3100, Loss: 21.420574188232422\n",
      "Average Training Loss: 21.22824153463349\n",
      "Average Validation Loss: 22.023105089715187\n",
      "Epoch 60/100\n",
      "----------\n",
      "Batch 0, Loss: 21.924972534179688\n",
      "Batch 100, Loss: 21.790584564208984\n",
      "Batch 200, Loss: 21.016023635864258\n",
      "Batch 300, Loss: 20.44974136352539\n",
      "Batch 400, Loss: 21.377586364746094\n",
      "Batch 500, Loss: 21.363216400146484\n",
      "Batch 600, Loss: 21.372356414794922\n",
      "Batch 700, Loss: 20.696090698242188\n",
      "Batch 800, Loss: 20.01071548461914\n",
      "Batch 900, Loss: 21.56368637084961\n",
      "Batch 1000, Loss: 21.783964157104492\n",
      "Batch 1100, Loss: 20.925582885742188\n",
      "Batch 1200, Loss: 21.03731346130371\n",
      "Batch 1300, Loss: 21.43722152709961\n",
      "Batch 1400, Loss: 20.617633819580078\n",
      "Batch 1500, Loss: 21.97046661376953\n",
      "Batch 1600, Loss: 21.971160888671875\n",
      "Batch 1700, Loss: 21.617279052734375\n",
      "Batch 1800, Loss: 20.258668899536133\n",
      "Batch 1900, Loss: 20.45960235595703\n",
      "Batch 2000, Loss: 19.909818649291992\n",
      "Batch 2100, Loss: 21.180824279785156\n",
      "Batch 2200, Loss: 21.509994506835938\n",
      "Batch 2300, Loss: 20.967830657958984\n",
      "Batch 2400, Loss: 20.964685440063477\n",
      "Batch 2500, Loss: 21.89872169494629\n",
      "Batch 2600, Loss: 19.56663703918457\n",
      "Batch 2700, Loss: 20.854557037353516\n",
      "Batch 2800, Loss: 21.90219497680664\n",
      "Batch 2900, Loss: 20.75572967529297\n",
      "Batch 3000, Loss: 21.606185913085938\n",
      "Batch 3100, Loss: 21.612607955932617\n",
      "Average Training Loss: 21.227685347768187\n",
      "Average Validation Loss: 22.023099318971024\n",
      "Epoch 61/100\n",
      "----------\n",
      "Batch 0, Loss: 20.593826293945312\n",
      "Batch 100, Loss: 20.641433715820312\n",
      "Batch 200, Loss: 22.030242919921875\n",
      "Batch 300, Loss: 20.392578125\n",
      "Batch 400, Loss: 20.555404663085938\n",
      "Batch 500, Loss: 20.56049156188965\n",
      "Batch 600, Loss: 21.1370849609375\n",
      "Batch 700, Loss: 21.110755920410156\n",
      "Batch 800, Loss: 21.391937255859375\n",
      "Batch 900, Loss: 20.42407989501953\n",
      "Batch 1000, Loss: 21.504995346069336\n",
      "Batch 1100, Loss: 21.76231575012207\n",
      "Batch 1200, Loss: 21.19417953491211\n",
      "Batch 1300, Loss: 21.724811553955078\n",
      "Batch 1400, Loss: 20.34016227722168\n",
      "Batch 1500, Loss: 19.791507720947266\n",
      "Batch 1600, Loss: 21.304668426513672\n",
      "Batch 1700, Loss: 22.315689086914062\n",
      "Batch 1800, Loss: 20.257705688476562\n",
      "Batch 1900, Loss: 21.91256332397461\n",
      "Batch 2000, Loss: 20.238618850708008\n",
      "Batch 2100, Loss: 19.92148208618164\n",
      "Batch 2200, Loss: 21.49820327758789\n",
      "Batch 2300, Loss: 21.725460052490234\n",
      "Batch 2400, Loss: 20.79733657836914\n",
      "Batch 2500, Loss: 20.784164428710938\n",
      "Batch 2600, Loss: 21.398168563842773\n",
      "Batch 2700, Loss: 21.79254722595215\n",
      "Batch 2800, Loss: 21.431509017944336\n",
      "Batch 2900, Loss: 20.17751693725586\n",
      "Batch 3000, Loss: 20.73556137084961\n",
      "Batch 3100, Loss: 21.789566040039062\n",
      "Average Training Loss: 21.228923668994856\n",
      "Average Validation Loss: 22.02219372607292\n",
      "Epoch 62/100\n",
      "----------\n",
      "Batch 0, Loss: 20.404132843017578\n",
      "Batch 100, Loss: 20.929094314575195\n",
      "Batch 200, Loss: 21.727783203125\n",
      "Batch 300, Loss: 20.749549865722656\n",
      "Batch 400, Loss: 21.0459041595459\n",
      "Batch 500, Loss: 20.947547912597656\n",
      "Batch 600, Loss: 20.337196350097656\n",
      "Batch 700, Loss: 20.105430603027344\n",
      "Batch 800, Loss: 20.251758575439453\n",
      "Batch 900, Loss: 21.490768432617188\n",
      "Batch 1000, Loss: 19.76001739501953\n",
      "Batch 1100, Loss: 21.04990005493164\n",
      "Batch 1200, Loss: 20.799724578857422\n",
      "Batch 1300, Loss: 21.088401794433594\n",
      "Batch 1400, Loss: 20.36908721923828\n",
      "Batch 1500, Loss: 21.079788208007812\n",
      "Batch 1600, Loss: 21.468366622924805\n",
      "Batch 1700, Loss: 21.208919525146484\n",
      "Batch 1800, Loss: 21.223800659179688\n",
      "Batch 1900, Loss: 21.2515869140625\n",
      "Batch 2000, Loss: 21.594322204589844\n",
      "Batch 2100, Loss: 21.878997802734375\n",
      "Batch 2200, Loss: 21.815122604370117\n",
      "Batch 2300, Loss: 21.16781234741211\n",
      "Batch 2400, Loss: 22.119609832763672\n",
      "Batch 2500, Loss: 20.757909774780273\n",
      "Batch 2600, Loss: 20.864418029785156\n",
      "Batch 2700, Loss: 20.104248046875\n",
      "Batch 2800, Loss: 22.222755432128906\n",
      "Batch 2900, Loss: 19.999759674072266\n",
      "Batch 3000, Loss: 20.592548370361328\n",
      "Batch 3100, Loss: 20.95288848876953\n",
      "Average Training Loss: 21.227677621307567\n",
      "Average Validation Loss: 22.021515407968074\n",
      "Epoch 63/100\n",
      "----------\n",
      "Batch 0, Loss: 21.341833114624023\n",
      "Batch 100, Loss: 20.81747055053711\n",
      "Batch 200, Loss: 21.189125061035156\n",
      "Batch 300, Loss: 21.64234161376953\n",
      "Batch 400, Loss: 20.73415756225586\n",
      "Batch 500, Loss: 20.50337028503418\n",
      "Batch 600, Loss: 22.411087036132812\n",
      "Batch 700, Loss: 21.289535522460938\n",
      "Batch 800, Loss: 19.72919464111328\n",
      "Batch 900, Loss: 21.89048194885254\n",
      "Batch 1000, Loss: 21.21231460571289\n",
      "Batch 1100, Loss: 21.608701705932617\n",
      "Batch 1200, Loss: 20.100521087646484\n",
      "Batch 1300, Loss: 21.48601531982422\n",
      "Batch 1400, Loss: 20.9188289642334\n",
      "Batch 1500, Loss: 21.66059684753418\n",
      "Batch 1600, Loss: 20.127965927124023\n",
      "Batch 1700, Loss: 21.58553695678711\n",
      "Batch 1800, Loss: 20.581451416015625\n",
      "Batch 1900, Loss: 21.256999969482422\n",
      "Batch 2000, Loss: 21.215042114257812\n",
      "Batch 2100, Loss: 20.03317642211914\n",
      "Batch 2200, Loss: 21.958040237426758\n",
      "Batch 2300, Loss: 20.364234924316406\n",
      "Batch 2400, Loss: 21.794464111328125\n",
      "Batch 2500, Loss: 20.64959716796875\n",
      "Batch 2600, Loss: 21.148712158203125\n",
      "Batch 2700, Loss: 20.107284545898438\n",
      "Batch 2800, Loss: 21.305992126464844\n",
      "Batch 2900, Loss: 22.300251007080078\n",
      "Batch 3000, Loss: 20.403369903564453\n",
      "Batch 3100, Loss: 21.582035064697266\n",
      "Average Training Loss: 21.22832932484362\n",
      "Average Validation Loss: 22.025283326493934\n",
      "Epoch 64/100\n",
      "----------\n",
      "Batch 0, Loss: 20.478729248046875\n",
      "Batch 100, Loss: 21.828277587890625\n",
      "Batch 200, Loss: 21.004730224609375\n",
      "Batch 300, Loss: 21.316532135009766\n",
      "Batch 400, Loss: 22.24480438232422\n",
      "Batch 500, Loss: 21.347028732299805\n",
      "Batch 600, Loss: 20.311115264892578\n",
      "Batch 700, Loss: 20.79742431640625\n",
      "Batch 800, Loss: 20.403560638427734\n",
      "Batch 900, Loss: 21.489463806152344\n",
      "Batch 1000, Loss: 20.224687576293945\n",
      "Batch 1100, Loss: 22.257896423339844\n",
      "Batch 1200, Loss: 21.269046783447266\n",
      "Batch 1300, Loss: 23.331584930419922\n",
      "Batch 1400, Loss: 20.31198501586914\n",
      "Batch 1500, Loss: 21.758899688720703\n",
      "Batch 1600, Loss: 21.393543243408203\n",
      "Batch 1700, Loss: 21.420238494873047\n",
      "Batch 1800, Loss: 20.8248291015625\n",
      "Batch 1900, Loss: 21.19631576538086\n",
      "Batch 2000, Loss: 20.740211486816406\n",
      "Batch 2100, Loss: 21.456317901611328\n",
      "Batch 2200, Loss: 20.960268020629883\n",
      "Batch 2300, Loss: 22.111574172973633\n",
      "Batch 2400, Loss: 21.368755340576172\n",
      "Batch 2500, Loss: 20.69119644165039\n",
      "Batch 2600, Loss: 21.39084243774414\n",
      "Batch 2700, Loss: 22.013736724853516\n",
      "Batch 2800, Loss: 20.595340728759766\n",
      "Batch 2900, Loss: 21.72028160095215\n",
      "Batch 3000, Loss: 22.019031524658203\n",
      "Batch 3100, Loss: 20.819473266601562\n",
      "Average Training Loss: 21.228402734106126\n",
      "Average Validation Loss: 22.024667196070894\n",
      "Epoch 65/100\n",
      "----------\n",
      "Batch 0, Loss: 20.57321548461914\n",
      "Batch 100, Loss: 21.388410568237305\n",
      "Batch 200, Loss: 20.415203094482422\n",
      "Batch 300, Loss: 20.302154541015625\n",
      "Batch 400, Loss: 21.421566009521484\n",
      "Batch 500, Loss: 21.481563568115234\n",
      "Batch 600, Loss: 21.274887084960938\n",
      "Batch 700, Loss: 21.611534118652344\n",
      "Batch 800, Loss: 21.861183166503906\n",
      "Batch 900, Loss: 22.78860092163086\n",
      "Batch 1000, Loss: 21.89251136779785\n",
      "Batch 1100, Loss: 21.715084075927734\n",
      "Batch 1200, Loss: 21.05148696899414\n",
      "Batch 1300, Loss: 21.256275177001953\n",
      "Batch 1400, Loss: 20.97522735595703\n",
      "Batch 1500, Loss: 20.993133544921875\n",
      "Batch 1600, Loss: 20.331424713134766\n",
      "Batch 1700, Loss: 20.55975341796875\n",
      "Batch 1800, Loss: 20.866943359375\n",
      "Batch 1900, Loss: 20.22620391845703\n",
      "Batch 2000, Loss: 20.723058700561523\n",
      "Batch 2100, Loss: 21.223751068115234\n",
      "Batch 2200, Loss: 22.342418670654297\n",
      "Batch 2300, Loss: 22.351530075073242\n",
      "Batch 2400, Loss: 21.411239624023438\n",
      "Batch 2500, Loss: 21.5421142578125\n",
      "Batch 2600, Loss: 20.851303100585938\n",
      "Batch 2700, Loss: 20.382150650024414\n",
      "Batch 2800, Loss: 20.66902732849121\n",
      "Batch 2900, Loss: 21.501949310302734\n",
      "Batch 3000, Loss: 21.00779914855957\n",
      "Batch 3100, Loss: 22.47898292541504\n",
      "Average Training Loss: 21.228979196257264\n",
      "Average Validation Loss: 22.024741749053305\n",
      "Epoch 66/100\n",
      "----------\n",
      "Batch 0, Loss: 20.562644958496094\n",
      "Batch 100, Loss: 20.601058959960938\n",
      "Batch 200, Loss: 23.24010467529297\n",
      "Batch 300, Loss: 20.42041778564453\n",
      "Batch 400, Loss: 19.723724365234375\n",
      "Batch 500, Loss: 22.443077087402344\n",
      "Batch 600, Loss: 20.95861053466797\n",
      "Batch 700, Loss: 20.154857635498047\n",
      "Batch 800, Loss: 21.440967559814453\n",
      "Batch 900, Loss: 20.67005157470703\n",
      "Batch 1000, Loss: 21.256576538085938\n",
      "Batch 1100, Loss: 21.16295623779297\n",
      "Batch 1200, Loss: 21.969764709472656\n",
      "Batch 1300, Loss: 21.461284637451172\n",
      "Batch 1400, Loss: 23.210102081298828\n",
      "Batch 1500, Loss: 21.30382537841797\n",
      "Batch 1600, Loss: 20.535507202148438\n",
      "Batch 1700, Loss: 20.716501235961914\n",
      "Batch 1800, Loss: 20.853290557861328\n",
      "Batch 1900, Loss: 21.729122161865234\n",
      "Batch 2000, Loss: 20.071414947509766\n",
      "Batch 2100, Loss: 20.613109588623047\n",
      "Batch 2200, Loss: 21.509977340698242\n",
      "Batch 2300, Loss: 20.849262237548828\n",
      "Batch 2400, Loss: 20.874366760253906\n",
      "Batch 2500, Loss: 21.63176155090332\n",
      "Batch 2600, Loss: 22.41551971435547\n",
      "Batch 2700, Loss: 21.0533390045166\n",
      "Batch 2800, Loss: 20.942428588867188\n",
      "Batch 2900, Loss: 22.302284240722656\n",
      "Batch 3000, Loss: 22.046039581298828\n",
      "Batch 3100, Loss: 20.863941192626953\n",
      "Average Training Loss: 21.22942495042738\n",
      "Average Validation Loss: 22.01863630984692\n",
      "Epoch 67/100\n",
      "----------\n",
      "Batch 0, Loss: 21.392311096191406\n",
      "Batch 100, Loss: 20.93793487548828\n",
      "Batch 200, Loss: 23.19698715209961\n",
      "Batch 300, Loss: 20.534076690673828\n",
      "Batch 400, Loss: 20.45652198791504\n",
      "Batch 500, Loss: 21.597246170043945\n",
      "Batch 600, Loss: 21.83038330078125\n",
      "Batch 700, Loss: 20.608238220214844\n",
      "Batch 800, Loss: 19.961132049560547\n",
      "Batch 900, Loss: 21.526809692382812\n",
      "Batch 1000, Loss: 21.48794937133789\n",
      "Batch 1100, Loss: 21.78757095336914\n",
      "Batch 1200, Loss: 20.74282455444336\n",
      "Batch 1300, Loss: 21.455102920532227\n",
      "Batch 1400, Loss: 22.313758850097656\n",
      "Batch 1500, Loss: 21.10582733154297\n",
      "Batch 1600, Loss: 21.479585647583008\n",
      "Batch 1700, Loss: 20.544809341430664\n",
      "Batch 1800, Loss: 21.721118927001953\n",
      "Batch 1900, Loss: 19.835067749023438\n",
      "Batch 2000, Loss: 20.845190048217773\n",
      "Batch 2100, Loss: 21.132322311401367\n",
      "Batch 2200, Loss: 21.603717803955078\n",
      "Batch 2300, Loss: 21.75757598876953\n",
      "Batch 2400, Loss: 20.95547866821289\n",
      "Batch 2500, Loss: 21.791152954101562\n",
      "Batch 2600, Loss: 20.801170349121094\n",
      "Batch 2700, Loss: 21.554473876953125\n",
      "Batch 2800, Loss: 21.144498825073242\n",
      "Batch 2900, Loss: 21.196922302246094\n",
      "Batch 3000, Loss: 21.17989730834961\n",
      "Batch 3100, Loss: 20.775062561035156\n",
      "Average Training Loss: 21.227851228252927\n",
      "Average Validation Loss: 22.02137979953847\n",
      "Epoch 68/100\n",
      "----------\n",
      "Batch 0, Loss: 21.53965950012207\n",
      "Batch 100, Loss: 20.969642639160156\n",
      "Batch 200, Loss: 19.56108856201172\n",
      "Batch 300, Loss: 20.785221099853516\n",
      "Batch 400, Loss: 20.781757354736328\n",
      "Batch 500, Loss: 20.21182632446289\n",
      "Batch 600, Loss: 21.75770378112793\n",
      "Batch 700, Loss: 21.975662231445312\n",
      "Batch 800, Loss: 22.085159301757812\n",
      "Batch 900, Loss: 21.987934112548828\n",
      "Batch 1000, Loss: 21.149885177612305\n",
      "Batch 1100, Loss: 21.44736671447754\n",
      "Batch 1200, Loss: 21.34334945678711\n",
      "Batch 1300, Loss: 20.682802200317383\n",
      "Batch 1400, Loss: 20.998634338378906\n",
      "Batch 1500, Loss: 21.04068374633789\n",
      "Batch 1600, Loss: 21.167470932006836\n",
      "Batch 1700, Loss: 21.428298950195312\n",
      "Batch 1800, Loss: 21.760074615478516\n",
      "Batch 1900, Loss: 21.96601676940918\n",
      "Batch 2000, Loss: 21.07082748413086\n",
      "Batch 2100, Loss: 22.048429489135742\n",
      "Batch 2200, Loss: 22.40125846862793\n",
      "Batch 2300, Loss: 21.902557373046875\n",
      "Batch 2400, Loss: 20.125450134277344\n",
      "Batch 2500, Loss: 19.86841583251953\n",
      "Batch 2600, Loss: 21.08510398864746\n",
      "Batch 2700, Loss: 20.632488250732422\n",
      "Batch 2800, Loss: 20.991317749023438\n",
      "Batch 2900, Loss: 21.065284729003906\n",
      "Batch 3000, Loss: 22.40999412536621\n",
      "Batch 3100, Loss: 22.153594970703125\n",
      "Average Training Loss: 21.228003990862508\n",
      "Average Validation Loss: 22.022213554382326\n",
      "Epoch 69/100\n",
      "----------\n",
      "Batch 0, Loss: 21.058170318603516\n",
      "Batch 100, Loss: 20.48736000061035\n",
      "Batch 200, Loss: 21.38428497314453\n",
      "Batch 300, Loss: 19.834884643554688\n",
      "Batch 400, Loss: 22.075801849365234\n",
      "Batch 500, Loss: 20.247058868408203\n",
      "Batch 600, Loss: 22.083768844604492\n",
      "Batch 700, Loss: 20.319337844848633\n",
      "Batch 800, Loss: 20.095077514648438\n",
      "Batch 900, Loss: 20.429349899291992\n",
      "Batch 1000, Loss: 20.96858024597168\n",
      "Batch 1100, Loss: 20.62894058227539\n",
      "Batch 1200, Loss: 21.51518440246582\n",
      "Batch 1300, Loss: 21.33620834350586\n",
      "Batch 1400, Loss: 21.18539047241211\n",
      "Batch 1500, Loss: 20.963008880615234\n",
      "Batch 1600, Loss: 22.07924461364746\n",
      "Batch 1700, Loss: 20.726703643798828\n",
      "Batch 1800, Loss: 22.414766311645508\n",
      "Batch 1900, Loss: 20.73495101928711\n",
      "Batch 2000, Loss: 20.16263771057129\n",
      "Batch 2100, Loss: 21.484821319580078\n",
      "Batch 2200, Loss: 20.303504943847656\n",
      "Batch 2300, Loss: 22.017507553100586\n",
      "Batch 2400, Loss: 21.263023376464844\n",
      "Batch 2500, Loss: 20.27860450744629\n",
      "Batch 2600, Loss: 21.74867820739746\n",
      "Batch 2700, Loss: 22.129100799560547\n",
      "Batch 2800, Loss: 21.627628326416016\n",
      "Batch 2900, Loss: 21.29730987548828\n",
      "Batch 3000, Loss: 21.55054473876953\n",
      "Batch 3100, Loss: 22.435640335083008\n",
      "Average Training Loss: 21.22801957906961\n",
      "Average Validation Loss: 22.02705215697593\n",
      "Epoch 70/100\n",
      "----------\n",
      "Batch 0, Loss: 20.947612762451172\n",
      "Batch 100, Loss: 21.793716430664062\n",
      "Batch 200, Loss: 22.39142608642578\n",
      "Batch 300, Loss: 20.527572631835938\n",
      "Batch 400, Loss: 20.195425033569336\n",
      "Batch 500, Loss: 20.50414276123047\n",
      "Batch 600, Loss: 21.050251007080078\n",
      "Batch 700, Loss: 20.482608795166016\n",
      "Batch 800, Loss: 21.6220703125\n",
      "Batch 900, Loss: 21.939279556274414\n",
      "Batch 1000, Loss: 22.072998046875\n",
      "Batch 1100, Loss: 21.222944259643555\n",
      "Batch 1200, Loss: 21.182727813720703\n",
      "Batch 1300, Loss: 21.034713745117188\n",
      "Batch 1400, Loss: 21.812580108642578\n",
      "Batch 1500, Loss: 19.82990264892578\n",
      "Batch 1600, Loss: 20.804195404052734\n",
      "Batch 1700, Loss: 20.338485717773438\n",
      "Batch 1800, Loss: 22.108627319335938\n",
      "Batch 1900, Loss: 20.683839797973633\n",
      "Batch 2000, Loss: 21.864582061767578\n",
      "Batch 2100, Loss: 20.08967399597168\n",
      "Batch 2200, Loss: 21.54297637939453\n",
      "Batch 2300, Loss: 21.150402069091797\n",
      "Batch 2400, Loss: 22.021162033081055\n",
      "Batch 2500, Loss: 20.480846405029297\n",
      "Batch 2600, Loss: 21.869596481323242\n",
      "Batch 2700, Loss: 20.647912979125977\n",
      "Batch 2800, Loss: 22.170194625854492\n",
      "Batch 2900, Loss: 21.039321899414062\n",
      "Batch 3000, Loss: 21.464065551757812\n",
      "Batch 3100, Loss: 21.831050872802734\n",
      "Average Training Loss: 21.227994949156393\n",
      "Average Validation Loss: 22.022761417957064\n",
      "Epoch 71/100\n",
      "----------\n",
      "Batch 0, Loss: 20.7882080078125\n",
      "Batch 100, Loss: 22.170860290527344\n",
      "Batch 200, Loss: 20.94517707824707\n",
      "Batch 300, Loss: 22.20610809326172\n",
      "Batch 400, Loss: 20.499340057373047\n",
      "Batch 500, Loss: 20.582300186157227\n",
      "Batch 600, Loss: 21.26779556274414\n",
      "Batch 700, Loss: 21.11276626586914\n",
      "Batch 800, Loss: 22.04827880859375\n",
      "Batch 900, Loss: 21.013957977294922\n",
      "Batch 1000, Loss: 20.266494750976562\n",
      "Batch 1100, Loss: 21.13951873779297\n",
      "Batch 1200, Loss: 21.8205623626709\n",
      "Batch 1300, Loss: 21.68875503540039\n",
      "Batch 1400, Loss: 21.9151611328125\n",
      "Batch 1500, Loss: 20.779560089111328\n",
      "Batch 1600, Loss: 21.04692268371582\n",
      "Batch 1700, Loss: 21.146312713623047\n",
      "Batch 1800, Loss: 22.41996955871582\n",
      "Batch 1900, Loss: 20.445274353027344\n",
      "Batch 2000, Loss: 21.651241302490234\n",
      "Batch 2100, Loss: 21.2220458984375\n",
      "Batch 2200, Loss: 21.33721351623535\n",
      "Batch 2300, Loss: 22.00970458984375\n",
      "Batch 2400, Loss: 21.814605712890625\n",
      "Batch 2500, Loss: 21.486339569091797\n",
      "Batch 2600, Loss: 22.268177032470703\n",
      "Batch 2700, Loss: 21.71844482421875\n",
      "Batch 2800, Loss: 21.845569610595703\n",
      "Batch 2900, Loss: 21.476900100708008\n",
      "Batch 3000, Loss: 21.675743103027344\n",
      "Batch 3100, Loss: 21.715843200683594\n",
      "Average Training Loss: 21.228670909507887\n",
      "Average Validation Loss: 22.019805514558833\n",
      "Epoch 72/100\n",
      "----------\n",
      "Batch 0, Loss: 21.63274574279785\n",
      "Batch 100, Loss: 22.09148406982422\n",
      "Batch 200, Loss: 21.473369598388672\n",
      "Batch 300, Loss: 20.348817825317383\n",
      "Batch 400, Loss: 22.768266677856445\n",
      "Batch 500, Loss: 21.974864959716797\n",
      "Batch 600, Loss: 20.754179000854492\n",
      "Batch 700, Loss: 21.68425750732422\n",
      "Batch 800, Loss: 21.118648529052734\n",
      "Batch 900, Loss: 20.70197296142578\n",
      "Batch 1000, Loss: 21.679994583129883\n",
      "Batch 1100, Loss: 22.725969314575195\n",
      "Batch 1200, Loss: 20.95684051513672\n",
      "Batch 1300, Loss: 22.496862411499023\n",
      "Batch 1400, Loss: 22.14165496826172\n",
      "Batch 1500, Loss: 20.210147857666016\n",
      "Batch 1600, Loss: 20.424076080322266\n",
      "Batch 1700, Loss: 20.906755447387695\n",
      "Batch 1800, Loss: 20.996334075927734\n",
      "Batch 1900, Loss: 21.789657592773438\n",
      "Batch 2000, Loss: 22.895267486572266\n",
      "Batch 2100, Loss: 21.464550018310547\n",
      "Batch 2200, Loss: 20.442974090576172\n",
      "Batch 2300, Loss: 20.77219009399414\n",
      "Batch 2400, Loss: 19.375370025634766\n",
      "Batch 2500, Loss: 20.848648071289062\n",
      "Batch 2600, Loss: 21.501869201660156\n",
      "Batch 2700, Loss: 20.812294006347656\n",
      "Batch 2800, Loss: 21.416770935058594\n",
      "Batch 2900, Loss: 21.296646118164062\n",
      "Batch 3000, Loss: 21.722877502441406\n",
      "Batch 3100, Loss: 21.800750732421875\n",
      "Average Training Loss: 21.227199482250455\n",
      "Average Validation Loss: 22.020368316325737\n",
      "Epoch 73/100\n",
      "----------\n",
      "Batch 0, Loss: 21.248777389526367\n",
      "Batch 100, Loss: 20.944583892822266\n",
      "Batch 200, Loss: 20.78523063659668\n",
      "Batch 300, Loss: 20.91156005859375\n",
      "Batch 400, Loss: 20.470264434814453\n",
      "Batch 500, Loss: 21.16646385192871\n",
      "Batch 600, Loss: 20.655628204345703\n",
      "Batch 700, Loss: 20.745059967041016\n",
      "Batch 800, Loss: 20.75045394897461\n",
      "Batch 900, Loss: 20.14874267578125\n",
      "Batch 1000, Loss: 22.040939331054688\n",
      "Batch 1100, Loss: 21.752817153930664\n",
      "Batch 1200, Loss: 20.241609573364258\n",
      "Batch 1300, Loss: 21.585693359375\n",
      "Batch 1400, Loss: 22.304996490478516\n",
      "Batch 1500, Loss: 21.72135353088379\n",
      "Batch 1600, Loss: 20.974536895751953\n",
      "Batch 1700, Loss: 21.276477813720703\n",
      "Batch 1800, Loss: 21.266258239746094\n",
      "Batch 1900, Loss: 22.469810485839844\n",
      "Batch 2000, Loss: 20.83960723876953\n",
      "Batch 2100, Loss: 19.782596588134766\n",
      "Batch 2200, Loss: 20.830549240112305\n",
      "Batch 2300, Loss: 19.262868881225586\n",
      "Batch 2400, Loss: 22.513864517211914\n",
      "Batch 2500, Loss: 21.318374633789062\n",
      "Batch 2600, Loss: 22.385831832885742\n",
      "Batch 2700, Loss: 21.294694900512695\n",
      "Batch 2800, Loss: 22.496156692504883\n",
      "Batch 2900, Loss: 20.658660888671875\n",
      "Batch 3000, Loss: 22.39773178100586\n",
      "Batch 3100, Loss: 21.20574188232422\n",
      "Average Training Loss: 21.228503413479444\n",
      "Average Validation Loss: 22.020813174957926\n",
      "Epoch 74/100\n",
      "----------\n",
      "Batch 0, Loss: 21.003583908081055\n",
      "Batch 100, Loss: 22.104598999023438\n",
      "Batch 200, Loss: 21.191322326660156\n",
      "Batch 300, Loss: 20.443889617919922\n",
      "Batch 400, Loss: 20.43255615234375\n",
      "Batch 500, Loss: 21.156824111938477\n",
      "Batch 600, Loss: 21.664173126220703\n",
      "Batch 700, Loss: 20.710628509521484\n",
      "Batch 800, Loss: 20.65643310546875\n",
      "Batch 900, Loss: 21.778823852539062\n",
      "Batch 1000, Loss: 21.647836685180664\n",
      "Batch 1100, Loss: 21.162656784057617\n",
      "Batch 1200, Loss: 21.102943420410156\n",
      "Batch 1300, Loss: 21.113203048706055\n",
      "Batch 1400, Loss: 21.887035369873047\n",
      "Batch 1500, Loss: 21.748077392578125\n",
      "Batch 1600, Loss: 21.792850494384766\n",
      "Batch 1700, Loss: 20.971900939941406\n",
      "Batch 1800, Loss: 21.634300231933594\n",
      "Batch 1900, Loss: 20.985702514648438\n",
      "Batch 2000, Loss: 21.534652709960938\n",
      "Batch 2100, Loss: 21.143646240234375\n",
      "Batch 2200, Loss: 20.983627319335938\n",
      "Batch 2300, Loss: 20.84978485107422\n",
      "Batch 2400, Loss: 20.195419311523438\n",
      "Batch 2500, Loss: 21.278430938720703\n",
      "Batch 2600, Loss: 22.172954559326172\n",
      "Batch 2700, Loss: 21.690948486328125\n",
      "Batch 2800, Loss: 21.493013381958008\n",
      "Batch 2900, Loss: 21.013057708740234\n",
      "Batch 3000, Loss: 21.87548065185547\n",
      "Batch 3100, Loss: 20.680885314941406\n",
      "Average Training Loss: 21.227740665731844\n",
      "Average Validation Loss: 22.0203138148531\n",
      "Epoch 75/100\n",
      "----------\n",
      "Batch 0, Loss: 21.09872055053711\n",
      "Batch 100, Loss: 20.86456871032715\n",
      "Batch 200, Loss: 20.948299407958984\n",
      "Batch 300, Loss: 21.524791717529297\n",
      "Batch 400, Loss: 21.39388084411621\n",
      "Batch 500, Loss: 22.293529510498047\n",
      "Batch 600, Loss: 20.435224533081055\n",
      "Batch 700, Loss: 21.17775535583496\n",
      "Batch 800, Loss: 21.300622940063477\n",
      "Batch 900, Loss: 21.45931625366211\n",
      "Batch 1000, Loss: 20.91266441345215\n",
      "Batch 1100, Loss: 21.302833557128906\n",
      "Batch 1200, Loss: 20.83824920654297\n",
      "Batch 1300, Loss: 22.258480072021484\n",
      "Batch 1400, Loss: 20.410659790039062\n",
      "Batch 1500, Loss: 22.192882537841797\n",
      "Batch 1600, Loss: 20.414995193481445\n",
      "Batch 1700, Loss: 20.441688537597656\n",
      "Batch 1800, Loss: 21.1751708984375\n",
      "Batch 1900, Loss: 22.209014892578125\n",
      "Batch 2000, Loss: 22.44514274597168\n",
      "Batch 2100, Loss: 21.408245086669922\n",
      "Batch 2200, Loss: 21.88486671447754\n",
      "Batch 2300, Loss: 22.36282730102539\n",
      "Batch 2400, Loss: 21.366073608398438\n",
      "Batch 2500, Loss: 21.70519256591797\n",
      "Batch 2600, Loss: 20.959625244140625\n",
      "Batch 2700, Loss: 21.318187713623047\n",
      "Batch 2800, Loss: 21.26447296142578\n",
      "Batch 2900, Loss: 20.934085845947266\n",
      "Batch 3000, Loss: 21.945117950439453\n",
      "Batch 3100, Loss: 21.442882537841797\n",
      "Average Training Loss: 21.227972544483254\n",
      "Average Validation Loss: 22.022164981923204\n",
      "Epoch 76/100\n",
      "----------\n",
      "Batch 0, Loss: 22.015056610107422\n",
      "Batch 100, Loss: 21.6932373046875\n",
      "Batch 200, Loss: 20.64657211303711\n",
      "Batch 300, Loss: 20.910655975341797\n",
      "Batch 400, Loss: 20.369335174560547\n",
      "Batch 500, Loss: 20.380664825439453\n",
      "Batch 600, Loss: 21.10972023010254\n",
      "Batch 700, Loss: 21.60995101928711\n",
      "Batch 800, Loss: 20.03592300415039\n",
      "Batch 900, Loss: 21.40376091003418\n",
      "Batch 1000, Loss: 23.086181640625\n",
      "Batch 1100, Loss: 19.64598846435547\n",
      "Batch 1200, Loss: 21.633676528930664\n",
      "Batch 1300, Loss: 21.84483528137207\n",
      "Batch 1400, Loss: 21.159135818481445\n",
      "Batch 1500, Loss: 21.429100036621094\n",
      "Batch 1600, Loss: 21.9591121673584\n",
      "Batch 1700, Loss: 21.48056983947754\n",
      "Batch 1800, Loss: 20.929784774780273\n",
      "Batch 1900, Loss: 20.86026382446289\n",
      "Batch 2000, Loss: 20.647171020507812\n",
      "Batch 2100, Loss: 21.385223388671875\n",
      "Batch 2200, Loss: 21.041046142578125\n",
      "Batch 2300, Loss: 22.0587100982666\n",
      "Batch 2400, Loss: 22.11565399169922\n",
      "Batch 2500, Loss: 22.292194366455078\n",
      "Batch 2600, Loss: 22.419504165649414\n",
      "Batch 2700, Loss: 20.819103240966797\n",
      "Batch 2800, Loss: 20.85802459716797\n",
      "Batch 2900, Loss: 20.71006202697754\n",
      "Batch 3000, Loss: 21.325977325439453\n",
      "Batch 3100, Loss: 21.99859619140625\n",
      "Average Training Loss: 21.22840226940223\n",
      "Average Validation Loss: 22.02194567335413\n",
      "Epoch 77/100\n",
      "----------\n",
      "Batch 0, Loss: 21.472219467163086\n",
      "Batch 100, Loss: 20.456451416015625\n",
      "Batch 200, Loss: 21.117151260375977\n",
      "Batch 300, Loss: 21.52802276611328\n",
      "Batch 400, Loss: 21.253360748291016\n",
      "Batch 500, Loss: 21.608287811279297\n",
      "Batch 600, Loss: 21.82470703125\n",
      "Batch 700, Loss: 21.778827667236328\n",
      "Batch 800, Loss: 21.659454345703125\n",
      "Batch 900, Loss: 21.032798767089844\n",
      "Batch 1000, Loss: 21.985031127929688\n",
      "Batch 1100, Loss: 21.791818618774414\n",
      "Batch 1200, Loss: 20.558961868286133\n",
      "Batch 1300, Loss: 20.893733978271484\n",
      "Batch 1400, Loss: 21.688674926757812\n",
      "Batch 1500, Loss: 22.037731170654297\n",
      "Batch 1600, Loss: 20.226654052734375\n",
      "Batch 1700, Loss: 20.359943389892578\n",
      "Batch 1800, Loss: 20.98874282836914\n",
      "Batch 1900, Loss: 20.851993560791016\n",
      "Batch 2000, Loss: 20.00579071044922\n",
      "Batch 2100, Loss: 21.98080062866211\n",
      "Batch 2200, Loss: 22.11450958251953\n",
      "Batch 2300, Loss: 21.186176300048828\n",
      "Batch 2400, Loss: 21.073650360107422\n",
      "Batch 2500, Loss: 21.976974487304688\n",
      "Batch 2600, Loss: 20.73383331298828\n",
      "Batch 2700, Loss: 20.350406646728516\n",
      "Batch 2800, Loss: 20.97229766845703\n",
      "Batch 2900, Loss: 20.520217895507812\n",
      "Batch 3000, Loss: 21.085304260253906\n",
      "Batch 3100, Loss: 20.35271644592285\n",
      "Average Training Loss: 21.22866312541428\n",
      "Average Validation Loss: 22.021595946778643\n",
      "Epoch 78/100\n",
      "----------\n",
      "Batch 0, Loss: 21.218040466308594\n",
      "Batch 100, Loss: 20.584278106689453\n",
      "Batch 200, Loss: 21.064315795898438\n",
      "Batch 300, Loss: 21.215145111083984\n",
      "Batch 400, Loss: 20.72728729248047\n",
      "Batch 500, Loss: 20.200830459594727\n",
      "Batch 600, Loss: 21.683639526367188\n",
      "Batch 700, Loss: 20.800525665283203\n",
      "Batch 800, Loss: 22.314088821411133\n",
      "Batch 900, Loss: 21.78284454345703\n",
      "Batch 1000, Loss: 21.009342193603516\n",
      "Batch 1100, Loss: 21.531417846679688\n",
      "Batch 1200, Loss: 20.895824432373047\n",
      "Batch 1300, Loss: 21.321849822998047\n",
      "Batch 1400, Loss: 20.86065673828125\n",
      "Batch 1500, Loss: 20.09356689453125\n",
      "Batch 1600, Loss: 21.30476951599121\n",
      "Batch 1700, Loss: 20.931278228759766\n",
      "Batch 1800, Loss: 21.333168029785156\n",
      "Batch 1900, Loss: 21.62656021118164\n",
      "Batch 2000, Loss: 20.41461944580078\n",
      "Batch 2100, Loss: 21.928531646728516\n",
      "Batch 2200, Loss: 20.684459686279297\n",
      "Batch 2300, Loss: 22.221574783325195\n",
      "Batch 2400, Loss: 21.913070678710938\n",
      "Batch 2500, Loss: 20.607067108154297\n",
      "Batch 2600, Loss: 20.74359703063965\n",
      "Batch 2700, Loss: 21.303882598876953\n",
      "Batch 2800, Loss: 21.170957565307617\n",
      "Batch 2900, Loss: 20.986242294311523\n",
      "Batch 3000, Loss: 21.134479522705078\n",
      "Batch 3100, Loss: 22.66594696044922\n",
      "Average Training Loss: 21.230010381485062\n",
      "Average Validation Loss: 22.021837782352527\n",
      "Epoch 79/100\n",
      "----------\n",
      "Batch 0, Loss: 21.46276092529297\n",
      "Batch 100, Loss: 21.293134689331055\n",
      "Batch 200, Loss: 21.539413452148438\n",
      "Batch 300, Loss: 21.366825103759766\n",
      "Batch 400, Loss: 21.879098892211914\n",
      "Batch 500, Loss: 21.51950454711914\n",
      "Batch 600, Loss: 21.471519470214844\n",
      "Batch 700, Loss: 21.798866271972656\n",
      "Batch 800, Loss: 21.551198959350586\n",
      "Batch 900, Loss: 21.632930755615234\n",
      "Batch 1000, Loss: 21.216882705688477\n",
      "Batch 1100, Loss: 21.27468490600586\n",
      "Batch 1200, Loss: 21.184484481811523\n",
      "Batch 1300, Loss: 19.234375\n",
      "Batch 1400, Loss: 21.022705078125\n",
      "Batch 1500, Loss: 22.61566925048828\n",
      "Batch 1600, Loss: 21.20801544189453\n",
      "Batch 1700, Loss: 20.844547271728516\n",
      "Batch 1800, Loss: 22.134456634521484\n",
      "Batch 1900, Loss: 20.660484313964844\n",
      "Batch 2000, Loss: 21.299558639526367\n",
      "Batch 2100, Loss: 22.01165008544922\n",
      "Batch 2200, Loss: 20.680252075195312\n",
      "Batch 2300, Loss: 20.62615203857422\n",
      "Batch 2400, Loss: 22.07803726196289\n",
      "Batch 2500, Loss: 20.62592887878418\n",
      "Batch 2600, Loss: 20.761579513549805\n",
      "Batch 2700, Loss: 21.808128356933594\n",
      "Batch 2800, Loss: 21.45602798461914\n",
      "Batch 2900, Loss: 21.138874053955078\n",
      "Batch 3000, Loss: 21.72092056274414\n",
      "Batch 3100, Loss: 21.362518310546875\n",
      "Average Training Loss: 21.228557314884874\n",
      "Average Validation Loss: 22.027476744956157\n",
      "Epoch 80/100\n",
      "----------\n",
      "Batch 0, Loss: 21.78227996826172\n",
      "Batch 100, Loss: 20.828174591064453\n",
      "Batch 200, Loss: 20.928787231445312\n",
      "Batch 300, Loss: 20.074962615966797\n",
      "Batch 400, Loss: 22.302295684814453\n",
      "Batch 500, Loss: 19.887943267822266\n",
      "Batch 600, Loss: 21.02121353149414\n",
      "Batch 700, Loss: 21.663837432861328\n",
      "Batch 800, Loss: 21.55785369873047\n",
      "Batch 900, Loss: 22.021404266357422\n",
      "Batch 1000, Loss: 21.79207992553711\n",
      "Batch 1100, Loss: 20.92810821533203\n",
      "Batch 1200, Loss: 21.062721252441406\n",
      "Batch 1300, Loss: 22.58722686767578\n",
      "Batch 1400, Loss: 22.871158599853516\n",
      "Batch 1500, Loss: 20.115015029907227\n",
      "Batch 1600, Loss: 21.933372497558594\n",
      "Batch 1700, Loss: 21.9984188079834\n",
      "Batch 1800, Loss: 21.77151870727539\n",
      "Batch 1900, Loss: 22.538124084472656\n",
      "Batch 2000, Loss: 20.941946029663086\n",
      "Batch 2100, Loss: 20.776172637939453\n",
      "Batch 2200, Loss: 21.62609100341797\n",
      "Batch 2300, Loss: 20.57769203186035\n",
      "Batch 2400, Loss: 20.094661712646484\n",
      "Batch 2500, Loss: 20.618913650512695\n",
      "Batch 2600, Loss: 21.86715316772461\n",
      "Batch 2700, Loss: 20.752058029174805\n",
      "Batch 2800, Loss: 21.45633888244629\n",
      "Batch 2900, Loss: 22.348175048828125\n",
      "Batch 3000, Loss: 21.0328311920166\n",
      "Batch 3100, Loss: 20.906490325927734\n",
      "Average Training Loss: 21.2281195413672\n",
      "Average Validation Loss: 22.02003885634402\n",
      "Epoch 81/100\n",
      "----------\n",
      "Batch 0, Loss: 20.909881591796875\n",
      "Batch 100, Loss: 21.158945083618164\n",
      "Batch 200, Loss: 21.128116607666016\n",
      "Batch 300, Loss: 22.380840301513672\n",
      "Batch 400, Loss: 21.387470245361328\n",
      "Batch 500, Loss: 23.06808853149414\n",
      "Batch 600, Loss: 21.6156005859375\n",
      "Batch 700, Loss: 21.7905330657959\n",
      "Batch 800, Loss: 21.00106430053711\n",
      "Batch 900, Loss: 21.807796478271484\n",
      "Batch 1000, Loss: 20.511022567749023\n",
      "Batch 1100, Loss: 22.394804000854492\n",
      "Batch 1200, Loss: 20.936410903930664\n",
      "Batch 1300, Loss: 20.99289321899414\n",
      "Batch 1400, Loss: 20.668109893798828\n",
      "Batch 1500, Loss: 22.4992733001709\n",
      "Batch 1600, Loss: 21.64208984375\n",
      "Batch 1700, Loss: 20.922351837158203\n",
      "Batch 1800, Loss: 21.81075668334961\n",
      "Batch 1900, Loss: 21.553178787231445\n",
      "Batch 2000, Loss: 22.211395263671875\n",
      "Batch 2100, Loss: 20.598094940185547\n",
      "Batch 2200, Loss: 21.655288696289062\n",
      "Batch 2300, Loss: 21.024099349975586\n",
      "Batch 2400, Loss: 20.69379425048828\n",
      "Batch 2500, Loss: 20.340373992919922\n",
      "Batch 2600, Loss: 21.291309356689453\n",
      "Batch 2700, Loss: 20.93973731994629\n",
      "Batch 2800, Loss: 20.894088745117188\n",
      "Batch 2900, Loss: 21.149784088134766\n",
      "Batch 3000, Loss: 20.426908493041992\n",
      "Batch 3100, Loss: 21.13694953918457\n",
      "Average Training Loss: 21.227589623618673\n",
      "Average Validation Loss: 22.02646106557643\n",
      "Epoch 82/100\n",
      "----------\n",
      "Batch 0, Loss: 21.435745239257812\n",
      "Batch 100, Loss: 19.496152877807617\n",
      "Batch 200, Loss: 20.689231872558594\n",
      "Batch 300, Loss: 21.849170684814453\n",
      "Batch 400, Loss: 21.428922653198242\n",
      "Batch 500, Loss: 21.365793228149414\n",
      "Batch 600, Loss: 21.503456115722656\n",
      "Batch 700, Loss: 21.411272048950195\n",
      "Batch 800, Loss: 21.951412200927734\n",
      "Batch 900, Loss: 20.326698303222656\n",
      "Batch 1000, Loss: 21.42242431640625\n",
      "Batch 1100, Loss: 20.303565979003906\n",
      "Batch 1200, Loss: 20.757143020629883\n",
      "Batch 1300, Loss: 22.575672149658203\n",
      "Batch 1400, Loss: 21.577800750732422\n",
      "Batch 1500, Loss: 19.550329208374023\n",
      "Batch 1600, Loss: 21.710758209228516\n",
      "Batch 1700, Loss: 20.952655792236328\n",
      "Batch 1800, Loss: 21.309837341308594\n",
      "Batch 1900, Loss: 21.275239944458008\n",
      "Batch 2000, Loss: 22.354806900024414\n",
      "Batch 2100, Loss: 20.66080665588379\n",
      "Batch 2200, Loss: 19.920989990234375\n",
      "Batch 2300, Loss: 20.517566680908203\n",
      "Batch 2400, Loss: 20.619386672973633\n",
      "Batch 2500, Loss: 21.834457397460938\n",
      "Batch 2600, Loss: 20.761655807495117\n",
      "Batch 2700, Loss: 20.936992645263672\n",
      "Batch 2800, Loss: 21.11334800720215\n",
      "Batch 2900, Loss: 20.55071258544922\n",
      "Batch 3000, Loss: 21.863300323486328\n",
      "Batch 3100, Loss: 21.196678161621094\n",
      "Average Training Loss: 21.228364442141\n",
      "Average Validation Loss: 22.0233629429594\n",
      "Epoch 83/100\n",
      "----------\n",
      "Batch 0, Loss: 20.248626708984375\n",
      "Batch 100, Loss: 20.84326171875\n",
      "Batch 200, Loss: 21.64497947692871\n",
      "Batch 300, Loss: 20.977371215820312\n",
      "Batch 400, Loss: 21.93181800842285\n",
      "Batch 500, Loss: 19.87487030029297\n",
      "Batch 600, Loss: 21.090360641479492\n",
      "Batch 700, Loss: 20.769596099853516\n",
      "Batch 800, Loss: 21.424577713012695\n",
      "Batch 900, Loss: 21.772300720214844\n",
      "Batch 1000, Loss: 20.785200119018555\n",
      "Batch 1100, Loss: 20.72804832458496\n",
      "Batch 1200, Loss: 20.93096351623535\n",
      "Batch 1300, Loss: 21.56693458557129\n",
      "Batch 1400, Loss: 21.715469360351562\n",
      "Batch 1500, Loss: 20.514209747314453\n",
      "Batch 1600, Loss: 21.16069984436035\n",
      "Batch 1700, Loss: 21.379650115966797\n",
      "Batch 1800, Loss: 21.576412200927734\n",
      "Batch 1900, Loss: 22.15518569946289\n",
      "Batch 2000, Loss: 21.311744689941406\n",
      "Batch 2100, Loss: 21.955297470092773\n",
      "Batch 2200, Loss: 21.54791259765625\n",
      "Batch 2300, Loss: 22.687538146972656\n",
      "Batch 2400, Loss: 21.118078231811523\n",
      "Batch 2500, Loss: 21.68608856201172\n",
      "Batch 2600, Loss: 19.82000732421875\n",
      "Batch 2700, Loss: 19.99549102783203\n",
      "Batch 2800, Loss: 21.228425979614258\n",
      "Batch 2900, Loss: 21.401195526123047\n",
      "Batch 3000, Loss: 20.970571517944336\n",
      "Batch 3100, Loss: 21.53068733215332\n",
      "Average Training Loss: 21.228845182569277\n",
      "Average Validation Loss: 22.02352863879914\n",
      "Epoch 84/100\n",
      "----------\n",
      "Batch 0, Loss: 21.157333374023438\n",
      "Batch 100, Loss: 21.336374282836914\n",
      "Batch 200, Loss: 22.379627227783203\n",
      "Batch 300, Loss: 21.196468353271484\n",
      "Batch 400, Loss: 21.47660255432129\n",
      "Batch 500, Loss: 21.473295211791992\n",
      "Batch 600, Loss: 20.52315902709961\n",
      "Batch 700, Loss: 21.536245346069336\n",
      "Batch 800, Loss: 21.308177947998047\n",
      "Batch 900, Loss: 20.80801010131836\n",
      "Batch 1000, Loss: 22.17380142211914\n",
      "Batch 1100, Loss: 22.112384796142578\n",
      "Batch 1200, Loss: 21.16283416748047\n",
      "Batch 1300, Loss: 21.13829231262207\n",
      "Batch 1400, Loss: 21.16852569580078\n",
      "Batch 1500, Loss: 21.287818908691406\n",
      "Batch 1600, Loss: 20.75716781616211\n",
      "Batch 1700, Loss: 21.714187622070312\n",
      "Batch 1800, Loss: 21.428184509277344\n",
      "Batch 1900, Loss: 21.37128257751465\n",
      "Batch 2000, Loss: 20.954063415527344\n",
      "Batch 2100, Loss: 21.28754997253418\n",
      "Batch 2200, Loss: 21.669530868530273\n",
      "Batch 2300, Loss: 20.604997634887695\n",
      "Batch 2400, Loss: 21.94831085205078\n",
      "Batch 2500, Loss: 22.276920318603516\n",
      "Batch 2600, Loss: 20.760730743408203\n",
      "Batch 2700, Loss: 21.43951988220215\n",
      "Batch 2800, Loss: 21.10297393798828\n",
      "Batch 2900, Loss: 20.315181732177734\n",
      "Batch 3000, Loss: 21.301021575927734\n",
      "Batch 3100, Loss: 19.8815860748291\n",
      "Average Training Loss: 21.22884111974682\n",
      "Average Validation Loss: 22.020746474570416\n",
      "Epoch 85/100\n",
      "----------\n",
      "Batch 0, Loss: 20.436450958251953\n",
      "Batch 100, Loss: 20.973590850830078\n",
      "Batch 200, Loss: 20.5548095703125\n",
      "Batch 300, Loss: 21.16914939880371\n",
      "Batch 400, Loss: 21.169628143310547\n",
      "Batch 500, Loss: 22.208669662475586\n",
      "Batch 600, Loss: 20.753007888793945\n",
      "Batch 700, Loss: 20.78274154663086\n",
      "Batch 800, Loss: 20.467693328857422\n",
      "Batch 900, Loss: 20.872438430786133\n",
      "Batch 1000, Loss: 22.923316955566406\n",
      "Batch 1100, Loss: 22.076889038085938\n",
      "Batch 1200, Loss: 21.988513946533203\n",
      "Batch 1300, Loss: 20.892221450805664\n",
      "Batch 1400, Loss: 22.21500015258789\n",
      "Batch 1500, Loss: 20.87580108642578\n",
      "Batch 1600, Loss: 21.847291946411133\n",
      "Batch 1700, Loss: 21.227027893066406\n",
      "Batch 1800, Loss: 20.839908599853516\n",
      "Batch 1900, Loss: 20.19005584716797\n",
      "Batch 2000, Loss: 20.968563079833984\n",
      "Batch 2100, Loss: 21.41921615600586\n",
      "Batch 2200, Loss: 20.823144912719727\n",
      "Batch 2300, Loss: 21.033519744873047\n",
      "Batch 2400, Loss: 20.795846939086914\n",
      "Batch 2500, Loss: 21.598403930664062\n",
      "Batch 2600, Loss: 20.730667114257812\n",
      "Batch 2700, Loss: 21.13795280456543\n",
      "Batch 2800, Loss: 21.463098526000977\n",
      "Batch 2900, Loss: 20.80484390258789\n",
      "Batch 3000, Loss: 21.979633331298828\n",
      "Batch 3100, Loss: 21.72794532775879\n",
      "Average Training Loss: 21.228670768762058\n",
      "Average Validation Loss: 22.022306320515085\n",
      "Epoch 86/100\n",
      "----------\n",
      "Batch 0, Loss: 21.35152244567871\n",
      "Batch 100, Loss: 20.085372924804688\n",
      "Batch 200, Loss: 22.55765724182129\n",
      "Batch 300, Loss: 21.039730072021484\n",
      "Batch 400, Loss: 20.8795223236084\n",
      "Batch 500, Loss: 21.37910270690918\n",
      "Batch 600, Loss: 19.959503173828125\n",
      "Batch 700, Loss: 22.303226470947266\n",
      "Batch 800, Loss: 21.564420700073242\n",
      "Batch 900, Loss: 20.96958351135254\n",
      "Batch 1000, Loss: 21.15768051147461\n",
      "Batch 1100, Loss: 21.190692901611328\n",
      "Batch 1200, Loss: 20.83367919921875\n",
      "Batch 1300, Loss: 20.812149047851562\n",
      "Batch 1400, Loss: 20.91939926147461\n",
      "Batch 1500, Loss: 21.338430404663086\n",
      "Batch 1600, Loss: 21.68486785888672\n",
      "Batch 1700, Loss: 20.06026840209961\n",
      "Batch 1800, Loss: 20.896312713623047\n",
      "Batch 1900, Loss: 21.207950592041016\n",
      "Batch 2000, Loss: 20.978294372558594\n",
      "Batch 2100, Loss: 21.85453987121582\n",
      "Batch 2200, Loss: 20.415239334106445\n",
      "Batch 2300, Loss: 21.226661682128906\n",
      "Batch 2400, Loss: 21.436138153076172\n",
      "Batch 2500, Loss: 20.197399139404297\n",
      "Batch 2600, Loss: 20.986251831054688\n",
      "Batch 2700, Loss: 21.922969818115234\n",
      "Batch 2800, Loss: 20.911701202392578\n",
      "Batch 2900, Loss: 20.840129852294922\n",
      "Batch 3000, Loss: 20.389989852905273\n",
      "Batch 3100, Loss: 21.11472511291504\n",
      "Average Training Loss: 21.226728375631435\n",
      "Average Validation Loss: 22.022640341900765\n",
      "Epoch 87/100\n",
      "----------\n",
      "Batch 0, Loss: 20.968400955200195\n",
      "Batch 100, Loss: 20.880306243896484\n",
      "Batch 200, Loss: 21.214054107666016\n",
      "Batch 300, Loss: 19.981952667236328\n",
      "Batch 400, Loss: 22.104202270507812\n",
      "Batch 500, Loss: 20.141353607177734\n",
      "Batch 600, Loss: 20.941383361816406\n",
      "Batch 700, Loss: 21.52565574645996\n",
      "Batch 800, Loss: 20.9851131439209\n",
      "Batch 900, Loss: 21.668663024902344\n",
      "Batch 1000, Loss: 21.900062561035156\n",
      "Batch 1100, Loss: 21.94159698486328\n",
      "Batch 1200, Loss: 20.50091552734375\n",
      "Batch 1300, Loss: 22.29524040222168\n",
      "Batch 1400, Loss: 20.971763610839844\n",
      "Batch 1500, Loss: 22.398202896118164\n",
      "Batch 1600, Loss: 21.32782745361328\n",
      "Batch 1700, Loss: 21.912355422973633\n",
      "Batch 1800, Loss: 21.370128631591797\n",
      "Batch 1900, Loss: 20.678916931152344\n",
      "Batch 2000, Loss: 22.71183967590332\n",
      "Batch 2100, Loss: 21.50328826904297\n",
      "Batch 2200, Loss: 21.72894859313965\n",
      "Batch 2300, Loss: 21.393877029418945\n",
      "Batch 2400, Loss: 21.112186431884766\n",
      "Batch 2500, Loss: 20.623159408569336\n",
      "Batch 2600, Loss: 21.056835174560547\n",
      "Batch 2700, Loss: 21.854883193969727\n",
      "Batch 2800, Loss: 21.959842681884766\n",
      "Batch 2900, Loss: 21.06806182861328\n",
      "Batch 3000, Loss: 21.90912437438965\n",
      "Batch 3100, Loss: 21.049476623535156\n",
      "Average Training Loss: 21.229502836256536\n",
      "Average Validation Loss: 22.020719645885713\n",
      "Epoch 88/100\n",
      "----------\n",
      "Batch 0, Loss: 20.212345123291016\n",
      "Batch 100, Loss: 20.864526748657227\n",
      "Batch 200, Loss: 22.088764190673828\n",
      "Batch 300, Loss: 21.084754943847656\n",
      "Batch 400, Loss: 20.99724769592285\n",
      "Batch 500, Loss: 21.796903610229492\n",
      "Batch 600, Loss: 20.53166389465332\n",
      "Batch 700, Loss: 20.585281372070312\n",
      "Batch 800, Loss: 21.760541915893555\n",
      "Batch 900, Loss: 21.463329315185547\n",
      "Batch 1000, Loss: 21.988845825195312\n",
      "Batch 1100, Loss: 21.69892120361328\n",
      "Batch 1200, Loss: 21.509294509887695\n",
      "Batch 1300, Loss: 20.509126663208008\n",
      "Batch 1400, Loss: 20.46703338623047\n",
      "Batch 1500, Loss: 21.5145320892334\n",
      "Batch 1600, Loss: 20.52404022216797\n",
      "Batch 1700, Loss: 20.23770523071289\n",
      "Batch 1800, Loss: 21.310699462890625\n",
      "Batch 1900, Loss: 21.376901626586914\n",
      "Batch 2000, Loss: 21.838722229003906\n",
      "Batch 2100, Loss: 20.039899826049805\n",
      "Batch 2200, Loss: 20.812889099121094\n",
      "Batch 2300, Loss: 20.536376953125\n",
      "Batch 2400, Loss: 21.743370056152344\n",
      "Batch 2500, Loss: 20.598241806030273\n",
      "Batch 2600, Loss: 19.94038963317871\n",
      "Batch 2700, Loss: 20.982913970947266\n",
      "Batch 2800, Loss: 22.30760955810547\n",
      "Batch 2900, Loss: 21.237884521484375\n",
      "Batch 3000, Loss: 21.533357620239258\n",
      "Batch 3100, Loss: 21.23531723022461\n",
      "Average Training Loss: 21.228865847332788\n",
      "Average Validation Loss: 22.023997927726583\n",
      "Epoch 89/100\n",
      "----------\n",
      "Batch 0, Loss: 20.747238159179688\n",
      "Batch 100, Loss: 21.31308364868164\n",
      "Batch 200, Loss: 20.90290069580078\n",
      "Batch 300, Loss: 21.287307739257812\n",
      "Batch 400, Loss: 21.096494674682617\n",
      "Batch 500, Loss: 21.756629943847656\n",
      "Batch 600, Loss: 21.841033935546875\n",
      "Batch 700, Loss: 21.477703094482422\n",
      "Batch 800, Loss: 21.83441734313965\n",
      "Batch 900, Loss: 21.534042358398438\n",
      "Batch 1000, Loss: 21.845687866210938\n",
      "Batch 1100, Loss: 21.00642204284668\n",
      "Batch 1200, Loss: 21.419662475585938\n",
      "Batch 1300, Loss: 22.28925132751465\n",
      "Batch 1400, Loss: 20.170957565307617\n",
      "Batch 1500, Loss: 21.169841766357422\n",
      "Batch 1600, Loss: 21.026718139648438\n",
      "Batch 1700, Loss: 21.66095542907715\n",
      "Batch 1800, Loss: 21.204566955566406\n",
      "Batch 1900, Loss: 21.844833374023438\n",
      "Batch 2000, Loss: 21.315265655517578\n",
      "Batch 2100, Loss: 21.35533905029297\n",
      "Batch 2200, Loss: 20.566242218017578\n",
      "Batch 2300, Loss: 21.458459854125977\n",
      "Batch 2400, Loss: 20.073129653930664\n",
      "Batch 2500, Loss: 20.70855140686035\n",
      "Batch 2600, Loss: 20.295032501220703\n",
      "Batch 2700, Loss: 21.145790100097656\n",
      "Batch 2800, Loss: 21.645811080932617\n",
      "Batch 2900, Loss: 22.009572982788086\n",
      "Batch 3000, Loss: 21.28759002685547\n",
      "Batch 3100, Loss: 20.956457138061523\n",
      "Average Training Loss: 21.227899963315814\n",
      "Average Validation Loss: 22.023645206207924\n",
      "Epoch 90/100\n",
      "----------\n",
      "Batch 0, Loss: 20.862884521484375\n",
      "Batch 100, Loss: 21.57685089111328\n",
      "Batch 200, Loss: 22.319091796875\n",
      "Batch 300, Loss: 22.275188446044922\n",
      "Batch 400, Loss: 21.00332260131836\n",
      "Batch 500, Loss: 20.91889190673828\n",
      "Batch 600, Loss: 20.9027099609375\n",
      "Batch 700, Loss: 22.172157287597656\n",
      "Batch 800, Loss: 20.823646545410156\n",
      "Batch 900, Loss: 21.51223373413086\n",
      "Batch 1000, Loss: 20.74224090576172\n",
      "Batch 1100, Loss: 22.729324340820312\n",
      "Batch 1200, Loss: 20.91570281982422\n",
      "Batch 1300, Loss: 20.988567352294922\n",
      "Batch 1400, Loss: 21.830413818359375\n",
      "Batch 1500, Loss: 20.777786254882812\n",
      "Batch 1600, Loss: 21.406173706054688\n",
      "Batch 1700, Loss: 21.223735809326172\n",
      "Batch 1800, Loss: 20.77446174621582\n",
      "Batch 1900, Loss: 20.021347045898438\n",
      "Batch 2000, Loss: 20.45587921142578\n",
      "Batch 2100, Loss: 22.079425811767578\n",
      "Batch 2200, Loss: 20.567127227783203\n",
      "Batch 2300, Loss: 21.09949493408203\n",
      "Batch 2400, Loss: 21.97083282470703\n",
      "Batch 2500, Loss: 20.05830955505371\n",
      "Batch 2600, Loss: 20.71175765991211\n",
      "Batch 2700, Loss: 21.89920997619629\n",
      "Batch 2800, Loss: 21.02474594116211\n",
      "Batch 2900, Loss: 21.369415283203125\n",
      "Batch 3000, Loss: 19.302104949951172\n",
      "Batch 3100, Loss: 21.072725296020508\n",
      "Average Training Loss: 21.22789916494724\n",
      "Average Validation Loss: 22.026383984342534\n",
      "Epoch 91/100\n",
      "----------\n",
      "Batch 0, Loss: 21.513248443603516\n",
      "Batch 100, Loss: 22.257787704467773\n",
      "Batch 200, Loss: 21.276296615600586\n",
      "Batch 300, Loss: 19.9874210357666\n",
      "Batch 400, Loss: 21.590452194213867\n",
      "Batch 500, Loss: 21.962678909301758\n",
      "Batch 600, Loss: 21.057601928710938\n",
      "Batch 700, Loss: 20.885147094726562\n",
      "Batch 800, Loss: 20.2833194732666\n",
      "Batch 900, Loss: 21.409744262695312\n",
      "Batch 1000, Loss: 20.244394302368164\n",
      "Batch 1100, Loss: 21.423662185668945\n",
      "Batch 1200, Loss: 21.395116806030273\n",
      "Batch 1300, Loss: 20.665565490722656\n",
      "Batch 1400, Loss: 21.09347915649414\n",
      "Batch 1500, Loss: 21.02301788330078\n",
      "Batch 1600, Loss: 20.430028915405273\n",
      "Batch 1700, Loss: 20.990060806274414\n",
      "Batch 1800, Loss: 21.654434204101562\n",
      "Batch 1900, Loss: 21.350738525390625\n",
      "Batch 2000, Loss: 20.60043716430664\n",
      "Batch 2100, Loss: 20.587331771850586\n",
      "Batch 2200, Loss: 21.69205093383789\n",
      "Batch 2300, Loss: 21.244556427001953\n",
      "Batch 2400, Loss: 20.954029083251953\n",
      "Batch 2500, Loss: 21.865238189697266\n",
      "Batch 2600, Loss: 21.33937644958496\n",
      "Batch 2700, Loss: 21.55866241455078\n",
      "Batch 2800, Loss: 21.20711898803711\n",
      "Batch 2900, Loss: 20.431880950927734\n",
      "Batch 3000, Loss: 20.073974609375\n",
      "Batch 3100, Loss: 22.294239044189453\n",
      "Average Training Loss: 21.227916213998963\n",
      "Average Validation Loss: 22.029142882976124\n",
      "Epoch 92/100\n",
      "----------\n",
      "Batch 0, Loss: 21.413909912109375\n",
      "Batch 100, Loss: 20.599889755249023\n",
      "Batch 200, Loss: 20.83124542236328\n",
      "Batch 300, Loss: 21.874820709228516\n",
      "Batch 400, Loss: 19.979660034179688\n",
      "Batch 500, Loss: 21.849943161010742\n",
      "Batch 600, Loss: 21.136640548706055\n",
      "Batch 700, Loss: 20.59920883178711\n",
      "Batch 800, Loss: 21.431873321533203\n",
      "Batch 900, Loss: 20.622303009033203\n",
      "Batch 1000, Loss: 22.093191146850586\n",
      "Batch 1100, Loss: 20.55367088317871\n",
      "Batch 1200, Loss: 21.780790328979492\n",
      "Batch 1300, Loss: 20.825164794921875\n",
      "Batch 1400, Loss: 20.79629135131836\n",
      "Batch 1500, Loss: 21.27324867248535\n",
      "Batch 1600, Loss: 21.40270233154297\n",
      "Batch 1700, Loss: 20.58420181274414\n",
      "Batch 1800, Loss: 22.063201904296875\n",
      "Batch 1900, Loss: 22.40117073059082\n",
      "Batch 2000, Loss: 21.918472290039062\n",
      "Batch 2100, Loss: 21.94306182861328\n",
      "Batch 2200, Loss: 21.500303268432617\n",
      "Batch 2300, Loss: 21.346168518066406\n",
      "Batch 2400, Loss: 21.072513580322266\n",
      "Batch 2500, Loss: 21.233171463012695\n",
      "Batch 2600, Loss: 21.533714294433594\n",
      "Batch 2700, Loss: 21.638803482055664\n",
      "Batch 2800, Loss: 20.68765640258789\n",
      "Batch 2900, Loss: 22.47702407836914\n",
      "Batch 3000, Loss: 21.911840438842773\n",
      "Batch 3100, Loss: 20.45800018310547\n",
      "Average Training Loss: 21.231052524867554\n",
      "Average Validation Loss: 22.025279629484135\n",
      "Epoch 93/100\n",
      "----------\n",
      "Batch 0, Loss: 20.670246124267578\n",
      "Batch 100, Loss: 20.718616485595703\n",
      "Batch 200, Loss: 19.760173797607422\n",
      "Batch 300, Loss: 22.127498626708984\n",
      "Batch 400, Loss: 21.990692138671875\n",
      "Batch 500, Loss: 21.723051071166992\n",
      "Batch 600, Loss: 21.741687774658203\n",
      "Batch 700, Loss: 20.428760528564453\n",
      "Batch 800, Loss: 21.06707000732422\n",
      "Batch 900, Loss: 22.395570755004883\n",
      "Batch 1000, Loss: 22.35956573486328\n",
      "Batch 1100, Loss: 21.45444107055664\n",
      "Batch 1200, Loss: 20.971492767333984\n",
      "Batch 1300, Loss: 20.49385643005371\n",
      "Batch 1400, Loss: 20.583126068115234\n",
      "Batch 1500, Loss: 21.142791748046875\n",
      "Batch 1600, Loss: 21.260021209716797\n",
      "Batch 1700, Loss: 20.858938217163086\n",
      "Batch 1800, Loss: 20.885841369628906\n",
      "Batch 1900, Loss: 20.220834732055664\n",
      "Batch 2000, Loss: 21.631439208984375\n",
      "Batch 2100, Loss: 21.570037841796875\n",
      "Batch 2200, Loss: 21.503582000732422\n",
      "Batch 2300, Loss: 21.471359252929688\n",
      "Batch 2400, Loss: 21.3555965423584\n",
      "Batch 2500, Loss: 20.50986671447754\n",
      "Batch 2600, Loss: 21.709026336669922\n",
      "Batch 2700, Loss: 21.580657958984375\n",
      "Batch 2800, Loss: 21.950149536132812\n",
      "Batch 2900, Loss: 21.259126663208008\n",
      "Batch 3000, Loss: 21.048067092895508\n",
      "Batch 3100, Loss: 21.390548706054688\n",
      "Average Training Loss: 21.229276470252273\n",
      "Average Validation Loss: 22.021349192680198\n",
      "Epoch 94/100\n",
      "----------\n",
      "Batch 0, Loss: 21.03889274597168\n",
      "Batch 100, Loss: 21.615262985229492\n",
      "Batch 200, Loss: 20.585391998291016\n",
      "Batch 300, Loss: 22.081933975219727\n",
      "Batch 400, Loss: 20.617216110229492\n",
      "Batch 500, Loss: 21.089574813842773\n",
      "Batch 600, Loss: 21.020492553710938\n",
      "Batch 700, Loss: 22.712554931640625\n",
      "Batch 800, Loss: 20.259540557861328\n",
      "Batch 900, Loss: 21.507373809814453\n",
      "Batch 1000, Loss: 20.468257904052734\n",
      "Batch 1100, Loss: 21.41676139831543\n",
      "Batch 1200, Loss: 21.21259307861328\n",
      "Batch 1300, Loss: 22.42776107788086\n",
      "Batch 1400, Loss: 20.843982696533203\n",
      "Batch 1500, Loss: 21.117795944213867\n",
      "Batch 1600, Loss: 20.633914947509766\n",
      "Batch 1700, Loss: 20.573928833007812\n",
      "Batch 1800, Loss: 23.477163314819336\n",
      "Batch 1900, Loss: 20.771526336669922\n",
      "Batch 2000, Loss: 20.759811401367188\n",
      "Batch 2100, Loss: 21.45948028564453\n",
      "Batch 2200, Loss: 23.57014274597168\n",
      "Batch 2300, Loss: 21.444713592529297\n",
      "Batch 2400, Loss: 19.4238338470459\n",
      "Batch 2500, Loss: 21.88314437866211\n",
      "Batch 2600, Loss: 21.361919403076172\n",
      "Batch 2700, Loss: 20.967098236083984\n",
      "Batch 2800, Loss: 20.952377319335938\n",
      "Batch 2900, Loss: 21.48066520690918\n",
      "Batch 3000, Loss: 20.492128372192383\n",
      "Batch 3100, Loss: 21.799875259399414\n",
      "Average Training Loss: 21.22762041419517\n",
      "Average Validation Loss: 22.0244124635737\n",
      "Epoch 95/100\n",
      "----------\n",
      "Batch 0, Loss: 20.47549057006836\n",
      "Batch 100, Loss: 20.947067260742188\n",
      "Batch 200, Loss: 21.347434997558594\n",
      "Batch 300, Loss: 21.71846580505371\n",
      "Batch 400, Loss: 20.77710723876953\n",
      "Batch 500, Loss: 21.02279281616211\n",
      "Batch 600, Loss: 20.80817222595215\n",
      "Batch 700, Loss: 21.238805770874023\n",
      "Batch 800, Loss: 19.839767456054688\n",
      "Batch 900, Loss: 21.010562896728516\n",
      "Batch 1000, Loss: 20.027503967285156\n",
      "Batch 1100, Loss: 21.60952377319336\n",
      "Batch 1200, Loss: 21.195165634155273\n",
      "Batch 1300, Loss: 21.383346557617188\n",
      "Batch 1400, Loss: 20.826330184936523\n",
      "Batch 1500, Loss: 20.97075653076172\n",
      "Batch 1600, Loss: 20.32999038696289\n",
      "Batch 1700, Loss: 20.726116180419922\n",
      "Batch 1800, Loss: 21.682659149169922\n",
      "Batch 1900, Loss: 22.06529998779297\n",
      "Batch 2000, Loss: 19.74443817138672\n",
      "Batch 2100, Loss: 21.452722549438477\n",
      "Batch 2200, Loss: 22.80776596069336\n",
      "Batch 2300, Loss: 21.7318172454834\n",
      "Batch 2400, Loss: 21.135425567626953\n",
      "Batch 2500, Loss: 21.8192081451416\n",
      "Batch 2600, Loss: 20.440292358398438\n",
      "Batch 2700, Loss: 21.037677764892578\n",
      "Batch 2800, Loss: 20.488357543945312\n",
      "Batch 2900, Loss: 22.263111114501953\n",
      "Batch 3000, Loss: 22.264144897460938\n",
      "Batch 3100, Loss: 20.12368392944336\n",
      "Average Training Loss: 21.229299624760944\n",
      "Average Validation Loss: 22.022227758042355\n",
      "Epoch 96/100\n",
      "----------\n",
      "Batch 0, Loss: 20.863487243652344\n",
      "Batch 100, Loss: 21.923736572265625\n",
      "Batch 200, Loss: 20.4652099609375\n",
      "Batch 300, Loss: 21.065223693847656\n",
      "Batch 400, Loss: 20.43022918701172\n",
      "Batch 500, Loss: 22.106103897094727\n",
      "Batch 600, Loss: 21.793176651000977\n",
      "Batch 700, Loss: 21.335674285888672\n",
      "Batch 800, Loss: 21.826683044433594\n",
      "Batch 900, Loss: 21.082666397094727\n",
      "Batch 1000, Loss: 20.748546600341797\n",
      "Batch 1100, Loss: 20.489734649658203\n",
      "Batch 1200, Loss: 21.685588836669922\n",
      "Batch 1300, Loss: 21.542316436767578\n",
      "Batch 1400, Loss: 21.806819915771484\n",
      "Batch 1500, Loss: 20.910114288330078\n",
      "Batch 1600, Loss: 21.225772857666016\n",
      "Batch 1700, Loss: 20.767902374267578\n",
      "Batch 1800, Loss: 21.150373458862305\n",
      "Batch 1900, Loss: 21.480045318603516\n",
      "Batch 2000, Loss: 20.443222045898438\n",
      "Batch 2100, Loss: 21.693347930908203\n",
      "Batch 2200, Loss: 21.032196044921875\n",
      "Batch 2300, Loss: 20.91286849975586\n",
      "Batch 2400, Loss: 20.095657348632812\n",
      "Batch 2500, Loss: 21.52426528930664\n",
      "Batch 2600, Loss: 20.865293502807617\n",
      "Batch 2700, Loss: 21.12027359008789\n",
      "Batch 2800, Loss: 22.92640495300293\n",
      "Batch 2900, Loss: 20.539562225341797\n",
      "Batch 3000, Loss: 20.898754119873047\n",
      "Batch 3100, Loss: 20.949953079223633\n",
      "Average Training Loss: 21.228941911959467\n",
      "Average Validation Loss: 22.02640058639202\n",
      "Epoch 97/100\n",
      "----------\n",
      "Batch 0, Loss: 20.550992965698242\n",
      "Batch 100, Loss: 20.25331687927246\n",
      "Batch 200, Loss: 20.280471801757812\n",
      "Batch 300, Loss: 21.07095718383789\n",
      "Batch 400, Loss: 20.903690338134766\n",
      "Batch 500, Loss: 21.836196899414062\n",
      "Batch 600, Loss: 20.910064697265625\n",
      "Batch 700, Loss: 21.358415603637695\n",
      "Batch 800, Loss: 21.715883255004883\n",
      "Batch 900, Loss: 22.325502395629883\n",
      "Batch 1000, Loss: 22.531728744506836\n",
      "Batch 1100, Loss: 21.79633140563965\n",
      "Batch 1200, Loss: 21.818017959594727\n",
      "Batch 1300, Loss: 20.856483459472656\n",
      "Batch 1400, Loss: 22.332672119140625\n",
      "Batch 1500, Loss: 20.381988525390625\n",
      "Batch 1600, Loss: 23.032344818115234\n",
      "Batch 1700, Loss: 22.070966720581055\n",
      "Batch 1800, Loss: 21.33574676513672\n",
      "Batch 1900, Loss: 20.314590454101562\n",
      "Batch 2000, Loss: 22.579471588134766\n",
      "Batch 2100, Loss: 21.396381378173828\n",
      "Batch 2200, Loss: 20.54302215576172\n",
      "Batch 2300, Loss: 21.680519104003906\n",
      "Batch 2400, Loss: 21.153772354125977\n",
      "Batch 2500, Loss: 21.667783737182617\n",
      "Batch 2600, Loss: 21.141647338867188\n",
      "Batch 2700, Loss: 20.97180938720703\n",
      "Batch 2800, Loss: 20.859874725341797\n",
      "Batch 2900, Loss: 21.711341857910156\n",
      "Batch 3000, Loss: 21.29261016845703\n",
      "Batch 3100, Loss: 20.78173828125\n",
      "Average Training Loss: 21.22815598543667\n",
      "Average Validation Loss: 22.026498051907154\n",
      "Epoch 98/100\n",
      "----------\n",
      "Batch 0, Loss: 22.021135330200195\n",
      "Batch 100, Loss: 21.834575653076172\n",
      "Batch 200, Loss: 21.36493682861328\n",
      "Batch 300, Loss: 19.98749542236328\n",
      "Batch 400, Loss: 21.41111183166504\n",
      "Batch 500, Loss: 21.099937438964844\n",
      "Batch 600, Loss: 21.352222442626953\n",
      "Batch 700, Loss: 20.64459228515625\n",
      "Batch 800, Loss: 21.913225173950195\n",
      "Batch 900, Loss: 20.892597198486328\n",
      "Batch 1000, Loss: 21.080242156982422\n",
      "Batch 1100, Loss: 21.170021057128906\n",
      "Batch 1200, Loss: 20.694429397583008\n",
      "Batch 1300, Loss: 21.733654022216797\n",
      "Batch 1400, Loss: 21.356658935546875\n",
      "Batch 1500, Loss: 21.243236541748047\n",
      "Batch 1600, Loss: 20.97076416015625\n",
      "Batch 1700, Loss: 21.589303970336914\n",
      "Batch 1800, Loss: 20.32697868347168\n",
      "Batch 1900, Loss: 21.215560913085938\n",
      "Batch 2000, Loss: 22.010488510131836\n",
      "Batch 2100, Loss: 21.637924194335938\n",
      "Batch 2200, Loss: 21.24910545349121\n",
      "Batch 2300, Loss: 20.349870681762695\n",
      "Batch 2400, Loss: 21.578346252441406\n",
      "Batch 2500, Loss: 21.519668579101562\n",
      "Batch 2600, Loss: 21.59121322631836\n",
      "Batch 2700, Loss: 19.850242614746094\n",
      "Batch 2800, Loss: 20.783950805664062\n",
      "Batch 2900, Loss: 22.242618560791016\n",
      "Batch 3000, Loss: 21.527477264404297\n",
      "Batch 3100, Loss: 22.401981353759766\n",
      "Average Training Loss: 21.229326663733136\n",
      "Average Validation Loss: 22.024517225711904\n",
      "Epoch 99/100\n",
      "----------\n",
      "Batch 0, Loss: 20.387401580810547\n",
      "Batch 100, Loss: 21.422687530517578\n",
      "Batch 200, Loss: 19.88155746459961\n",
      "Batch 300, Loss: 21.992597579956055\n",
      "Batch 400, Loss: 21.033554077148438\n",
      "Batch 500, Loss: 20.936382293701172\n",
      "Batch 600, Loss: 20.26163101196289\n",
      "Batch 700, Loss: 21.304157257080078\n",
      "Batch 800, Loss: 20.43846893310547\n",
      "Batch 900, Loss: 21.410097122192383\n",
      "Batch 1000, Loss: 21.288928985595703\n",
      "Batch 1100, Loss: 20.535436630249023\n",
      "Batch 1200, Loss: 22.195215225219727\n",
      "Batch 1300, Loss: 21.553565979003906\n",
      "Batch 1400, Loss: 21.07223892211914\n",
      "Batch 1500, Loss: 20.28886604309082\n",
      "Batch 1600, Loss: 21.410083770751953\n",
      "Batch 1700, Loss: 20.892898559570312\n",
      "Batch 1800, Loss: 20.106800079345703\n",
      "Batch 1900, Loss: 21.631996154785156\n",
      "Batch 2000, Loss: 21.35544776916504\n",
      "Batch 2100, Loss: 21.300416946411133\n",
      "Batch 2200, Loss: 21.371440887451172\n",
      "Batch 2300, Loss: 21.555370330810547\n",
      "Batch 2400, Loss: 19.88949203491211\n",
      "Batch 2500, Loss: 21.276525497436523\n",
      "Batch 2600, Loss: 22.10104751586914\n",
      "Batch 2700, Loss: 20.158323287963867\n",
      "Batch 2800, Loss: 22.425945281982422\n",
      "Batch 2900, Loss: 20.636245727539062\n",
      "Batch 3000, Loss: 20.65653419494629\n",
      "Batch 3100, Loss: 20.176040649414062\n",
      "Average Training Loss: 21.2296241124471\n",
      "Average Validation Loss: 22.026202546789293\n",
      "Epoch 100/100\n",
      "----------\n",
      "Batch 0, Loss: 20.841426849365234\n",
      "Batch 100, Loss: 21.568450927734375\n",
      "Batch 200, Loss: 21.08472442626953\n",
      "Batch 300, Loss: 21.633243560791016\n",
      "Batch 400, Loss: 21.20263671875\n",
      "Batch 500, Loss: 21.9700927734375\n",
      "Batch 600, Loss: 20.16889190673828\n",
      "Batch 700, Loss: 22.410043716430664\n",
      "Batch 800, Loss: 21.630443572998047\n",
      "Batch 900, Loss: 21.005510330200195\n",
      "Batch 1000, Loss: 20.504695892333984\n",
      "Batch 1100, Loss: 21.71337890625\n",
      "Batch 1200, Loss: 21.018526077270508\n",
      "Batch 1300, Loss: 20.61476707458496\n",
      "Batch 1400, Loss: 21.119476318359375\n",
      "Batch 1500, Loss: 21.271684646606445\n",
      "Batch 1600, Loss: 20.180744171142578\n",
      "Batch 1700, Loss: 21.602340698242188\n",
      "Batch 1800, Loss: 22.153059005737305\n",
      "Batch 1900, Loss: 21.227703094482422\n",
      "Batch 2000, Loss: 20.948707580566406\n",
      "Batch 2100, Loss: 21.343231201171875\n",
      "Batch 2200, Loss: 20.43012046813965\n",
      "Batch 2300, Loss: 21.491296768188477\n",
      "Batch 2400, Loss: 21.497896194458008\n",
      "Batch 2500, Loss: 21.2811279296875\n",
      "Batch 2600, Loss: 21.267202377319336\n",
      "Batch 2700, Loss: 20.55769920349121\n",
      "Batch 2800, Loss: 22.236114501953125\n",
      "Batch 2900, Loss: 21.44039535522461\n",
      "Batch 3000, Loss: 20.208797454833984\n",
      "Batch 3100, Loss: 21.682214736938477\n",
      "Average Training Loss: 21.22865191731441\n",
      "Average Validation Loss: 22.021825782288897\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (X, y, _, _) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {loss.item()}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Training Loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, _, _ in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Validation Loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(\"-\" * 10)\n",
    "    train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, loss_fn, device)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4700d82b",
   "metadata": {
    "id": "cT7dvhsmHuPp",
    "papermill": {
     "duration": 0.127856,
     "end_time": "2024-03-24T05:58:45.585330",
     "exception": false,
     "start_time": "2024-03-24T05:58:45.457474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mobilenet_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdb92d-470f-4523-8158-4147e80985b5",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988d661f",
   "metadata": {
    "papermill": {
     "duration": 0.077554,
     "end_time": "2024-03-24T05:58:45.740199",
     "exception": false,
     "start_time": "2024-03-24T05:58:45.662645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X, _, _, _ in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            pred = (pred > threshold).float()  # Convert probabilities to binary predictions\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "    return np.vstack(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da948c99-8b99-44a2-84ae-f818c3542f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0591ff1e-bece-4fa1-ac56-c6bc3d62e87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3214547c-8d81-404c-8f07-7665eeae1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'attr_name': ['Age-Young',\n",
    "#   'Age-Adult',\n",
    "#   'Age-Old',\n",
    "#   'Gender-Female',\n",
    "#   'Hair-Length-Short',\n",
    "#   'Hair-Length-Long',\n",
    "#   'Hair-Length-Bald',\n",
    "#   'UpperBody-Length-Short',\n",
    "#   'UpperBody-Color-Black',\n",
    "#   'UpperBody-Color-Blue',\n",
    "#   'UpperBody-Color-Brown',\n",
    "#   'UpperBody-Color-Green',\n",
    "#   'UpperBody-Color-Grey',\n",
    "#   'UpperBody-Color-Orange',\n",
    "#   'UpperBody-Color-Pink',\n",
    "#   'UpperBody-Color-Purple',\n",
    "#   'UpperBody-Color-Red',\n",
    "#   'UpperBody-Color-White',\n",
    "#   'UpperBody-Color-Yellow',\n",
    "#   'UpperBody-Color-Other',\n",
    "#   'LowerBody-Length-Short',\n",
    "#   'LowerBody-Color-Black',\n",
    "#   'LowerBody-Color-Blue',\n",
    "#   'LowerBody-Color-Brown',\n",
    "#   'LowerBody-Color-Green',\n",
    "#   'LowerBody-Color-Grey',\n",
    "#   'LowerBody-Color-Orange',\n",
    "#   'LowerBody-Color-Pink',\n",
    "#   'LowerBody-Color-Purple',\n",
    "#   'LowerBody-Color-Red',\n",
    "#   'LowerBody-Color-White',\n",
    "#   'LowerBody-Color-Yellow',\n",
    "#   'LowerBody-Color-Other',\n",
    "#   'LowerBody-Type-Trousers&Shorts',\n",
    "#   'LowerBody-Type-Skirt&Dress',\n",
    "#   'Accessory-Backpack',\n",
    "#   'Accessory-Bag',\n",
    "#   'Accessory-Glasses-Normal',\n",
    "#   'Accessory-Glasses-Sun',\n",
    "#   'Accessory-Hat'],\n",
    "\n",
    "#  ...],\n",
    "#  'label': array([[0, 1, 0, ..., 1, 0, 0],\n",
    "#         [0, 1, 0, ..., 0, 0, 0],\n",
    "#         [0, 1, 0, ..., 1, 0, 0],\n",
    "#         ...,\n",
    "#         [0, 1, 0, ..., 0, 0, 0],\n",
    "#         [0, 1, 0, ..., 0, 0, 0],\n",
    "#         [0, 1, 0, ..., 0, 0, 0]]),"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2636.405041,
   "end_time": "2024-03-24T05:58:48.512518",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-24T05:14:52.107477",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
