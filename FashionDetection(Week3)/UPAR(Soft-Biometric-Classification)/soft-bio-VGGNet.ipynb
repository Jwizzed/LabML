{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d79377",
   "metadata": {
    "id": "PfgBeuAevIXc",
    "outputId": "940c3dce-bd09-4aea-eef9-8c79207167be",
    "papermill": {
     "duration": 2.792201,
     "end_time": "2024-03-24T04:10:59.271552",
     "exception": false,
     "start_time": "2024-03-24T04:10:56.479351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'upar_dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/speckean/upar_dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c5dd8",
   "metadata": {
    "id": "N-Qkzag4vzzN",
    "papermill": {
     "duration": 0.006223,
     "end_time": "2024-03-24T04:10:59.288053",
     "exception": false,
     "start_time": "2024-03-24T04:10:59.281830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08800ac",
   "metadata": {
    "id": "qPx3W0h-vT97",
    "papermill": {
     "duration": 7.596493,
     "end_time": "2024-03-24T04:11:06.890586",
     "exception": false,
     "start_time": "2024-03-24T04:10:59.294093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0acbb5",
   "metadata": {
    "id": "g3cYG_hs_bhO",
    "outputId": "f79cb5d2-c440-46b4-8db0-44f1532b73ba",
    "papermill": {
     "duration": 0.015262,
     "end_time": "2024-03-24T04:11:06.912010",
     "exception": false,
     "start_time": "2024-03-24T04:11:06.896748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Downloads\\\\LabML\\\\FashionDetection(Week3)\\\\UPAR(Soft-Biometric-Classification)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a752bb2",
   "metadata": {
    "id": "lUdsEedzid6v",
    "papermill": {
     "duration": 0.005696,
     "end_time": "2024-03-24T04:11:06.923807",
     "exception": false,
     "start_time": "2024-03-24T04:11:06.918111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8604f",
   "metadata": {
    "id": "96_bEv_IiiA6",
    "papermill": {
     "duration": 0.005974,
     "end_time": "2024-03-24T04:11:06.935800",
     "exception": false,
     "start_time": "2024-03-24T04:11:06.929826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get a data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f0a548",
   "metadata": {
    "id": "oRRDnWjIi-zS",
    "outputId": "89e7bd3d-ce24-4a17-c47d-3db23c5491fc",
    "papermill": {
     "duration": 15.687015,
     "end_time": "2024-03-24T04:11:22.628902",
     "exception": false,
     "start_time": "2024-03-24T04:11:06.941887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Downloads\\LabML\\FashionDetection(Week3)\\UPAR(Soft-Biometric-Classification)\\upar_challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'upar_challenge' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: gdown in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\anaconda3\\envs\\yolo\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/speckean/upar_challenge.git\n",
    "%cd upar_challenge\n",
    "!pip install tqdm gdown requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ecdaf8",
   "metadata": {
    "id": "6vrUotcziocm",
    "outputId": "2d7bdb8c-f538-44be-e008-4aadf50c74ed",
    "papermill": {
     "duration": 55.393361,
     "end_time": "2024-03-24T04:12:18.029882",
     "exception": false,
     "start_time": "2024-03-24T04:11:22.636521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Downloads\\LabML\\FashionDetection(Week3)\\UPAR(Soft-Biometric-Classification)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"download_datasets.py\", line 9, in <module>\n",
      "    import gdown\n",
      "ModuleNotFoundError: No module named 'gdown'\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!python3 download_datasets.py\n",
    "%cd {HOME}\n",
    "!mv upar_challenge/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33bb04b",
   "metadata": {
    "id": "PYxNcR4XyKL0",
    "outputId": "457e5b22-fb5d-466e-faf5-334207115f4b",
    "papermill": {
     "duration": 0.094401,
     "end_time": "2024-03-24T04:12:19.654755",
     "exception": false,
     "start_time": "2024-03-24T04:12:19.560354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'data/PA100k/release_data/release_data' does not exist.\n"
     ]
    }
   ],
   "source": [
    "current_dir_path = 'data/PA100k/release_data/release_data'\n",
    "new_dir_path = 'data/PA100k/data'\n",
    "\n",
    "\n",
    "if os.path.exists(current_dir_path):\n",
    "    if not os.path.exists(new_dir_path):\n",
    "        os.rename(current_dir_path, new_dir_path)\n",
    "        print(f\"Directory renamed from '{current_dir_path}' to '{new_dir_path}'\")\n",
    "\n",
    "    else:\n",
    "        print(f\"The target directory '{new_dir_path}' already exists.\")\n",
    "else:\n",
    "    print(f\"The directory '{current_dir_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc646317",
   "metadata": {
    "id": "Ipq3tXRkijzw",
    "papermill": {
     "duration": 0.091721,
     "end_time": "2024-03-24T04:12:19.836718",
     "exception": false,
     "start_time": "2024-03-24T04:12:19.744997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Construct dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f161d7",
   "metadata": {
    "id": "7Y15r_o5v07f",
    "papermill": {
     "duration": 0.108607,
     "end_time": "2024-03-24T04:12:20.017509",
     "exception": false,
     "start_time": "2024-03-24T04:12:19.908902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class UPAR(data.Dataset):\n",
    "  \"\"\"\n",
    "  Load UPAR dataset from pickle file\n",
    "\n",
    "  split: whether to use train/val/trainval/test split\n",
    "  partition: partition id 0-9\n",
    "  root: path to datasets, original datasets must be in this directory\n",
    "  data_path: path to UPAR pickle file\n",
    "  transform: training data transforms\n",
    "  target_transforms: evaluation data transforms\n",
    "  \"\"\"\n",
    "  def __init__(self, split='train', partition=0, root=HOME, data_path='upar_dataset/UPAR/dataset_all.pkl', transform=None, target_transform=None):\n",
    "    dataset_info = pickle.load(open(data_path, 'rb+'))\n",
    "    self.dataset_info = dataset_info\n",
    "\n",
    "    img_id = dataset_info.image_name\n",
    "    attr_label = dataset_info.label\n",
    "\n",
    "    assert split in dataset_info.partition.keys(), f'split {split} does not exist'\n",
    "\n",
    "    self.dataset = 'UPAR'\n",
    "    self.transform = transform  # data transforms during training\n",
    "    self.target_transform = target_transform  # data transforms during testing\n",
    "\n",
    "    self.root_path = root+\"/data\"  # path to datasets\n",
    "\n",
    "    self.attr_id = dataset_info.attr_name  # attribute names\n",
    "    self.attr_num = len(self.attr_id)  # number of attributes\n",
    "\n",
    "\t\t# load partition\n",
    "    self.img_idx = dataset_info.partition[split]\n",
    "\n",
    "    if isinstance(self.img_idx, list):\n",
    "      self.img_idx = self.img_idx[partition]\n",
    "\n",
    "    if isinstance(self.img_idx, list):\n",
    "      self.img_idx = np.hstack(self.img_idx)\n",
    "\n",
    "    self.img_idx = np.array([i for i in self.img_idx if not any(folder in img_id[i] for folder in [\"RAP\"])])\n",
    "\n",
    "\n",
    "    self.img_num = self.img_idx.shape[0]\n",
    "    self.img_id = [img_id[i] for i in self.img_idx]\n",
    "    self.label = attr_label[self.img_idx]\n",
    "\n",
    "    # set sub-dataset lengths to enable evaluation on sub-datasets\n",
    "    self.sub_dataset_lengths = [len(d) for d in dataset_info.partition.test[partition]]\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      \"\"\"\n",
    "      get dataset item by index\n",
    "\n",
    "      index: item index\n",
    "      return: image data (img) with corresponding ground truth labels (gt_label), dataset id (did), and image path (imgname)\n",
    "      \"\"\"\n",
    "      imgname, gt_label, imgidx = self.img_id[index], self.label[index], self.img_idx[index]\n",
    "      did = self.dataset_info.dataset_ids[imgidx]\n",
    "      imgpath = os.path.join(self.root_path, imgname)\n",
    "      img = Image.open(imgpath)\n",
    "\n",
    "      if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "\n",
    "      gt_label = gt_label.astype(np.float32)\n",
    "\n",
    "      if self.target_transform is not None:\n",
    "          gt_label = self.transform(gt_label)\n",
    "\n",
    "      return img, gt_label, did, imgname\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    get length of dataset\n",
    "    \"\"\"\n",
    "    return len(self.img_id)\n",
    "\n",
    "\n",
    "def get_transform(height, width):\n",
    "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    train_transform = [\n",
    "        T.Resize((height, width))\n",
    "    ]\n",
    "\n",
    "    train_transform += [\n",
    "        T.Pad(10),\n",
    "        T.RandomCrop((height, width)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "    ]\n",
    "\n",
    "    train_transform += [\n",
    "        T.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    "    train_transform = T.Compose(train_transform)\n",
    "\n",
    "    valid_transform = T.Compose([\n",
    "        T.Resize((height, width)),\n",
    "        T.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    return train_transform, valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab94abd",
   "metadata": {
    "id": "C__-utMewC83",
    "papermill": {
     "duration": 0.982437,
     "end_time": "2024-03-24T04:12:21.071441",
     "exception": false,
     "start_time": "2024-03-24T04:12:20.089004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height = 224  # Example height\n",
    "width = 224   # Example width\n",
    "train_transform, valid_transform = get_transform(height, width)\n",
    "\n",
    "\n",
    "# For training\n",
    "train_dataset = UPAR(split='train', partition=0, transform=train_transform)\n",
    "\n",
    "# For validation\n",
    "val_dataset = UPAR(split='val', partition=0, transform=valid_transform)\n",
    "\n",
    "# For testing\n",
    "test_dataset = UPAR(split='test', partition=0, transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94907ea8",
   "metadata": {
    "id": "bAwTtSBfwpBn",
    "papermill": {
     "duration": 0.0777,
     "end_time": "2024-03-24T04:12:21.219862",
     "exception": false,
     "start_time": "2024-03-24T04:12:21.142162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32  # Example batch size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75109b14",
   "metadata": {
    "id": "zYqxrG3p_sc0",
    "papermill": {
     "duration": 0.067721,
     "end_time": "2024-03-24T04:12:21.358012",
     "exception": false,
     "start_time": "2024-03-24T04:12:21.290291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a202597",
   "metadata": {
    "id": "We87CAdU-cww",
    "papermill": {
     "duration": 0.087126,
     "end_time": "2024-03-24T04:12:21.532475",
     "exception": false,
     "start_time": "2024-03-24T04:12:21.445349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UPARVGGModel(nn.Module):\n",
    "    def __init__(self, num_attributes):\n",
    "        super(UPARVGGModel, self).__init__()\n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = self.vgg16.features\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # VGG-16 features dimension before classifier is 512*7*7 assuming input size is 224x224\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_attributes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45880ac8",
   "metadata": {
    "id": "NrK7JzSxDzZD",
    "papermill": {
     "duration": 1.141955,
     "end_time": "2024-03-24T04:12:22.745062",
     "exception": false,
     "start_time": "2024-03-24T04:12:21.603107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_attributes = 40\n",
    "model = UPARVGGModel(num_attributes=num_attributes)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7151ef97",
   "metadata": {
    "id": "guhqNIXxHkHS",
    "outputId": "b6849ff6-53fc-4bac-97b2-8d80d4211774",
    "papermill": {
     "duration": 3536.750095,
     "end_time": "2024-03-24T05:11:19.564840",
     "exception": false,
     "start_time": "2024-03-24T04:12:22.814745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Batch 0, Loss: 0.8904762268066406\n",
      "Batch 100, Loss: 0.6772779226303101\n",
      "Batch 200, Loss: 0.6788403987884521\n",
      "Batch 300, Loss: 0.676496684551239\n",
      "Batch 400, Loss: 0.6796216368675232\n",
      "Batch 500, Loss: 0.6819654107093811\n",
      "Batch 600, Loss: 0.6772779226303101\n",
      "Batch 700, Loss: 0.6780591607093811\n",
      "Batch 800, Loss: 0.6780591607093811\n",
      "Batch 900, Loss: 0.6819654107093811\n",
      "Batch 1000, Loss: 0.6796216368675232\n",
      "Batch 1100, Loss: 0.676496684551239\n",
      "Batch 1200, Loss: 0.6772779226303101\n",
      "Batch 1300, Loss: 0.6796216368675232\n",
      "Batch 1400, Loss: 0.6772779226303101\n",
      "Batch 1500, Loss: 0.6772779226303101\n",
      "Batch 1600, Loss: 0.6796216368675232\n",
      "Batch 1700, Loss: 0.6811841726303101\n",
      "Batch 1800, Loss: 0.680402934551239\n",
      "Batch 1900, Loss: 0.6741529107093811\n",
      "Batch 2000, Loss: 0.6827466487884521\n",
      "Batch 2100, Loss: 0.6780591607093811\n",
      "Batch 2200, Loss: 0.676496684551239\n",
      "Batch 2300, Loss: 0.6757153868675232\n",
      "Batch 2400, Loss: 0.6757153868675232\n",
      "Batch 2500, Loss: 0.6772779226303101\n",
      "Batch 2600, Loss: 0.6811841726303101\n",
      "Batch 2700, Loss: 0.676496684551239\n",
      "Batch 2800, Loss: 0.6780591607093811\n",
      "Batch 2900, Loss: 0.6788403987884521\n",
      "Batch 3000, Loss: 0.680402934551239\n",
      "Batch 3100, Loss: 0.6780591607093811\n",
      "Average Training Loss: 0.6787266338258299\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 2/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6749341487884521\n",
      "Batch 100, Loss: 0.6788403987884521\n",
      "Batch 200, Loss: 0.680402934551239\n",
      "Batch 300, Loss: 0.6772779226303101\n",
      "Batch 400, Loss: 0.680402934551239\n",
      "Batch 500, Loss: 0.680402934551239\n",
      "Batch 600, Loss: 0.6788403987884521\n",
      "Batch 700, Loss: 0.6811841726303101\n",
      "Batch 800, Loss: 0.6780591607093811\n",
      "Batch 900, Loss: 0.6796216368675232\n",
      "Batch 1000, Loss: 0.6827466487884521\n",
      "Batch 1100, Loss: 0.6796216368675232\n",
      "Batch 1200, Loss: 0.6788403987884521\n",
      "Batch 1300, Loss: 0.6780591607093811\n",
      "Batch 1400, Loss: 0.6772779226303101\n",
      "Batch 1500, Loss: 0.6780591607093811\n",
      "Batch 1600, Loss: 0.680402934551239\n",
      "Batch 1700, Loss: 0.6780591607093811\n",
      "Batch 1800, Loss: 0.6757153868675232\n",
      "Batch 1900, Loss: 0.6796216368675232\n",
      "Batch 2000, Loss: 0.6788403987884521\n",
      "Batch 2100, Loss: 0.6780591607093811\n",
      "Batch 2200, Loss: 0.6811841726303101\n",
      "Batch 2300, Loss: 0.6780591607093811\n",
      "Batch 2400, Loss: 0.6749341487884521\n",
      "Batch 2500, Loss: 0.6827466487884521\n",
      "Batch 2600, Loss: 0.6788403987884521\n",
      "Batch 2700, Loss: 0.6772779226303101\n",
      "Batch 2800, Loss: 0.6780591607093811\n",
      "Batch 2900, Loss: 0.6780591607093811\n",
      "Batch 3000, Loss: 0.676496684551239\n",
      "Batch 3100, Loss: 0.6772779226303101\n",
      "Average Training Loss: 0.6786587070531518\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 3/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6780591607093811\n",
      "Batch 100, Loss: 0.6811841726303101\n",
      "Batch 200, Loss: 0.6796216368675232\n",
      "Batch 300, Loss: 0.6780591607093811\n",
      "Batch 400, Loss: 0.6780591607093811\n",
      "Batch 500, Loss: 0.6796216368675232\n",
      "Batch 600, Loss: 0.6780591607093811\n",
      "Batch 700, Loss: 0.6772779226303101\n",
      "Batch 800, Loss: 0.6788403987884521\n",
      "Batch 900, Loss: 0.680402934551239\n",
      "Batch 1000, Loss: 0.676496684551239\n",
      "Batch 1100, Loss: 0.6772779226303101\n",
      "Batch 1200, Loss: 0.6780591607093811\n",
      "Batch 1300, Loss: 0.676496684551239\n",
      "Batch 1400, Loss: 0.6780591607093811\n",
      "Batch 1500, Loss: 0.6757153868675232\n",
      "Batch 1600, Loss: 0.6788403987884521\n",
      "Batch 1700, Loss: 0.6757153868675232\n",
      "Batch 1800, Loss: 0.6772779226303101\n",
      "Batch 1900, Loss: 0.6772779226303101\n",
      "Batch 2000, Loss: 0.6788403987884521\n",
      "Batch 2100, Loss: 0.6788403987884521\n",
      "Batch 2200, Loss: 0.6796216368675232\n",
      "Batch 2300, Loss: 0.6780591607093811\n",
      "Batch 2400, Loss: 0.6819654107093811\n",
      "Batch 2500, Loss: 0.6780591607093811\n",
      "Batch 2600, Loss: 0.6780591607093811\n",
      "Batch 2700, Loss: 0.6772779226303101\n",
      "Batch 2800, Loss: 0.6772779226303101\n",
      "Batch 2900, Loss: 0.6788403987884521\n",
      "Batch 3000, Loss: 0.6780591607093811\n",
      "Batch 3100, Loss: 0.676496684551239\n",
      "Average Training Loss: 0.6786587067119038\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 4/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6788403987884521\n",
      "Batch 100, Loss: 0.6827466487884521\n",
      "Batch 200, Loss: 0.680402934551239\n",
      "Batch 300, Loss: 0.6780591607093811\n",
      "Batch 400, Loss: 0.6819654107093811\n",
      "Batch 500, Loss: 0.6772779226303101\n",
      "Batch 600, Loss: 0.6780591607093811\n",
      "Batch 700, Loss: 0.676496684551239\n",
      "Batch 800, Loss: 0.6796216368675232\n",
      "Batch 900, Loss: 0.6780591607093811\n",
      "Batch 1000, Loss: 0.6819654107093811\n",
      "Batch 1100, Loss: 0.6796216368675232\n",
      "Batch 1200, Loss: 0.6796216368675232\n",
      "Batch 1300, Loss: 0.6772779226303101\n",
      "Batch 1400, Loss: 0.6796216368675232\n",
      "Batch 1500, Loss: 0.6780591607093811\n",
      "Batch 1600, Loss: 0.6772779226303101\n",
      "Batch 1700, Loss: 0.6772779226303101\n",
      "Batch 1800, Loss: 0.6796216368675232\n",
      "Batch 1900, Loss: 0.6780591607093811\n",
      "Batch 2000, Loss: 0.680402934551239\n",
      "Batch 2100, Loss: 0.6819654107093811\n",
      "Batch 2200, Loss: 0.6796216368675232\n",
      "Batch 2300, Loss: 0.6788403987884521\n",
      "Batch 2400, Loss: 0.680402934551239\n",
      "Batch 2500, Loss: 0.6780591607093811\n",
      "Batch 2600, Loss: 0.6796216368675232\n",
      "Batch 2700, Loss: 0.6788403987884521\n",
      "Batch 2800, Loss: 0.676496684551239\n",
      "Batch 2900, Loss: 0.6819654107093811\n",
      "Batch 3000, Loss: 0.676496684551239\n",
      "Batch 3100, Loss: 0.6780591607093811\n",
      "Average Training Loss: 0.6786589268737167\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 5/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6780591607093811\n",
      "Batch 100, Loss: 0.6796216368675232\n",
      "Batch 200, Loss: 0.6788403987884521\n",
      "Batch 300, Loss: 0.6788403987884521\n",
      "Batch 400, Loss: 0.680402934551239\n",
      "Batch 500, Loss: 0.6796216368675232\n",
      "Batch 600, Loss: 0.6780591607093811\n",
      "Batch 700, Loss: 0.676496684551239\n",
      "Batch 800, Loss: 0.6827466487884521\n",
      "Batch 900, Loss: 0.6780591607093811\n",
      "Batch 1000, Loss: 0.6835278868675232\n",
      "Batch 1100, Loss: 0.6811841726303101\n",
      "Batch 1200, Loss: 0.676496684551239\n",
      "Batch 1300, Loss: 0.6772779226303101\n",
      "Batch 1400, Loss: 0.6780591607093811\n",
      "Batch 1500, Loss: 0.6757153868675232\n",
      "Batch 1600, Loss: 0.6780591607093811\n",
      "Batch 1700, Loss: 0.6796216368675232\n",
      "Batch 1800, Loss: 0.6788403987884521\n",
      "Batch 1900, Loss: 0.6780591607093811\n",
      "Batch 2000, Loss: 0.6772779226303101\n",
      "Batch 2100, Loss: 0.6780591607093811\n",
      "Batch 2200, Loss: 0.680402934551239\n",
      "Batch 2300, Loss: 0.6780591607093811\n",
      "Batch 2400, Loss: 0.6772779226303101\n",
      "Batch 2500, Loss: 0.6788403987884521\n",
      "Batch 2600, Loss: 0.6757153868675232\n",
      "Batch 2700, Loss: 0.6811841726303101\n",
      "Batch 2800, Loss: 0.6772779226303101\n",
      "Batch 2900, Loss: 0.676496684551239\n",
      "Batch 3000, Loss: 0.6772779226303101\n",
      "Batch 3100, Loss: 0.6757153868675232\n",
      "Average Training Loss: 0.6786591447605431\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 6/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6796216368675232\n",
      "Batch 100, Loss: 0.6835278868675232\n",
      "Batch 200, Loss: 0.6780591607093811\n",
      "Batch 300, Loss: 0.6788403987884521\n",
      "Batch 400, Loss: 0.680402934551239\n",
      "Batch 500, Loss: 0.6796216368675232\n",
      "Batch 600, Loss: 0.6772779226303101\n",
      "Batch 700, Loss: 0.6772779226303101\n",
      "Batch 800, Loss: 0.6772779226303101\n",
      "Batch 900, Loss: 0.680402934551239\n",
      "Batch 1000, Loss: 0.676496684551239\n",
      "Batch 1100, Loss: 0.6780591607093811\n",
      "Batch 1200, Loss: 0.6788403987884521\n",
      "Batch 1300, Loss: 0.6749341487884521\n",
      "Batch 1400, Loss: 0.6796216368675232\n",
      "Batch 1500, Loss: 0.6749341487884521\n",
      "Batch 1600, Loss: 0.6819654107093811\n",
      "Batch 1700, Loss: 0.680402934551239\n",
      "Batch 1800, Loss: 0.676496684551239\n",
      "Batch 1900, Loss: 0.676496684551239\n",
      "Batch 2000, Loss: 0.6780591607093811\n",
      "Batch 2100, Loss: 0.6796216368675232\n",
      "Batch 2200, Loss: 0.6772779226303101\n",
      "Batch 2300, Loss: 0.6780591607093811\n",
      "Batch 2400, Loss: 0.6796216368675232\n",
      "Batch 2500, Loss: 0.6796216368675232\n",
      "Batch 2600, Loss: 0.680402934551239\n",
      "Batch 2700, Loss: 0.6780591607093811\n",
      "Batch 2800, Loss: 0.676496684551239\n",
      "Batch 2900, Loss: 0.6788403987884521\n",
      "Batch 3000, Loss: 0.676496684551239\n",
      "Batch 3100, Loss: 0.6780591607093811\n",
      "Average Training Loss: 0.6786593646379827\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 7/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6827466487884521\n",
      "Batch 100, Loss: 0.6796216368675232\n",
      "Batch 200, Loss: 0.676496684551239\n",
      "Batch 300, Loss: 0.6780591607093811\n",
      "Batch 400, Loss: 0.676496684551239\n",
      "Batch 500, Loss: 0.6780591607093811\n",
      "Batch 600, Loss: 0.6796216368675232\n",
      "Batch 700, Loss: 0.6780591607093811\n",
      "Batch 800, Loss: 0.6819654107093811\n",
      "Batch 900, Loss: 0.6780591607093811\n",
      "Batch 1000, Loss: 0.6788403987884521\n",
      "Batch 1100, Loss: 0.676496684551239\n",
      "Batch 1200, Loss: 0.6788403987884521\n",
      "Batch 1300, Loss: 0.680402934551239\n",
      "Batch 1400, Loss: 0.6788403987884521\n",
      "Batch 1500, Loss: 0.6788403987884521\n",
      "Batch 1600, Loss: 0.6780591607093811\n",
      "Batch 1700, Loss: 0.6811841726303101\n",
      "Batch 1800, Loss: 0.6819654107093811\n",
      "Batch 1900, Loss: 0.6780591607093811\n",
      "Batch 2000, Loss: 0.680402934551239\n",
      "Batch 2100, Loss: 0.6780591607093811\n",
      "Batch 2200, Loss: 0.6749341487884521\n",
      "Batch 2300, Loss: 0.6819654107093811\n",
      "Batch 2400, Loss: 0.6796216368675232\n",
      "Batch 2500, Loss: 0.6780591607093811\n",
      "Batch 2600, Loss: 0.6796216368675232\n",
      "Batch 2700, Loss: 0.6780591607093811\n",
      "Batch 2800, Loss: 0.6780591607093811\n",
      "Batch 2900, Loss: 0.680402934551239\n",
      "Batch 3000, Loss: 0.680402934551239\n",
      "Batch 3100, Loss: 0.6835278868675232\n",
      "Average Training Loss: 0.6786589262291371\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 8/10\n",
      "----------\n",
      "Batch 0, Loss: 0.680402934551239\n",
      "Batch 100, Loss: 0.6772779226303101\n",
      "Batch 200, Loss: 0.6757153868675232\n",
      "Batch 300, Loss: 0.6788403987884521\n",
      "Batch 400, Loss: 0.6780591607093811\n",
      "Batch 500, Loss: 0.6827466487884521\n",
      "Batch 600, Loss: 0.6788403987884521\n",
      "Batch 700, Loss: 0.6788403987884521\n",
      "Batch 800, Loss: 0.676496684551239\n",
      "Batch 900, Loss: 0.6772779226303101\n",
      "Batch 1000, Loss: 0.6780591607093811\n",
      "Batch 1100, Loss: 0.6772779226303101\n",
      "Batch 1200, Loss: 0.6811841726303101\n",
      "Batch 1300, Loss: 0.680402934551239\n",
      "Batch 1400, Loss: 0.680402934551239\n",
      "Batch 1500, Loss: 0.676496684551239\n",
      "Batch 1600, Loss: 0.6788403987884521\n",
      "Batch 1700, Loss: 0.6796216368675232\n",
      "Batch 1800, Loss: 0.6788403987884521\n",
      "Batch 1900, Loss: 0.676496684551239\n",
      "Batch 2000, Loss: 0.6796216368675232\n",
      "Batch 2100, Loss: 0.6811841726303101\n",
      "Batch 2200, Loss: 0.6788403987884521\n",
      "Batch 2300, Loss: 0.6788403987884521\n",
      "Batch 2400, Loss: 0.6772779226303101\n",
      "Batch 2500, Loss: 0.6788403987884521\n",
      "Batch 2600, Loss: 0.6796216368675232\n",
      "Batch 2700, Loss: 0.6788403987884521\n",
      "Batch 2800, Loss: 0.6788403987884521\n",
      "Batch 2900, Loss: 0.6835278868675232\n",
      "Batch 3000, Loss: 0.6780591607093811\n",
      "Batch 3100, Loss: 0.6788403987884521\n",
      "Average Training Loss: 0.6786582680376432\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 9/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6796216368675232\n",
      "Batch 100, Loss: 0.6788403987884521\n",
      "Batch 200, Loss: 0.6757153868675232\n",
      "Batch 300, Loss: 0.6772779226303101\n",
      "Batch 400, Loss: 0.6796216368675232\n",
      "Batch 500, Loss: 0.6796216368675232\n",
      "Batch 600, Loss: 0.6819654107093811\n",
      "Batch 700, Loss: 0.6780591607093811\n",
      "Batch 800, Loss: 0.680402934551239\n",
      "Batch 900, Loss: 0.6796216368675232\n",
      "Batch 1000, Loss: 0.6757153868675232\n",
      "Batch 1100, Loss: 0.676496684551239\n",
      "Batch 1200, Loss: 0.6788403987884521\n",
      "Batch 1300, Loss: 0.6772779226303101\n",
      "Batch 1400, Loss: 0.6772779226303101\n",
      "Batch 1500, Loss: 0.6796216368675232\n",
      "Batch 1600, Loss: 0.6772779226303101\n",
      "Batch 1700, Loss: 0.6780591607093811\n",
      "Batch 1800, Loss: 0.680402934551239\n",
      "Batch 1900, Loss: 0.6772779226303101\n",
      "Batch 2000, Loss: 0.6780591607093811\n",
      "Batch 2100, Loss: 0.6788403987884521\n",
      "Batch 2200, Loss: 0.6796216368675232\n",
      "Batch 2300, Loss: 0.6780591607093811\n",
      "Batch 2400, Loss: 0.6796216368675232\n",
      "Batch 2500, Loss: 0.6788403987884521\n",
      "Batch 2600, Loss: 0.6788403987884521\n",
      "Batch 2700, Loss: 0.6788403987884521\n",
      "Batch 2800, Loss: 0.6796216368675232\n",
      "Batch 2900, Loss: 0.680402934551239\n",
      "Batch 3000, Loss: 0.6757153868675232\n",
      "Batch 3100, Loss: 0.6772779226303101\n",
      "Average Training Loss: 0.6786591453103316\n",
      "Average Validation Loss: 0.6794105570367042\n",
      "Epoch 10/10\n",
      "----------\n",
      "Batch 0, Loss: 0.6780591607093811\n",
      "Batch 100, Loss: 0.6796216368675232\n",
      "Batch 200, Loss: 0.6788403987884521\n",
      "Batch 300, Loss: 0.6772779226303101\n",
      "Batch 400, Loss: 0.6811841726303101\n",
      "Batch 500, Loss: 0.680402934551239\n",
      "Batch 600, Loss: 0.6827466487884521\n",
      "Batch 700, Loss: 0.6772779226303101\n",
      "Batch 800, Loss: 0.6788403987884521\n",
      "Batch 900, Loss: 0.6811841726303101\n",
      "Batch 1000, Loss: 0.6780591607093811\n",
      "Batch 1100, Loss: 0.6757153868675232\n",
      "Batch 1200, Loss: 0.6772779226303101\n",
      "Batch 1300, Loss: 0.6772779226303101\n",
      "Batch 1400, Loss: 0.6796216368675232\n",
      "Batch 1500, Loss: 0.6796216368675232\n",
      "Batch 1600, Loss: 0.6780591607093811\n",
      "Batch 1700, Loss: 0.6780591607093811\n",
      "Batch 1800, Loss: 0.6788403987884521\n",
      "Batch 1900, Loss: 0.6827466487884521\n",
      "Batch 2000, Loss: 0.676496684551239\n",
      "Batch 2100, Loss: 0.680402934551239\n",
      "Batch 2200, Loss: 0.6780591607093811\n",
      "Batch 2300, Loss: 0.6772779226303101\n",
      "Batch 2400, Loss: 0.680402934551239\n",
      "Batch 2500, Loss: 0.6772779226303101\n",
      "Batch 2600, Loss: 0.6780591607093811\n",
      "Batch 2700, Loss: 0.6819654107093811\n",
      "Batch 2800, Loss: 0.676496684551239\n",
      "Batch 2900, Loss: 0.6772779226303101\n",
      "Batch 3000, Loss: 0.6780591607093811\n",
      "Batch 3100, Loss: 0.6780591607093811\n",
      "Average Training Loss: 0.6786589255087249\n",
      "Average Validation Loss: 0.6794105570367042\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (X, y, _, _) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {loss.item()}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Training Loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, _, _ in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Validation Loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(\"-\" * 10)\n",
    "    train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, loss_fn, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035f5022",
   "metadata": {
    "id": "cT7dvhsmHuPp",
    "papermill": {
     "duration": 0.890332,
     "end_time": "2024-03-24T05:11:20.536527",
     "exception": false,
     "start_time": "2024-03-24T05:11:19.646195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to vgg_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"vgg_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a8276",
   "metadata": {
    "papermill": {
     "duration": 0.086813,
     "end_time": "2024-03-24T05:11:20.709463",
     "exception": false,
     "start_time": "2024-03-24T05:11:20.622650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3630.02537,
   "end_time": "2024-03-24T05:11:23.462049",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-24T04:10:53.436679",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
